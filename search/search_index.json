{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Allen Project","text":"<p>This is the documentation for Allen Project.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Functional connectivity analysis in the visual hierarchy is the basement towards studying continuous video stimuli processing. In this project, we aim to analyze the functional connection between the first two stage of the mouse visual cortex, specifically the V1 and LM. The neural activity data is obtained from the Visual Coding - Neuropixels dataset. I employed Reduced Rank Regression (RRR) to uncover the underlying relationships between these brain regions. This analysis will provide valuable insights into the neural mechanisms underlying visual processing.</p> <p>The experiments are generated using the DVC framework and all the previous experiments are documented in the experiment history.</p> <p>There are two Allen Brain Observatory datasets used in this project:</p> <ol> <li>Visual Behavior - Neuropixels: The most of the experiments are based on this dataset. Since the stimuli were very limited, we switched to the next dataset.</li> <li>Visual Coding - Neuropixels: The last experiment (<code>layer-rank</code> analysis) is based on this dataset. There are also movies in this dataset, which can be used for further analysis.</li> </ol> <p>Use the latter dataset for the analyses.</p> <p>Environment: I used the computer m3 and conda environment for pip.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ul> <li>DVC Framework</li> <li>Experiment History</li> <li>References</li> </ul>"},{"location":"#project-overview","title":"Project Overview","text":""},{"location":"#allen--allen-project","title":"Allen Project","text":"<p>Allen Project is a Python package for data analysis and visualization.</p>"},{"location":"#allen--packages","title":"Packages","text":"<p>Allen Project includes subpackages:</p> <ul> <li><code>utils</code>: Utilities for data analysis and visualization.</li> <li><code>analyses</code>: Analyses for data analysis.</li> </ul>"},{"location":"#allen--utility-tools","title":"Utility tools","text":"<p>The following main utility tools are available (in the <code>utils</code> subpackage):</p> <ul> <li><code>debug</code>: Debugging tool.</li> <li><code>megaplot</code>: This tool contains a class for creating and managing subplots in matplotlib.</li> <li><code>plots</code>: Plotting tool for all kinds of plots.</li> <li><code>AllenTable</code>: This tool contains a class for creating tables. See in the <code>utils/neuropixel.py</code> file.</li> </ul>"},{"location":"#allen--preprocessing-tools","title":"Preprocessing tools","text":"<ul> <li><code>preprocessing</code>: Preprocessing tool for further data preprocessing after data initialization.</li> <li><code>behav-preprocessing</code>: Preprocessing tool for behavioral data.</li> <li><code>layer-assignment-analysis</code>: Analysis tool for layer assignment.</li> </ul>"},{"location":"#allen--experiments","title":"Experiments","text":"<p>The following two-step experiments are available (make extra attention to the file names, as they are not always the same as the experiment names):</p> <ul> <li>layer-rank: This experiment is about the rank of the layers in the RRR model. Corresponding codes are <code>layer-rank-analysis</code> and <code>layer-rank-plot</code>.</li> <li>layer-interaction: This experiment is about the interaction between layers in a neural network. Corresponding codes are <code>layer-interaction-analysis</code> and <code>layer-interaction-plot</code>.</li> <li>rrr-time-slice: This experiment is about the time slice of the RRR model. Corresponding codes are <code>rrr-time-slice-analysis</code> and <code>rrr-time-slice-plot</code>.</li> <li>crosstime: This experiment is about the cross-time RRR model. Corresponding codes are <code>crosstime-analysis</code> and <code>crosstime-plot</code>.</li> <li>time-lag-search: This experiment is about the time lag search in the RRR model. Corresponding codes are <code>time-lag-search-analysis</code> and <code>time-lag-search-plot</code>.</li> <li>cv-time-lag: This experiment is about the cross-validation of the time lag in the RRR model. Corresponding codes are <code>cv-time-lag-analysis</code> and <code>cv-time-lag-plot</code>.</li> <li>rrr-score-time: This experiment is about the score along time in the RRR model. Corresponding codes are <code>rrr-score-time</code> and <code>rrr-score-time-plot</code>.</li> <li>cv-rank-time: This experiment is about the cross-validation of the rank along time in the RRR model. Corresponding codes are <code>cv-rank-time-analysis</code> and <code>cv-rank-time-plot</code>.</li> <li>lag-along-time: This experiment is about the lag along time in the RRR model. Corresponding codes are <code>lag-along-time-analysis</code>, <code>lags-along-time-plot</code>, and <code>max-lags-along-time-plot</code>.</li> <li>rank-along-time: This experiment is about the rank along time in the RRR model. Corresponding codes are <code>rank-along-time-analysis</code> and <code>rank-along-time-plot</code>.</li> <li>rrr-rank: This experiment is about the rank of the RRR model. Corresponding codes are <code>rrr-rank-analysis</code> and <code>rrr-rank-plot</code>.</li> <li>time-lag: This experiment is about the time lag in the RRR model. Corresponding codes are <code>time_lag_analysis</code> and <code>time_lag_plot</code>. For additional control: <code>cv-rank-cross-time</code>.</li> <li>pca: This experiment is about the PCA model. Corresponding codes are <code>pca-analysis</code> and <code>pca-plot</code>.</li> <li>rrr: This experiment is about the RRR model. Corresponding codes are <code>rrr_analysis</code> and <code>rrr_plot</code>.</li> <li>cca: This experiment is about the CCA model. Corresponding codes are <code>cca_analysis</code> and <code>cca_plot</code>.</li> </ul> <p>Singular experiments:</p> <ul> <li><code>multiple-timeslices-layers</code>: This utility contains functions for creating multiple time slices in the RRR model for each layer.</li> <li><code>multiple-timeslices</code>: This utility contains functions for creating multiple time slices in the RRR model.</li> <li><code>crosstime-timeslice-plot</code>: This utility contains functions for plotting the cross-time time slice in the RRR model.</li> <li><code>control-models</code>: This utility contains functions for creating control models.</li> <li><code>histograms</code>: This utility contains functions for creating histograms.</li> </ul> <p>The other experiment utilities:</p> <ul> <li><code>save-slices</code>: This utility contains functions for saving slices in the crosstime RRR model.</li> <li><code>layer-interaction-maxValues</code>: This utility contains functions for plotting the maximum values of the layer interaction in the RRR model.</li> <li><code>layer-interaction-stats</code>: This utility contains functions for plotting the statistics of the layer interaction in the RRR model.</li> <li><code>print-results</code>: This utility contains functions for printing the results of the RRR analyses.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started, make sure you have Python 3.7 or higher installed on your machine. Then, clone the project repository from GitHub:</p> <pre><code>git clone https://github.com/CSNLWigner/Allen\n</code></pre> <p>Next, navigate to the project directory:</p> <pre><code>cd Allen_project\n</code></pre> <p>Install the required dependencies using pip:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>Once the dependencies are installed, you can run the last experiment by the DVC framework using the following command:</p> <pre><code>dvc repro\n</code></pre> <p>This command will reproduce the entire pipeline and generate the results.</p>"},{"location":"#reproduce-previous-experiments","title":"Reproduce previous experiments","text":"<p>All the experiments are stored in the <code>pipelines</code> directory or in the experiment history documentation. To reproduce the experiments we use the by the DVC framework. To reproduce a specific experiment, follow these steps:</p> <p>, follow these steps:</p> <ol> <li>Copy the dvc file from the <code>pipelines</code> directory or from a specific experiment in the experiment history to the <code>dvc.yaml</code> file in the root directory.</li> <li>Copy the params file from the <code>pipelines</code> directory or from a specific experiment in the experiment history to the <code>params.yaml</code> file in the root directory.</li> <li>Run the following command:</li> </ol> <pre><code>   dvc repro\n</code></pre> <ol> <li>If there are plotting stage in the pipeline, the visual results can be found in the <code>figures</code> directory (see the <code>outs</code> in the last stage).</li> </ol>"},{"location":"DVC/","title":"DVC framework","text":"<p>DVC is a version control system developed for data science and machine learning projects. It enhances reproducibility and parameter tuning by tracking data, code, and models. DVC is designed to work with Git repositories.</p>"},{"location":"DVC/#table-of-contents","title":"Table Of Contents","text":"<ul> <li>DVC framework</li> <li>Table Of Contents</li> <li>Usage</li> <li>Parameters</li> <li>Pipeline</li> <li>Experiments</li> </ul>"},{"location":"DVC/#usage","title":"Usage","text":"<p>There is a <code>.dvc</code> directory that stores metadata and configuration files. The <code>dvc</code> command is used to interact with the DVC framework. The following are some common commands:</p> <ul> <li><code>dvc run -n &lt;name&gt; -d &lt;dependencies&gt; -o &lt;outputs&gt; &lt;command&gt;</code>: Runs a command and generates outputs.</li> <li><code>dvc repro</code>: Reproduces the entire pipeline.</li> <li><code>dvc repro -fs &lt;stage&gt;</code>: Forces the pipeline to run only a specific stage.</li> </ul> <p>After running a command, the DVC framework generates a <code>dvc.lock</code> file that contains information about the pipeline. This file should be committed to the Git repository. (This file contains versioned and indexed information about the pipeline, parameters, codes, data and results.)</p>"},{"location":"DVC/#parameters","title":"Parameters","text":"<p>DVC uses parameters to manage the experiments. These are the hyperparameters that are used in the pipeline. They are stored in a <code>params.yaml</code> file.</p> <p>Structure:</p> <ul> <li><code>cache</code>: parameters for data caching on the file system</li> <li><code>neuropixel</code>: historical parameters</li> <li><code>load</code>: parameters for loading and initial preprocessing of a specific session and location of the data</li> <li><code>preprocess</code>: parameters for preprocessing the data</li> <li><code>cca</code>: parameters for the CCA analysis</li> <li><code>rrr</code>: parameters for the RRR analysis</li> <li><code>rrr-time-slice</code>: parameters for the <code>rrr-time-slice</code> analysis</li> <li><code>crosstime</code>: parameters for the <code>crosstime</code> analysis</li> <li><code>rrr-param-search</code>: parameters for the RRR parameter search: <code>rrr-param-search</code> analysis (This is a very important analysis for determining the optimal parameters for the RRR analysis. For further information, see the <code>rrr-param-search</code> analysis and experiment history)</li> <li><code>layer-rank</code>: parameters for the <code>layer-rank</code> analysis</li> <li><code>interaction-layers</code>: determines the interaction structure between V1 and LM</li> </ul>"},{"location":"DVC/#pipeline","title":"Pipeline","text":"<p>The pipeline is a sequence of stages that are executed in order. Each stage is a separate step in the pipeline. The pipeline with the stages are defined in the <code>dvc.yaml</code> file. The pipeline is executed by running the <code>dvc repro</code> command.</p>"},{"location":"DVC/#experiments","title":"Experiments","text":"<p>Find the pipelines for the different experiments and the corresponding parameters in the pipelines folder or in the experiment history documentation.</p>"},{"location":"about/","title":"About","text":"<p>Welcome to the Allen Project documentation!</p>"},{"location":"about/#introduction","title":"Introduction","text":"<p>The Allen Project is a neuroscience research initiative aimed at understanding the complexities of the brain. Our team of experts is dedicated to advancing the field of neuroscience through cutting-edge research and collaboration.</p>"},{"location":"about/#goals","title":"Goals","text":"<p>Our primary goals are:</p> <ol> <li>Investigating the neural mechanisms underlying cognition and behavior.</li> <li>Developing innovative tools and techniques for studying the brain.</li> <li>Sharing our findings and resources with the scientific community.</li> </ol>"},{"location":"about/#research-areas","title":"Research Areas","text":"<p>We focus on the following research areas:</p> <ul> <li>Brain mapping and connectivity</li> <li>Neural circuitry and synaptic plasticity</li> <li>Cognitive processes and decision-making</li> <li>Neurodevelopment and aging</li> </ul>"},{"location":"about/#collaboration","title":"Collaboration","text":"<p>We actively collaborate with other research institutions, universities, and industry partners to foster interdisciplinary research and accelerate scientific discoveries.</p>"},{"location":"about/#get-involved","title":"Get Involved","text":"<p>If you are interested in joining our team or collaborating with us, please reach out to us via the contact information provided below.</p>"},{"location":"about/#contact","title":"Contact","text":"<p>For any inquiries or further information, please contact us at:</p> <p>Email: info@allenproject.org Phone: +1 (123) 456-7890</p> <p>We look forward to hearing from you!</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/","title":"Allen project","text":"<p>This is the documentation of the previous experiments and the future directions of the Allen project (see Future directions). The entries are in reverse chronological order.</p> <p>Entry structure:</p> <ul> <li>Date: the date of the experiment (documentation)</li> <li>Name/Commits: the corresponding commit names in git history</li> <li>Params: the parameters used in the experiment (see Parameters)</li> <li>Pipeline: the dvc pipeline code (see DVC framework)</li> <li>Results: the figures generated by the experiment (can be very long)</li> <li>What we see: The description of the results in words</li> <li>Interpretation: the interpretation of the results</li> <li>Next: the potential next steps or future directions in the project at that time</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Allen project</li> <li>Table of Contents</li> <li>Future directions</li> <li>Layer-Rank Analysis</li> <li>New data layer distribution</li> <li>Memory bug</li> <li>Normalized maxValues</li> <li>Undersampling neurons</li> <li>Unit number statistics</li> <li>Gabor</li> <li>Layer interaction plot<ul> <li>Transient</li> <li>Causal direction of time</li> <li>Negyszigetes</li> <li>Egyiranyu: L alaku</li> </ul> </li> <li>Layer in megaplot: first Blick</li> <li>LayerAssignment</li> <li>Units</li> <li>Layers</li> <li>After Progress Report<ul> <li>Marci</li> <li>debug</li> </ul> </li> <li>Stimulus residual</li> <li>Stimuli</li> <li>Mega plot 2</li> <li>Mega plot 1</li> <li>V1 to V1</li> <li>RRR time slice</li> <li>Z-scoring</li> <li>Units table</li> <li>Crosstime timeseries of lags<ul> <li>Top-down</li> <li>Bottom-up</li> <li>Within V1</li> <li>Within LM</li> <li>Interpretations</li> </ul> </li> <li>Time lag along trial time<ul> <li>Top-down:</li> <li>Bottom-up:</li> </ul> </li> <li>Time lag search<ul> <li>Top-down</li> <li>Bottom-up</li> </ul> </li> <li>Bottom-up lag</li> <li>Top-down lag exploration</li> <li>V2-V1 interaction rank</li> <li>Multiple session param search!!!</li> <li>Sessions</li> <li>V2 to V1</li> <li>RRR score by time</li> <li>Bootstrap above cv folds</li> <li>CV, timelag, timepoint exploration<ul> <li>More params</li> <li>Less params #1</li> <li>Less params #2</li> </ul> </li> <li>CV-rank-time exploration<ul> <li>Detailed</li> </ul> </li> <li>Time step</li> <li>Semedo</li> <li>Rank analysis from V2 to V1</li> <li>Rank analysis for gabor</li> <li>Control models</li> <li>Rank optimization</li> <li>Behav data</li> <li>GitHub codes</li> <li>Time lag between V1 and V2</li> <li>Siegle, 2021 on this data</li> <li>Areas</li> <li>Data distribution</li> <li>PCA</li> <li>RRR analysis along time<ul> <li>Me:</li> <li>Gergo:</li> <li>Corr:</li> <li>Marci:</li> <li>References:</li> <li>Future plan:</li> </ul> </li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#future-directions","title":"Future directions","text":"<ul> <li>make predictions via the first d predictive dimensions. same area, cross area, \u2026</li> <li>in the video, we can use seqPCA to extract the dims\u2026 only if its not a linear video\u2026?</li> <li>RRR compared to CCA.</li> <li>mozg\u00e1s controll \u00faj sessionokon is!</li> <li>t\u00fal sz\u00e9les time-bin, mindenbe belel\u00f3g, irrelev\u00e1ns lesz minden lag meg timepoint, szedd le 50-re</li> <li>id\u0151 ment\u00e9n rank analysis? biztos hogy \u00e1lland\u00f3?</li> <li>rankokkal j\u00e1tszani \u2192 pl kis rankok eset\u00e9n milyen a mega-plot?</li> <li>rankok m\u00e9rete grating VS natural images - bottom-up VS top-down</li> <li>neuronok sz\u00e1ma</li> <li>undersample param-search</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#layer-rank-analysis","title":"Layer-Rank Analysis","text":"<p>See references for more information.</p> <p>Commits:</p> <ul> <li>Layer-Rank Analysis</li> <li>Layer-Rank Plot</li> </ul> <p>Date: 2024.08.06.</p> <ul> <li>params</li> </ul> <pre><code>layer-rank:\n  cv: [2,3,4,5,6,7,8,9,10]\n  bestRank: 15\n  minRank: 5\n  maxRank: 25\n  stepRank: 1\n  timepoint: 100 # in milliseconds\n</code></pre> <ul> <li>code</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - preprocessing.py\n      - data/raw-area-responses/\n    params:\n      - preprocess\n    outs:\n      - data/area-responses/\n  analysis:\n    cmd: python3 -u layer-rank-analysis.py\n    deps:\n      - layer-rank-analysis.py\n      - analyses/layer_rank.py\n      - data/area-responses/\n      - data/units/\n    params:\n      - load\n      - preprocess\n      - layer-rank\n    outs:\n      - results/layer-rank.pickle\n  plot:\n    cmd: python3 -u layer-rank-plot.py\n    deps:\n      - layer-rank-plot.py\n      - results/layer-rank.pickle\n    params:\n      - preprocess\n    outs:\n      - figures/layer-rank.png\n</code></pre> <p>Result:</p> <p></p> <p>Interpretation: Rank is independent from layers but dependent from time.</p> <p>Next: simple rank-time analysis: </p> <pre><code>  rank-plot:\n    cmd: python3 -u layer-rank-plot.py -rank\n    deps:\n      - layer-rank-plot.py\n      - results/layer-rank.pickle\n    params:\n      - preprocess\n    outs:\n      - figures/layer-rank.png\n  r2-plot:\n    cmd: python3 -u layer-rank-plot.py -r2\n    deps:\n      - layer-rank-plot.py\n      - results/layer-rank.pickle\n    params:\n      - preprocess\n    outs:\n      - figures/layer-rank.png\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#new-data-layer-distribution","title":"New data layer distribution","text":"<p>date: 2024.07.31.</p> <pre><code>Unique values in VISp: [6 5 4 2]\nRatio of value 6: 26.67%\nRatio of value 5: 43.33%\nRatio of value 4: 21.67%\nRatio of value 2: 8.33%\nUnique values in VISl: [6 5 4 2]\nRatio of value 6: 16.22%\nRatio of value 5: 40.54%\nRatio of value 4: 32.43%\nRatio of value 2: 10.81%\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#memory-bug","title":"Memory bug","text":"<p>date: 2024.07.29.</p> <p>params:</p> <pre><code>- stimulus block: natural scene\n</code></pre> <p>new one:</p> <pre><code>&gt;&gt;&gt; np.zeros((n_unit, n_trial, n_step))\nnumpy.core._exceptions.MemoryError: Unable to allocate 40.5 GiB for an array with shape (3651, 5950, 250) and data type float64\n</code></pre> <p>old one:</p> <pre><code>&gt;&gt;&gt; print('Tensor shape:', tensor.shape)\nTensor shape: (165, 4805, 250)\n</code></pre> <p>solution:</p> <p>Use <code>units = session.units</code> instead of <code>units = cache.get_units()</code></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#normalized-maxvalues","title":"Normalized maxValues","text":"<p>Date: 07.15.</p> <p>Source: https://www.wikiwand.com/en/Coefficient_of_determination#Adjusted_R2</p> <p>Formula:</p> <p>$$ \\bar{r^2}=1-(1-r^2)\\frac{trials-1}{trials-neurons-1} $$</p> <p>Result:</p> <p></p> <p>What we see:</p> <ul> <li>Bottom-up max r2 at 50-50 ms and at 140-160 ms</li> <li>Bottom-up from layer 2,5 to layer 4,5 later from 5,6 to 2</li> <li>Top-down max r2 at 50-60 ms and at 140-160 ms</li> <li>Top-down from layer 5,6 to layer 5,6 later from 5,6 to 6</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#undersampling-neurons","title":"Undersampling neurons","text":"<p>https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/</p> <p>https://datascience.stackexchange.com/questions/45046/cross-validation-for-highly-imbalanced-data-with-undersampling</p> <p>https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#unit-number-statistics","title":"Unit number statistics","text":"<p>Date: 2024.07.03.</p> <p>across sessions</p> <p>Code:</p> <pre><code># Load the data\ndf = pd.read_csv('results/maxValues.csv')\ndf = df.set_index(['session', 'direction', 'slice', 'output layer', 'input layer'])\ndf = df.drop(1087720624, level='session')\n\nmean = df.groupby(['direction', 'output layer']).mean()[['output layer units']].rename(columns={'output layer units': 'mean'})\nstd = df.groupby(['direction', 'output layer']).std()[['output layer units']].rename(columns={'output layer units': 'std'}) * 2\nstat = pd.concat([mean, std], axis=1).rename(index={'LM-to-V1': 'LM', 'V1-to-LM': 'V1'})#.rename(level={'output layer': 'layer', 'direction': 'area'})\nprint(mean)\nstat.plot.bar(y='mean', yerr='std')\nplt.show()\n</code></pre> <p>Results:</p> Area Layer Units_mean LM 2 14.138462 4 20.415385 5 42.830769 6 22.353846 V1 2 17.619718 4 20.873239 5 37.295775 6 20.901408 <p></p> <p>challange: the number of features (units) affecting the accuracy of the analysis.</p> <ol> <li>feature selection<ol> <li>filter method: </li> <li>wrapper method:</li> <li>ensembled method: e.g. L1 (lasso) regularization</li> </ol> </li> <li> <p>Model Evaluation Metrics (e.g. f1-score for binary classification): </p> <p>Adjusted R-squared accounts for the number of features in your model. It penalizes adding irrelevant features that don\u2019t improve prediction.</p> </li> <li> <p>~~Ensemble Methods~~. its for improve the overall/common predictiveness, not to comparing.</p> </li> </ol> <p>todo:</p> <ul> <li>csak 5. r\u00e9teg undersample, mi az als\u00f3 limit to good performance.</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#gabor","title":"Gabor","text":"<p>Date: 2024.06.25</p> <p>Name: Gabor</p> <p>natural</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Gabor</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#layer-interaction-plot","title":"Layer interaction plot","text":"<p>See references for more information.</p> <p>Name: Layer interaction plot</p> <p>Date: 2024.06.14.-19.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  layer-assignment:\n    cmd: python3 -u layer-assignment-analysis.py\n    deps:\n      - layer-assignment-analysis.py\n      - data/ccf_volumes/\n    params:\n      - load\n    outs:\n      - data/units/\n  analysis:\n    cmd: python3 -u layer-interaction-analysis.py\n    deps:\n      - layer-interaction-analysis.py\n      - data/units/\n    params:\n      - load\n      - crosstime\n    outs:\n      - results/layer-interaction_V1-to-LM.pickle\n      - results/layer-interaction_LM-to-V1.pickle\n  plots:\n    cmd: python3 -u layer-interaction-plot.py\n    deps:\n      - layer-interaction-plot.py\n    params:\n      - preprocess\n    outs:\n      - figures/layer-interaction_V1-to-LM.png\n      - figures/layer-interaction_LM-to-V1.png\n</code></pre> <p>jav\u00edtand\u00f3:</p> <ul> <li>[x]  session number in title</li> <li>[x]  x and y axes ticks</li> <li>[x]  neuronok sz\u00e1ma</li> <li>[x]  x-y axes label bug</li> </ul> <p>Results:</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#transient","title":"Transient","text":"<ul> <li>1093864136, 1095340643</li> <li>transient representation to LM l4 and to V1 l4</li> <li>LM l2 \u21d2 V1 l6</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#causal-direction-of-time","title":"Causal direction of time","text":"<ul> <li>1091039376</li> <li>bottom-up l2 \u2192 l4, l5</li> <li>bottom-up predictive in the causal direction of time. \u2194 top-down<ul> <li>ink\u00e1bb bottom-up.</li> </ul> </li> <li>top-down n\u00e9gyszigetes LM l5 \u2192 V1 l6 (l4), m\u00e1shol transient<ul> <li>itt a k\u00fcl\u00f6nbs\u00e9g, melyik layer a felel\u0151s az \u00e1lland\u00f3 aktivit\u00e1s\u00e9rt.</li> </ul> </li> </ul> <ul> <li>1090803859</li> <li>bottom up l2 \u2192 l4, l5</li> <li>bottom-up predictive in the causal direction of time.</li> <li>cs\u00facs 80 ms to V1 l6</li> </ul> <ul> <li>1098119201</li> <li>bottom-up to l4 kev\u00e9sb\u00e9 f\u00fcgg\u0151leges vonal</li> <li>rather predictive in the causal direction of time.</li> <li>top-down LM l5 \u2192 V1 l2 l5 v\u00edzszintes</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#negyszigetes","title":"Negyszigetes","text":"<ul> <li>1111013640, 1081079981, 1048189115, 1049514117</li> <li>n\u00e9gyszigetes, ink\u00e1bb top-down a time causal k\u00e9sei response alapj\u00e1n, 50 ms alapj\u00e1n ink\u00e1bb bottom-up.</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#egyiranyu-l-alaku","title":"Egyiranyu: L alaku","text":"<ul> <li>1086200042</li> <li>bottom-up l5 \u2192 l2, l4</li> <li>top-down l2, l4 \u2192 l5</li> <li>bottom-up f\u00fcgg\u0151leges, top-down v\u00edzszintes<ul> <li>f\u0151k\u00e9nt top-down caught up</li> </ul> </li> </ul> <ul> <li>1067781390</li> <li>bottom-up to l2</li> <li>bottom-up f\u00fcgg\u0151leges, (tranzient)</li> <li>top-down l4, l6 \u2192 l2 v\u00edzszintes,</li> <li>top-down l4 \u2192 l6 f\u00fcgg\u0151leges</li> <li>top-down egyeb\u00fctt transient</li> </ul> <ul> <li>1055415082, 1052533639</li> <li>bottom-up to l2</li> <li>bottom-up f\u00fcgg\u0151leges, (tranzient)</li> <li>top-down L alak\u00fa l4, l5 \u2192 l6</li> </ul> <ul> <li>1109889304</li> <li>LM l5 \u2192 V1 l4</li> <li>V1 l2 l5 \u2192 LM l5</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#layer-in-megaplot-first-blick","title":"Layer in megaplot: first Blick","text":"<p>See references for more information.</p> <p>Name: LAYERPLOT.</p> <p>Date: 2024.06.12.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  layer-assignment:\n    cmd: python3 -u layer-assignment-analysis.py\n    deps:\n      - layer-assignment-analysis.py\n      - data/ccf_volumes/\n    params:\n      - load\n    outs:\n      - data/units/\n  plots:\n    cmd: python3 -u multiple-timeslices-layers.py\n    deps:\n      - multiple-timeslices-layers.py\n      - session-params-old.csv\n      - data/units/\n    params:\n      - load\n      - preprocess\n      - rrr-time-slice\n      - interaction-layers\n    outs:\n      - figures/rrr-cross-time-slice-mega-plot.png\n</code></pre> <ul> <li>Params</li> </ul> <pre><code>interaction-layers:\n  V1:\n    input: [5] # [1, 2]\n    output: [2] # 2 is 2/3\n  LM:\n    input: [4]\n    output: [6]\n</code></pre> <p>Result:</p> <p></p> <p>What we see:</p> <ul> <li>activity at 50 ms in LM predicts activity in all time in V1</li> <li>activity at 180 ms in LM predicts activity in all time in V1</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#layerassignment","title":"LayerAssignment","text":"<p>Date: 2024.06.11.</p> <p>session: 1048189115</p> <pre><code>Unique values in VISp: [5 2 4 6]\nRatio of value 5: 33.63%\nRatio of value 2: 30.97%\nRatio of value 4: 22.12%\nRatio of value 6: 13.27%\nUnique values in VISl: [5 2 4 6]\nRatio of value 5: 46.97%\nRatio of value 2: 9.09%\nRatio of value 4: 18.18%\nRatio of value 6: 25.76%\n</code></pre> <pre><code>layerAssignments_V1.isin(neurmask['V1']['output'])\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#units","title":"Units","text":"<p>Date: 2024.06.03</p> units from table units from session Load time 0.06 s 28.36 s columns <code>['ecephys_channel_id', 'ecephys_probe_id', 'ecephys_session_id', 'amplitude_cutoff', 'anterior_posterior_ccf_coordinate', 'dorsal_ventral_ccf_coordinate', 'left_right_ccf_coordinate', 'cumulative_drift', 'd_prime', 'structure_acronym', 'structure_id', 'firing_rate', 'isi_violations', 'isolation_distance', 'l_ratio', 'local_index', 'max_drift', 'nn_hit_rate', 'nn_miss_rate', 'presence_ratio', 'probe_horizontal_position', 'probe_vertical_position', 'silhouette_score', 'snr', 'quality', 'valid_data', 'amplitude', 'waveform_duration', 'waveform_halfwidth', 'PT_ratio', 'recovery_slope', 'repolarization_slope', 'spread', 'velocity_above', 'velocity_below'] | ['PT_ratio', 'amplitude', 'amplitude_cutoff', 'cluster_id', 'cumulative_drift', 'd_prime', 'firing_rate', 'isi_violations', 'isolation_distance', 'l_ratio', 'local_index', 'max_drift', 'nn_hit_rate', 'nn_miss_rate', 'peak_channel_id', 'presence_ratio', 'quality', 'recovery_slope', 'repolarization_slope', 'silhouette_score', 'snr', 'spread', 'velocity_above', 'velocity_below', 'waveform_duration']</code> code <code>units_table = cache.get_unit_table()</code> <code>units_cache = units_table[units_table['ecephys_session_id'] == session_id]</code> <code>session = cache.get_ecephys_session(ecephys_session_id=session_id)</code> <code>units_session = session.get_units()</code> - code <pre><code># %% Load the Allen Neuropixel dataset\n\n# Load parameters\nparams = yaml.safe_load(open('params.yaml'))['load']\n\n# An arbitrary session from the Allen Neuropixel dataset\nsession_id = params['session']  # 1064644573  # 1052533639\ncache = cache_allen()\n\n# %%\n\nstart = time.time()\nunits_table = cache.get_unit_table()\nunits_cache = units_table[units_table['ecephys_session_id'] == session_id]\nend = time.time()\nprint('Elapsed time:', end - start)\n\nstart = time.time()\nsession = cache.get_ecephys_session(ecephys_session_id=session_id)\nunits_session = session.get_units()\nend = time.time()\nprint('Elapsed time:', end - start)\n\nic(units_cache.columns)\nprint(units_cache)\nic(units_session.columns)\nprint(units_session)\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#layers","title":"Layers","text":"<p>Date: 2024.05.30.</p> <pre><code>params = yaml.safe_load(open('params.yaml'))['load']\n\n# An arbitrary session from the Allen Neuropixel dataset\nsession_id = params['session']  # 1064644573  # 1052533639\ncache = cache_allen()\nsession = cache.get_ecephys_session(ecephys_session_id=session_id)\n\nfrom utils.ccf_volumes import (cortical_layer_assignment,\n                               layer_assignment_to_channels)\n\nunits = cache.get_unit_table()\n\n# Get the units that are assigned to the current session\nunits = units[units['ecephys_session_id'] == session_id]\n\nunits = cortical_layer_assignment(session, units)\n\n# get the units that are assigned to a layer (no nan)\nlayer_assigned_units = units[units['layer'].notna()]\n\n# Get the length of the layer assigned units and the total number of units\nlayer_assigned_units_len = len(layer_assigned_units)\ntotal_units_len = len(units)\n\n# Print the results\nprint('Layer assigned units:', layer_assigned_units_len)\nprint('Total units:', total_units_len)\nprint('Percentage:', layer_assigned_units_len / total_units_len * 100)\nprint()\nprint(layer_assigned_units)\n</code></pre> <p>Layer assigned units: 1925</p> <p>Total units: 1925</p> <p>Percentage: 100.0</p> <pre><code>        ecephys_channel_id  ecephys_probe_id  ecephys_session_id  amplitude_cutoff  ...  spread  velocity_above  velocity_below  layer\n\n</code></pre> <p>output:</p> <pre><code>unit_id                                                                                 ...\n\n1053057176          1053054953        1048320287          1048189115          0.304306  ...    80.0        0.892797       -0.343384      0\n1053057218          1053055040        1048320287          1048189115          0.022801  ...    50.0       -0.686767       -0.686767      0\n1053057217          1053055040        1048320287          1048189115          0.273037  ...   100.0       -0.480737       -0.745633      0\n1053057134          1053054901        1048320287          1048189115          0.389520  ...    80.0        0.755444       -0.480737      0\n1053057133          1053054901        1048320287          1048189115          0.270332  ...    90.0        0.412060       -0.215841      0\n...                        ...               ...                 ...               ...  ...     ...             ...             ...    ...\n1053056643          1053054317        1048320283          1048189115          0.033838  ...    80.0        0.206030        0.412060      5\n1053056642          1053054317        1048320283          1048189115          0.500000  ...    80.0        0.480737       -0.206030      5\n1053056641          1053054317        1048320283          1048189115          0.000096  ...    80.0        0.206030        0.274707      5\n1053056639          1053054317        1048320283          1048189115          0.000002  ...    80.0        0.343384        0.343384      5\n1053056575          1053054258        1048320283          1048189115          0.003233  ...    70.0        0.000000        0.000000      1\n\n[1925 rows x 36 columns]\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#after-progress-report","title":"After Progress Report","text":"<p>IPR 2024.05.27. &amp; discussion with Gerg\u0151</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#marci","title":"Marci","text":"<p>proposal: stim n\u00e9lk\u00fcli id\u0151ben n\u00e9zni korrel\u00e1ci\u00f3t V1-LM</p> <p>result interpretation: local memory</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#debug","title":"debug","text":"<ul> <li>[x] trial sz\u00e1m cs\u00fasz\u00e1s</li> <li>[x] nem stim resid is rosszabodik-e?</li> <li>[x] neur\u00e1lis adat v\u00e1ltozik vajon?</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#stimulus-residual","title":"Stimulus residual","text":"<p>Name: STIMRESIDUAL</p> <p>Date: 2024.05.15.</p> <p>Result:</p> <p>Session 1113751921</p> <pre><code>maximum value(0.024) at 0 ms is at cv=2, lag=0, rank=8\nmaximum value(0.024) at 50 ms is at cv=2, lag=0, rank=8\nmaximum value(0.024) at 100 ms is at cv=2, lag=0, rank=8\nmaximum value(0.024) at 150 ms is at cv=2, lag=0, rank=8\n</code></pre> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#stimuli","title":"Stimuli","text":"<p>Date: 2024.05.09.</p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#mega-plot-2","title":"Mega plot 2","text":"<p>See references for more information.</p> <p>Date: 2024.05.08.</p> <p>Name:</p> <ul> <li>Pipeline:</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - preprocessing.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n    outs:\n      - data/area-responses/\n  plots:\n    cmd: python3 -u multiple-timeslices.py data/crosstime\n    deps:\n      - multiple-timeslices.py\n      # - results/cross-time-RRR.pickle\n      # - results/rrr-time-slice.pickle\n    params:\n      - load\n      - preprocess\n      - best-rrr-params\n      - rrr-time-slice\n    outs:\n      - figures/rrr-cross-time-slice-mega-plot.png\n</code></pre> <p>Most similar results:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Gerg\u0151:</p> <ol> <li>stimulusok feletti rezidu\u00e1lis sz\u00e1mol\u00e1s (hogy stim repetici\u00f3k \u2026.</li> <li>layerek</li> <li>dimenzionalit\u00e1s (alacsonyabb rankkel j\u00f3solni</li> </ol>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#mega-plot-1","title":"Mega plot 1","text":"<p>Date: 2024.05.08.</p> <p>Name:</p> <p>Results:</p> <p></p> <p>bottom-up: stabil k\u00f3d visszafel\u00e9</p> <p>top-down: tranziens k\u00f3d</p> <p></p> <p></p> <p></p> <p></p> <p>bottom-up: bif\u00e1zisos k\u00f3d (50 ms, 150 ms)</p> <p>top-down: gyeng\u00fcl\u0151 jel/tranziens k\u00f3d</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1-to-v1","title":"V1 to V1","text":"<p>Date: 2024.05.06.</p> <p></p> <p>V1 -&gt; V1 az a session (Fig 1: ...708) ahol a legstabilabb a k\u00f3d (gyakran csak a diagon\u00e1l barna, Fig2: ...624,\u00a0 note that itt is van asszimetrikus r\u00e9sz)</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#rrr-time-slice","title":"RRR time slice","text":"<p>See references for more information.</p> <p>Name: SLICE.</p> <p>Date: 2024.04.30.</p> <ul> <li>params</li> </ul> <pre><code>preprocess:\n  step-size: 0.010\n</code></pre> <ul> <li>pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - preprocessing.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n    outs:\n      - data/area-responses/\n  analysis:\n    cmd: python3 -u rrr-time-slice-analysis.py\n    deps:\n      - rrr-time-slice-analysis.py\n      - data/area-responses/\n    params:\n      - rrr-time-slice\n      - load\n      - best-rrr-params\n    outs:\n      - results/rrr-time-slice.pickle\n  plots:\n    cmd: python3 -u rrr-time-slice-plot.py\n    deps:\n      - rrr-time-slice-plot.py\n      - results/rrr-time-slice.pickle\n    params:\n      - preprocess\n    outs:\n      - figures/rrr-time-slice.png\n</code></pre> <p>Results:</p> <p>sessions: \u202684, \u202674, \u202603</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#z-scoring","title":"Z-scoring","text":"<p>Hogy hol z-scoreozol (in window of interest vagy eg\u00e9sz trial) az R^2-ben mindegy, de coefficiensben nem mindegy.</p> <p>Esetleg z-score mean legyen ITI-b\u0151l.</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#units-table","title":"Units table","text":"<p>General Metadata</p> <p><code>ecephys_channel_id</code>: unique ID of channel on which unit's peak waveform occurred</p> <p><code>ecephys_probe_id</code>: unique ID for probe on which unit was recorded</p> <p><code>ecephys_session_id</code>: unique ID for session during which unit was recorded</p> <p><code>anterior_posterior_ccf_coordinate</code>: CCF coord in the AP axis</p> <p><code>dorsal_ventral_ccf_coordinate</code>: CCF coord in the DV axis</p> <p><code>left_right_ccf_coordinate</code>: CCF coord in the left/right axis</p> <p><code>structure_acronym</code>: CCF acronym for area to which unit was assigned</p> <p><code>structure_id</code>: CCF structure ID for the area to which unit was assigned</p> <p><code>probe_horizontal_position</code>: Horizontal (perpindicular to shank) probe position of each unit's peak channel in microns</p> <p><code>probe_vertical_position</code>: Vertical (along shank) probe position of each unit's peak channel in microns</p> <p>Waveform metrics: Look\u00a0here\u00a0for more detail on these metrics and the code that computes them. For the below descriptions the '1D waveform' is defined as the waveform on the peak channel. The '2D waveform' is the waveform across channels centered on the peak channel.</p> <p><code>amplitude</code>: Peak to trough amplitude for mean 1D waveform in microvolts</p> <p><code>waveform_duration</code>: Time from trough to peak for 1D waveform in milliseconds</p> <p><code>waveform_halfwidth</code>: Width of 1D waveform at half-amplitude in milliseconds</p> <p><code>PT_ratio</code>: Ratio of the max (peak) to the min (trough) amplitudes for 1D waveform</p> <p><code>recovery_slope</code>: Slope of recovery of 1D waveform to baseline after repolarization (coming down from peak)</p> <p><code>repolarization_slope</code>: Slope of repolarization of 1D waveform to baseline after trough</p> <p><code>spread</code>: Range of channels for which the spike amplitude was above 12% of the peak channel amplitude</p> <p><code>velocity_above</code>: Slope of spike propagation velocity traveling in dorsal direction from soma (note to avoid infinite values, this is actaully the inverse of velocity: ms/mm)</p> <p><code>velocity_below</code>: Slope of spike propagation velocity traveling in ventral direction from soma (note to avoid infinite values, this is actually the inverse of velocity: ms/mm)</p> <p><code>snr</code>: signal-to-noise ratio for 1D waveform</p> <p>Quality metrics: Look\u00a0here\u00a0for more detail on these metrics and the code that computes them.</p> <p><code>amplitude_cutoff</code>: estimate of miss rate based on amplitude histogram (ie fraction of spikes estimated to have been below detection threshold)</p> <p><code>cumulative_drift</code>: cumulative change in spike depth along probe throughout the recording</p> <p><code>d_prime</code>: classification accuracy based on LDA</p> <p><code>firing_rate</code>: Mean firing rate over entire recording</p> <p><code>isi_violations</code>: Ratio of refractory violation rate to total spike rate</p> <p><code>isolation_distance</code>: Distance to nearest cluster in Mahalanobis space</p> <p><code>l_ratio</code>: The Mahalanobis distance and chi-squared inverse cdf are used to find the probability of cluster membership for each spike.</p> <p><code>max_drift</code>: Maximum change in unit depth across recording</p> <p><code>nn_hit_rate</code>: Fraction of nearest neighbors in PCA space for spikes in unit cluster that are also in unit cluster</p> <p><code>nn_miss_rate</code>: Fraction of nearest neighbors for spikes outside unit cluster than are in unit cluster</p> <p><code>presence_ratio</code>: Fraction of time during session for which a unit was spiking</p> <p><code>silhouette_score</code>: Standard metric for cluster overlap, computed in PCA space</p> <p><code>quality</code>: Label assigned based on waveform shape as described\u00a0here. Either 'good' for physiological waveforms or 'noise' for artifactual waveforms.</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#crosstime-timeseries-of-lags","title":"Crosstime timeseries of lags","text":"<p>See references for more information.</p> <p>Name: CROSSTIME.</p> <p>Date: 2024.04.09., 10.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  analysis:\n    cmd: python3 -u crosstime-analysis.py\n    deps:\n      - crosstime-analysis.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n      - rrr\n      - best-rrr-params\n      - rrr-param-search\n    outs:\n      - results/cross-time-RRR.pickle\n  plot:\n    cmd: python3 -u crosstime-plot.py\n    deps:\n      - crosstime-plot.py\n      - results/cross-time-RRR.pickle\n    params:\n      - rrr-param-search\n    outs:\n      - figures/cross-time-RRR.png\n</code></pre> <ul> <li>Params</li> </ul> <pre><code>preprocess:\n  areas: [\"VISp\", \"VISl\"] # \"VISpm\" # \"VISm\" has no spikes\n  lag-time: 0 # in milliseconds\n  lag-area: \"VISl\"\n  step-size: 0.010\n  bin-size: 0.100 # time-window in seconds # Joao's default: 0.100\n  stimulus-duration: 0.250 # in seconds\nrrr-param-search:\n  cv: [3]\n  rank: [14]\n  lag: [100,105,110,115,120,125,130,135,140,145,150]\n  timepoints: [0]\n  duration: 0.250\n  time-bin: 0.050\n  time-step: 0.025\n</code></pre> <p>Sessions: \u202684, \u202674, \u202603, \u202640, \u202621, \u202664, \u202615, \u202617, \u202639, GEN2 \u202685, \u2026981, \u202624, \u202636, \u202643, \u202601, \u202621</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#top-down","title":"Top-down","text":"<p>GEN1:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>GEN2:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#bottom-up","title":"Bottom-up","text":"<p>GEN1:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>GEN2:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#within-v1","title":"Within V1","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#within-lm","title":"Within LM","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#interpretations","title":"Interpretations","text":"<p>what we see:</p> <ul> <li>there is a correlation hub in the 50 ms between the two regions. This activity is partially correlated with the activity in both region until the 160th ms.</li> <li>There is an LM activity before the 50 ms hub, which is sustained and not correlated with the V1 activity (eigen subsoace)</li> <li>In the session no. \u202684 there is another activity from 100 to 150 ms, that is transferred from LM to V1.<ul> <li> <p>Maybe also in session \u202615</p> <p></p> </li> </ul> </li> </ul> <p>whats next:</p> <ul> <li>more sessions \u2705</li> <li>RRR within area \u2705</li> <li>cortical layer selective RRR (wich is scientifically known to the inter brain areas connections) (RRR says little about the causality) \u2192 where is the layer information?</li> <li>c\u00e9lzott k\u00e9rd\u00e9sekre DLAG \u2192 MATLABb\u00f3l kell ford\u00edtani</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#time-lag-along-trial-time","title":"Time lag along trial time","text":"<p>Name: LAGXTIME.</p> <p>Date: 2024.04.02.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  analysis:\n    cmd: python3 -u lag-along-time-analysis.py\n    deps:\n      - lag-along-time-analysis.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n      - rrr\n      - rrr-param-search\n      - best-rrr-params\n    outs:\n      - results/lags-along-time.pickle\n      - results/max-lags-along-time.pickle\n  imshow:\n    cmd: python3 -u lags-along-time-plot.py\n    deps:\n      - lags-along-time-plot.py\n      - results/lags-along-time.pickle\n    params:\n      - preprocess\n      - rrr-param-search\n    outs:\n      - figures/lags-along-time.png\n  plot:\n    cmd: python3 -u max-lags-along-time-plot.py\n    deps:\n      - max-lags-along-time-plot.py\n      - results/max-lags-along-time.pickle\n    params:\n      - preprocess\n      - rrr-param-search\n    outs:\n      - figures/max-lags-along-time.png\n</code></pre> <p>Sessions: \u202640, \u202684, \u202674</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#top-down_1","title":"Top-down:","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#bottom-up_1","title":"Bottom-up:","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#time-lag-search","title":"Time lag search","text":"<p>See references for more information.</p> <p>Name: LAGR2. LAGHANDL. </p> <p>Date: 2024.03.21., 26., 27.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  analysis:\n    cmd: python3 -u time-lag-search-analysis.py\n    deps:\n      - time-lag-search-analysis.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n      - rrr\n      - rrr-param-search\n      - best-rrr-params\n    outs:\n      - results/time-lag-search.pickle\n  plots:\n    cmd: python3 -u time-lag-search-plot.py\n    deps:\n      - time-lag-search-plot.py\n      - results/time-lag-search.pickle\n    params:\n      - rrr-param-search\n    outs:\n      - figures/time-lag-search.png\n</code></pre> <p>Session: \u202640, \u202684, \u202674, \u202603</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#top-down_2","title":"Top-down","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#bottom-up_2","title":"Bottom-up","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#bottom-up-lag","title":"Bottom-up lag","text":"<p>Session: 1108334384</p> <p>Lag: 4</p> <p></p> <p>Session: 1112302803</p> <p>Lag: 4</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#top-down-lag-exploration","title":"Top-down lag exploration","text":"<p>Session: 1111013640</p> <p>Lag: 4 ms</p> <p></p> <p>Session: 1108334384</p> <p>Lag: 3 ms</p> <p></p> <p>Session: 1112515874</p> <p>Lag: 5 ms</p> <p></p> <p>Session: 1112302803</p> <p>Lag: 2 ms, 7 ms</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v2-v1-interaction-rank","title":"V2-V1 interaction rank","text":"<p>Name: TDTIME, BUTIME, TDRANKTIME, BURANKTIME</p> <p>Date: 2024.03.19.-20.</p> <ul> <li>rank-along-time pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  analysis:\n    cmd: python3 -u rank-along-time-analysis.py\n    deps:\n      - rank-along-time-analysis.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n      - rrr\n      - rrr-param-search\n    outs:\n      - results/max-ranks.pickle\n  plots:\n    cmd: python3 -u rank-along-time-plot.py\n    deps:\n      - rank-along-time-plot.py\n      - results/max-ranks.pickle\n    params:\n      - preprocess\n    outs:\n      - figures/max-ranks.png\n</code></pre> <p>Session: 1109889304 (100 ms</p> <p>top-down</p> <p></p> <p>bottom-up</p> <p></p> <p>Session: 1108334384 (100 ms, cv2, rank14, lag0)</p> <p>top-down</p> <p></p> <p></p> <p>bottom-up</p> <p></p> <p></p> <p>Session: 1112515874</p> <p>top-down</p> <p></p> <p></p> <p>bottom-up</p> <p></p> <p></p> <p>Session: 1111013640</p> <p>top-down</p> <p></p> <p></p> <p>bottom-up</p> <p></p> <p></p> <p>What we see:</p> <ul> <li>peak is at 0.05 s both top-down and bottom-up</li> <li>there is a second smaller peak right before 0.15 s, or slopy descend</li> </ul> <p>Question:</p> <ul> <li>What is the geometry at the first and second peak? Similar or dissimilar?</li> </ul> <p>Gerg\u0151:</p> <ul> <li>causality is can only be said by positive time lag!</li> <li>the low-dimensional correlation at the 0 and 6 ms lag is the same or not? \u2192 cross test between 0 and 6 ms lag</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#multiple-session-param-search","title":"Multiple session param search!!!","text":"<p>See references for more intormation.</p> <p>name: TOPDOWN</p> <p>date: 2024.03.18.-19., 04.12.</p> <ul> <li>pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  analysis:\n    cmd: python3 -u cv-time-lag-analysis.py\n    deps:\n      - cv-time-lag-analysis.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n      - rrr\n      - rrr-param-search\n    outs:\n      - results/CV-lag-time.pickle\n  plots:\n    cmd: python3 -u cv-time-lag-plot.py\n    deps:\n      - cv-time-lag-plot.py\n      - results/CV-lag-time.pickle\n    params:\n      - preprocess\n      - rrr\n      - rrr-param-search\n    outs:\n      - figures/rrr-param-search.png\n</code></pre> <p>params:</p> <pre><code>- time-step 0.100\n- time-window 0.100\n</code></pre> <pre><code>preprocess:\n  step-size: 0.100 # time-step in seconds # 0.010\n  bin-size: 0.100 # time-window in seconds # Joao's default: 0.100\n  stimulus-duration: 0.250 # in seconds\nrrr-param-search:\n  cv: [2,3,4,5]\n  rank: [2,4,6,8,10,12,14,16,18,20]\n  lag: [0,3,6]\n  timepoints: [0,100]\n</code></pre> <ul> <li> <p>results</p> </li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1059678195","title":"~~1059678195~~","text":"<p>maximum value(0.13) at 0 ms is at cv=4, lag=0, rank=14 maximum value(0.13) at 100 ms is at cv=4, lag=0, rank=14</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1v2","title":"V1\u2192V2","text":"<p>All-NaN slice encountered</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1108334384-11-15","title":"1108334384 (11-15%)","text":"<p>maximum value(0.111) at 0 ms is at cv=2, lag=0, rank=14 maximum value(0.111) at 100 ms is at cv=2, lag=0, rank=14</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1v2_1","title":"V1\u2192V2","text":"<p>maximum value(0.158) at 0 ms is at cv=3, lag=0, rank=18 maximum value(0.158) at 100 ms is at cv=3, lag=0, rank=18</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1108531612","title":"~~1108531612~~","text":"<p>Spike count in the data: 0 (0, 4803, 250) Spike count in the data: 1209114 (150, 4803, 250)</p> <p>All-NaN</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1109889304-12-18","title":"1109889304 (12-18%)","text":"<p>Spike count in the data: 984080 (131, 4804, 250) Spike count in the data: 965488 (93, 4804, 250)</p> <p></p> <p>maximum value(0.181) at 0 ms is at cv=5, lag=0, rank=18 maximum value(0.181) at 100 ms is at cv=5, lag=0, rank=18</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#window-size-50-ms","title":"window size 50 ms","text":"<p>maximum value(0.125) at 0 ms is at cv=3, lag=0, rank=12 maximum value(0.125) at 100 ms is at cv=3, lag=0, rank=12</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1v2_2","title":"V1\u2192V2","text":"<p>maximum value(0.218) at 0 ms is at cv=2, lag=3, rank=8 maximum value(0.218) at 100 ms is at cv=2, lag=3, rank=8</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1111013640-10-12","title":"1111013640 (10-12%)","text":"<p>Spike count in the data: 1246081 (147, 4800, 250) Spike count in the data: 952839 (129, 4800, 250)</p> <p></p> <p>maximum value(0.125) at 0 ms is at cv=2, lag=0, rank=16 maximum value(0.125) at 100 ms is at cv=2, lag=0, rank=16</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1v2_3","title":"V1\u2192V2","text":"<p>maximum value(0.097) at 0 ms is at cv=2, lag=0, rank=16 maximum value(0.097) at 100 ms is at cv=2, lag=0, rank=16</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1111216934","title":"~~1111216934~~","text":"<p>Spike count in the data: 930540 (110, 4796, 250) Spike count in the data: 475659 (62, 4796, 250)</p> <p></p> <p>maximum value(0.114) at 0 ms is at cv=2, lag=0, rank=12 maximum value(0.114) at 100 ms is at cv=2, lag=0, rank=12</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1v2_4","title":"V1\u2192V2","text":"<p>All-NaN slice encountered</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1112302803-8","title":"1112302803 (8%)","text":"<p>Spike count in the data: 954940 (121, 4798, 250) Spike count in the data: 1207317 (137, 4798, 250)</p> <p></p> <p>maximum value(0.075) at 0 ms is at cv=3, lag=0, rank=14 maximum value(0.075) at 100 ms is at cv=3, lag=0, rank=14</p> <p></p> <p>maximum value(0.089) at 0 ms is at cv=2, lag=0, rank=14 maximum value(0.089) at 100 ms is at cv=2, lag=0, rank=14</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1112515874-6-16","title":"1112515874 (6-16%)","text":"<p>Spike count in the data: 336822 (28, 4796, 250) Spike count in the data: 554072 (65, 4796, 250)</p> <p></p> <p>maximum value(0.164) at 0 ms is at cv=3, lag=6, rank=12 maximum value(0.164) at 100 ms is at cv=3, lag=6, rank=12</p> <p></p> <p>maximum value(0.063) at 0 ms is at cv=2, lag=0, rank=12 maximum value(0.063) at 100 ms is at cv=2, lag=0, rank=12</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1113751921-8-13","title":"1113751921 (8-13%)","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1lm","title":"V1\u2192LM","text":"<p>maximum value(0.085) at 0 ms is at cv=2, lag=0, rank=10 maximum value(0.085) at 100 ms is at cv=2, lag=0, rank=10</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#lm-v1","title":"LM \u2192V1","text":"<p>maximum value(0.138) at 0 ms is at cv=4, lag=0, rank=20 maximum value(0.138) at 100 ms is at cv=4, lag=0, rank=20</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1113957627-12","title":"1113957627 (12%)","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#lm-v1_1","title":"LM \u2192 V1","text":"<p>maximum value(0.119) at 0 ms is at cv=3, lag=0, rank=10 maximum value(0.119) at 100 ms is at cv=3, lag=0, rank=10</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1-lm-all-failed","title":"V1 \u2192 LM all failed","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1115077618-there-is-no-spike-in-visl","title":"1115077618: There is no spike in VISl","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1115356973","title":"1115356973","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1-lm-all-failed_1","title":"V1 \u2192 LM all failed","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1118324999-18","title":"1118324999: 18%","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1-lm","title":"V1 \u2192 LM","text":"<p>maximum value(0.18) at 0 ms is at cv=4, lag=0, rank=16 maximum value(0.18) at 100 ms is at cv=4, lag=0, rank=16</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#lm-v1-all-failed","title":"LM \u2192 V1: all failed","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#1118512505-28","title":"1118512505: 28%","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#lm-v1_2","title":"LM \u2192 V1","text":"<p>maximum value(0.278) at 0 ms is at cv=4, lag=0, rank=16 maximum value(0.278) at 100 ms is at cv=4, lag=0, rank=16</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v1-lm-all-failed_2","title":"V1 \u2192 LM: all failed","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#sessions","title":"Sessions","text":"<p>All ecephys sessions had this issue except those listed here: ~~1059678195~~, 1108334384, ~~1108531612~~, ~~1109680280~~, 1109889304, 1111013640, ~~1111216934~~, 1112302803, 1112515874, 1113751921, 1113957627, 1115077618, 1115356973, 1118324999, 1118512505</p> performance top-down bottom-up time lag 1111013640 10-12% cv=2, rank=16 cv=2, rank=16 4 1108334384 11-15% cv=2, rank=14 cv=3, rank=18 3 1112515874 6-16% cv=3, lag=6, rank=12 cv=2, rank=12 5 1112302803 8% cv=3, rank=14 cv=2, rank=14 2 1109889304 18-21% (NO 100ms) cv=5, rank=18 cv=2, lag=3, rank=8 6"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#v2-to-v1","title":"V2 to V1","text":"<p>Date: 2024.02.28.</p> <p>Session: 1064644573</p> <pre><code>cv: [2,3,4,5]\nrank: [2,4,6,8,10,12,14,16,18,20]\nlag: [0,3,6]\n</code></pre> <p>All model failed (both timewindow 50 and 100</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#rrr-score-by-time","title":"RRR score by time","text":"<p>See references for more information.</p> <p>Name: RRRSCORETIME.</p> <p>Date: 2024.02.27.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - preprocessing.py\n      - data/raw-area-responses/\n    params:\n      - preprocess\n    outs:\n      - data/area-responses/\n  analysis:\n    cmd: python3 -u rrr-score-time.py\n    deps:\n      - rrr-score-time.py\n      - data/area-responses/\n    params:\n      - preprocess\n      - rrr\n    outs:\n      - results/rrr-score-time.pickle\n  plots:\n    cmd: python3 -u rrr-score-time-plot.py\n    deps:\n      - rrr-score-time-plot.py\n      - results/rrr-score-time.pickle\n    params:\n      - preprocess\n      - rrr\n    outs:\n      - figures/rrr-score-time.png\n</code></pre> <p>Params:</p> <pre><code>- Time window: 50 ms\n- Time step: 10 ms\n</code></pre> <p>Results:</p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#bootstrap-above-cv-folds","title":"Bootstrap above cv folds","text":"<p>Date: 2024.02.27.</p> <ul> <li>Code</li> </ul> <pre><code>cv=2\nrank=8\nt=1\n\n# Define the cross-validation strategy\ncv_gen = ShuffleSplit(n_splits=10, test_size=1/cv, random_state=0)\n\n# Run cross-validation\nscores = RRRR(X[:,:,t].T, Y[:,:,t].T, rank=rank, cv=cv_gen, log=False)\n\n# Mean score\n# print('Mean score:', np.mean(scores['test_score']))\n\n# Print the scores of the different cross-validation folds\nprint(scores['test_score'])\n</code></pre> <p>Params:</p> <ul> <li>time window: 50 ms</li> </ul> <p>Result:</p> <pre><code>[0.14546037 0.1439828  0.1444211  0.14555954 0.14235154 0.14418097\n0.14210302 0.14476292 0.14729195 0.14400448]\n</code></pre> <p>Interpretation: CV=2 is OK.</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#cv-timelag-timepoint-exploration","title":"CV, timelag, timepoint exploration","text":"<p>Name: CVLAGTIME-ANALYSIS. CVLAGRANKTIME.</p> <p>Date: 2024.02.26-27.</p> <ul> <li>Pipeline</li> </ul> <pre><code>\nstages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  analysis:\n    cmd: python3 -u cv-time-lag-analysis.py\n    deps:\n      - cv-time-lag-analysis.py\n      - data/raw-area-responses/\n    params:\n      - load\n      - preprocess\n      - rrr\n      - rrr-param-search\n    outs:\n      - results/CV-lag-time.pickle\n  plots:\n    cmd: python3 -u cv-time-lag-plot.py\n    deps:\n      - cv-time-lag-plot.py\n      - results/CV-lag-time.pickle\n    params:\n      - preprocess\n      - rrr\n      - rrr-param-search\n    outs:\n      - figures/CV-timelag.png\n</code></pre> <p>Params:</p> <pre><code>- rank: 8\n- time window: 50 ms\n</code></pre> <p>Results:</p> <ul> <li> <p>Lag 0, 2, 4, 6</p> <p></p> <p>Interpretation:</p> <ul> <li>time lag &lt; 6 ms</li> <li>At time <code>50-100 ms</code>, the optimal lag is <code>0 ms</code><ul> <li>The second best is <code>2 ms</code> lag at time <code>50-100 ms</code> and <code>150-200 ms</code></li> </ul> </li> <li>On CV <code>2 folds</code>, the optimal Lag is <code>2, 4 ms</code><ul> <li>The second best is <code>2 ms</code> at CV <code>4 folds</code></li> </ul> </li> <li>Lag 0, 1, 2, 3</li> </ul> <p></p> <p>Interpretation:</p> <ul> <li>At time <code>50-100 ms</code>, the optimal lag is <code>0 ms</code><ul> <li>The second best is <code>1 ms</code> lag at time <code>50-100 ms</code></li> </ul> </li> <li>On CV <code>2 folds</code>, the optimal Lag is  <code>1, 2 ms</code><ul> <li>The second best is <code>0 ms</code> at CV <code>2 folds</code></li> </ul> </li> </ul> </li> </ul> <p>maximum value (0.081) is at cv=4, lag=0, time=50</p> <p>maximum value (0.085) is at cv=4, lag=0, rank=12, time=50ms</p> <p>maximum value (0.086) is at cv=4, lag=0, rank=11, time=50ms</p> <ul> <li>all succesful model</li> </ul> <pre><code>lag 0:\nCV: 2, Rank: 8, Mean test score: 0.03201875490380125\nCV: 2, Rank: 8, Mean test score: 0.049927248709457306\nCV: 2, Rank: 10, Mean test score: 0.022322323390729783\nCV: 2, Rank: 10, Mean test score: 0.05349610883936824\nCV: 2, Rank: 11, Mean test score: 0.05315925285586909\nCV: 2, Rank: 12, Mean test score: 0.052840592630864505\nCV: 2, Rank: 13, Mean test score: 0.05191762385869492\nCV: 2, Rank: 14, Mean test score: 0.05184995660560535\nCV: 3, Rank: 8, Mean test score: 0.07135646956690475\nCV: 3, Rank: 8, Mean test score: 0.05719398436702671\nCV: 3, Rank: 8, Mean test score: 0.02339548877129166\nCV: 3, Rank: 10, Mean test score: 0.0633966003181519\nCV: 3, Rank: 10, Mean test score: 0.06005307370804186\nCV: 3, Rank: 10, Mean test score: 0.023503122594245005\nCV: 3, Rank: 11, Mean test score: 0.06317631165339903\nCV: 3, Rank: 11, Mean test score: 0.06112536866824452\nCV: 3, Rank: 11, Mean test score: 0.023802242484427746\nCV: 3, Rank: 12, Mean test score: 0.06597397657250023\nCV: 3, Rank: 12, Mean test score: 0.06180256381254188\nCV: 3, Rank: 12, Mean test score: 0.023333892825463994\nCV: 3, Rank: 13, Mean test score: 0.06460023900401318\nCV: 3, Rank: 13, Mean test score: 0.06318522145727225\nCV: 3, Rank: 13, Mean test score: 0.022124711238206234\nCV: 3, Rank: 14, Mean test score: 0.06384509391383798\nCV: 3, Rank: 14, Mean test score: 0.06336658862346407\nCV: 3, Rank: 14, Mean test score: 0.022174089424562785\nCV: 4, Rank: 8, Mean test score: 0.08053730811603779\nCV: 4, Rank: 8, Mean test score: 0.048252551269832356\nCV: 4, Rank: 8, Mean test score: 0.019233428702738354\nCV: 4, Rank: 10, Mean test score: 0.08269683049663624\nCV: 4, Rank: 10, Mean test score: 0.052297734551015024\nCV: 4, Rank: 10, Mean test score: 0.019285387583393654\nCV: 4, Rank: 11, Mean test score: 0.08597487376314111\nCV: 4, Rank: 11, Mean test score: 0.053008574859848986\nCV: 4, Rank: 11, Mean test score: 0.019259621346014142\nCV: 4, Rank: 12, Mean test score: 0.08548234358472524\nCV: 4, Rank: 12, Mean test score: 0.052974732186628674\nCV: 4, Rank: 12, Mean test score: 0.017439772376426382\nCV: 4, Rank: 13, Mean test score: 0.08515425098552482\nCV: 4, Rank: 13, Mean test score: 0.05306204079328371\nCV: 4, Rank: 13, Mean test score: 0.016415278431215635\nCV: 4, Rank: 14, Mean test score: 0.08509384785496672\nCV: 4, Rank: 14, Mean test score: 0.052844715308502946\nCV: 4, Rank: 14, Mean test score: 0.0156077599192398\nCV: 5, Rank: 8, Mean test score: 0.07161593565637613\nCV: 5, Rank: 8, Mean test score: 0.01480060916855867\nCV: 5, Rank: 10, Mean test score: 0.0710629204210168\nCV: 5, Rank: 10, Mean test score: 0.012543656424523028\nCV: 5, Rank: 11, Mean test score: 0.07501833950350371\nCV: 5, Rank: 11, Mean test score: 0.012510782472159792\nCV: 5, Rank: 12, Mean test score: 0.07537728842402695\nCV: 5, Rank: 12, Mean test score: 0.012168573006751008\nCV: 5, Rank: 13, Mean test score: 0.0759284833725914\nCV: 5, Rank: 13, Mean test score: 0.011805850439297164\nCV: 5, Rank: 14, Mean test score: 0.07471379946251691\nCV: 5, Rank: 14, Mean test score: 0.010920716345304088\nCV: 6, Rank: 8, Mean test score: 0.05463846503412088\nCV: 6, Rank: 10, Mean test score: 0.05697794981796168\nCV: 6, Rank: 11, Mean test score: 0.059212838407945334\nCV: 6, Rank: 12, Mean test score: 0.058260540074825284\nCV: 6, Rank: 13, Mean test score: 0.05857791379639099\nCV: 6, Rank: 14, Mean test score: 0.05268225272107676\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#more-params","title":"More params","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#less-params-1","title":"Less params #1","text":"<p>maximum value (0.033) is at cv=3, lag=0, rank=10, time=100ms</p> <p></p> <p>Interpretation:</p> <ul> <li>Lag 0 is far better then other independent of rank</li> <li>Rank 10 is the best.</li> <li>CV 3 (or 2</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#less-params-2","title":"Less params #2","text":"<p>maximum value (0.033) is at cv=3, lag=0, rank=9, time=100ms</p> <ul> <li>Time window 25 ms:</li> </ul> <pre><code>maximum value(0.044) at 50 ms is at cv=2, lag=0, rank=9\nmaximum value(0.044) at 75 ms is at cv=2, lag=0, rank=9\nmaximum value(0.044) at 100 ms is at cv=2, lag=0, rank=9\nmaximum value(0.044) at 125 ms is at cv=2, lag=0, rank=9\n</code></pre> <ul> <li>Time window 50 ms:</li> </ul> <pre><code>maximum value(0.086) at 50 ms is at cv=4, lag=0, rank=11\nmaximum value(0.086) at 100 ms is at cv=4, lag=0, rank=11\nmaximum value(0.086) at 150 ms is at cv=4, lag=0, rank=11\n</code></pre> <ul> <li>Time window 100 ms:</li> </ul> <pre><code>maximum value(0.085) at 0 ms is at cv=3, lag=0, rank=16\nmaximum value(0.085) at 100 ms is at cv=3, lag=0, rank=16\n</code></pre> <p>Interpretation:</p> <ul> <li>optimal time window: 50 ms</li> <li>time lag: 0 ms</li> <li>optimal CV fold: 4</li> <li>optimal rank: 11</li> </ul> <p>here the $r^2$ is 0.086</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#cv-rank-time-exploration","title":"CV-rank-time exploration","text":"<p>See references for more information.</p> <p>Date: 2024.02.21.</p> <p>Results:</p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#detailed","title":"Detailed","text":""},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#time-step","title":"Time step","text":"<p>Name: TIMESTEP, TIMESTEP2</p> <p>Date: 2024.02.20.</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#semedo","title":"Semedo","text":"<p>qualitatively similar results for a wide range of bin widths</p> <ul> <li>sliding time window</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#rank-analysis-from-v2-to-v1","title":"Rank analysis from V2 to V1","text":"<p>Date: 2024.02.20.</p> <p>Params:</p> <ul> <li>stimulus-block: 5</li> </ul> <p>Result: doesnt work for CV 2, 4</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#rank-analysis-for-gabor","title":"Rank analysis for gabor","text":"<p>Date: 2024.02.20.</p> <p>Params:</p> <ul> <li>stimulus-block: 2</li> </ul> <p>Result: doesnt work for CV 2, 5, 10</p> <p>Interpretation: stimulus is too sparse</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#control-models","title":"Control models","text":"<p>See references for more information.</p> <p>Name: RRRCONTROL.</p> <p>Date: 2024.02.15., 20.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - preprocessing.py\n    params:\n      - preprocess\n    outs:\n      - data/area-responses/\n  behav-preprocess:\n    cmd: python3 -u behav-preprocessing.py\n    deps:\n      - behav-preprocessing.py\n    params:\n      - preprocess\n    outs:\n      - data/behav-responses/\n  analysis:\n    cmd: python3 -u control-models.py\n    deps:\n      - control-models.py\n      - data/area-responses/\n      - data/behav-responses/\n    params:\n      - preprocess\n      - rrr\n    outs:\n      - figures/control-models.png\n</code></pre> <p>Params:</p> <pre><code>- stimulus block: 5 # passive replay\n- rank: 8\n</code></pre> <ul> <li> <p>Results</p> <p></p> <p>cv folds: 2</p> <p></p> <p>cv folds: 3</p> <p></p> <p>cv folds: 4</p> <p></p> <p>cv folds: 5</p> </li> </ul> <p>Interpretation:</p> <p>does the normaliyation give us indistinguishable values for movement and spiking? print these values .</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#rank-optimization","title":"Rank optimization","text":"<p>See references for more information.</p> <p>Name: RANKOPT. RANKOPTTIME. RANKOPTCV.</p> <p>Date: 2024.02.13., 17-21.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - preprocessing.py\n    params:\n      - cache\n      - preprocess\n    outs:\n      - data/area-responses/\n  analysis:\n    cmd: python3 -u rrr-rank-analysis.py\n    deps:\n      - rrr-rank-analysis.py\n      - data/area-responses/\n    params:\n      - preprocess\n      - rrr\n    outs:\n      - results/VISp_VISl_cross-time-test-scores.pickle\n  plots:\n    cmd: python3 -u rrr-rank-plot.py\n    deps:\n      - rrr-rank-plot.py\n      - results/VISp_VISl_cross-time-test-scores.pickle\n    params:\n      - preprocess\n      - rrr\n      - rrr-plot\n    outs:\n      - figures/V1-V2_cross-time_RRR-rank-analysis-2DIM.png\n      - figures/V1-V2_cross-time_RRR-rank-analysis-timewise.png\n      - figures/V1-V2_cross-time_RRR-rank-analysis-averaged-over-time.png\n</code></pre> <p>Params:</p> <pre><code>- stimulus block: 5 (passive replay)\n- CV: 5\n</code></pre> <p>Results:</p> <ul> <li> <p>minmax score</p> <p></p> <p></p> <p>Optimal rank: 8</p> <p>Leave out / discard first 500 ms (see also):</p> <p></p> <p></p> <p>Optimal rank: 41</p> </li> </ul> <p>z-score</p> <ul> <li> <p>egyes\u00e9vel\u2026</p> <p>cv folds: 3</p> <p></p> <p></p> <p>Optimal rank for 0-50 ms: 1 Optimal rank for 50-100 ms: 8 Optimal rank for 100-150 ms: 14 Optimal rank for 150-200 ms: 3 Optimal rank for 200-250 ms: 8</p> <p></p> <p>Optimal rank over time: 7</p> <p>cv folds: 5</p> <p></p> <p></p> <p>Optimal rank for 50-100 ms: 8 Optimal rank for 100-150 ms: 1 (negative!!! Optimal rank for 150-200 ms: 8 Optimal rank for 200-250 ms: 1 (negative!!!</p> </li> </ul> <p>step size: 50 ms time window: 25 ms</p> <p></p> <p></p> <p></p> <p>Optimal rank: 12 cv of the optimal rank: 3</p> <p>step size: 25 ms time window: 25 ms</p> <p></p> <p></p> <p></p> <p>Optimal rank: 27 ? cv of the optimal rank: 2</p> <p></p> <p>Conclusions:</p> <ul> <li>Optimal rank is 8.</li> <li>Probably we have to make the movement control first</li> <li>negative values mean that the model is wrong\u2026</li> <li>greater ranks perform worse bcs overfitting? so there are times when we cannot predict V2 from V1.</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#behav-data","title":"Behav data","text":"<p>stimulus-duration: 250 ms (movement!)</p> <p>at Joao Semedo: 1250 ms (not contaminated by movement)</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#github-codes","title":"GitHub codes","text":"<p>generate figures</p> <p>preprocessing and unit metrics</p> <p>spike sorting</p> <p>calculating stimulus metrics</p> <p>data acquisition/plugin GUI</p> <p>data acquisition/neuropixels3a</p> <p>data acquisition/neuropixelsPXI</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#time-lag-between-v1-and-v2","title":"Time lag between V1 and V2","text":"<p>See references for more information.</p> <p>Name: CROSSTIMELAG. CROSSTIMECOEFFS.</p> <p>Date: 2024.02.08., 22.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - data/raw-area-responses/\n      - preprocessing.py\n    params:\n      - cache\n      - preprocess\n    outs:\n      - data/area-responses/\n  analysis:\n    cmd: python3 -u time_lag_analysis.py\n    deps:\n      - data/area-responses/\n      - time_lag_analysis.py\n    params:\n      - preprocess\n      - rrr\n    outs:\n      - results/VISp_VISl_cross-time-coeffs.pickle\n      - results/VISp_VISl_cross-time-lag.pickle\n  plots:\n    cmd: python3 -u time_lag_plot.py\n    deps:\n      - results/VISp_VISl_cross-time-coeffs.pickle\n      - time_lag_plot.py\n    params:\n      - preprocess\n    outs:\n      - figures/Time_lag_between_V1_LM.png\n</code></pre> <p>Motivation: What is the time lag between the V1 and LM?</p> <ul> <li>Expected: 30-50 ms? (Marci)</li> <li>Will the time-binning effect on the V1-LM correlation analysis?</li> <li>Data exploration</li> </ul> <p>Method:</p> <ul> <li>RRR analysis on V1 and LM activity averaged over neurons</li> </ul> <p>Params:</p> <ul> <li>bin size: 5 ms</li> <li>stimulus block: 5 (passive replay)</li> <li>RRR rank: 10</li> <li>CV: 5</li> </ul> <p>Results:</p> <ul> <li>Time lag between V1 and V2: ~5 ms</li> </ul> <p></p> <pre><code>Cross-validation scores: [-1.62523803e+28 -2.47969342e+28 -1.52222458e+28]\n</code></pre> <p>With <code>CV=[2,3,4,5,6]</code> and <code>ranks=[3,4,5,6,7,8,9,10,11,12,13,14]</code> the Max score is NaN:</p> <pre><code>stages:\n  load:\n    cmd: python3 -u load_raw_activity_data.py\n    deps:\n      - load_raw_activity_data.py\n    params:\n      - load\n    outs:\n      - data/raw-area-responses/\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    deps:\n      - data/raw-area-responses/\n      - preprocessing.py\n    params:\n      - cache\n      - preprocess\n    outs:\n      - data/area-responses/\n  analysis:\n    cmd: python3 -u cv-rank-cross-time.py\n    deps:\n      - data/area-responses/\n      - cv-rank-cross-time.py\n    params:\n      - preprocess\n      - rrr\n    outs:\n      - figures/CV-rank_cross-time.png\n</code></pre> <p>What we see: </p> <ul> <li>The correlation starts at 35-55 ms<ul> <li>Interpretation: stim reach V1</li> </ul> </li> <li>What is that black thing after 100 ms? The activity of LM becomes more sophisticated and less dependent on the V1?<ul> <li>Probably the top-down effect? (bcs here we predicted V2 from V1)</li> </ul> </li> </ul> <p>Interpretation: not usable!</p> <p>What are the others saying:</p> <p>Siegle, 2021 (below):</p> <ul> <li>An example cross-area \u2018sharp peak\u2019 spiking interaction between a pair of units in V1 and LM (Fig2c)</li> <li>Distribution of CCG peak time lags between V1 and LM in one example mouse. The median is 3.9 ms (Fig2d)</li> </ul> <p>Gerg\u0151:</p> <ul> <li>time lag over time</li> </ul> <p>Whats next:</p> <ol> <li>read Joao Semedo</li> <li>~~behav data into notion~~</li> <li>~~choose rank by calculating reconstruction error?~~</li> <li>~~Marci controls (R^2, Corr)~~</li> <li>~~VISp\u2192VISl,  VISl\u2192VISp~~</li> <li>change task (motor contam) OR movie</li> <li>(Bernstein)</li> </ol>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#siegle-2021-on-this-data","title":"Siegle, 2021 on this data","text":"<p>Survey of spiking in the mouse visual system reveals functional hierarchy</p> <p></p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#areas","title":"Areas","text":"<p>Added VISpm (MT) and VISm (motion area) to analyses for video stimuli.</p> <p>Date: 2024.02.06.</p> <pre><code>VISp:   V1\nVISpl:  ?\nVISli:  ? (visually guided behav)\nVISl:   V2 (LM)?\nVISal:  anteromedial cuneus\nVISlla: ?\nVISrl:  ?(visually guided behav)\nVISam:  anteromedial cuneus\nVISpm:  V4-V5(MT), PMC (associative area)\nVISm:   V6 (medial motion area)\n</code></pre>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#data-distribution","title":"Data distribution","text":"<p>See references for more information.</p> <p>Date: 2024.02.06.</p> <p>Method: Shapiro-Wilk test.</p> <p>Params:</p> <ul> <li>stimulus block: 5 (passive replay)</li> <li>area: V1</li> </ul> <p>Result:</p> <ul> <li>The data does not look Gaussian nor per unit, nor per timebin</li> </ul> <pre><code>Statistics=0.601, p=0.000\nSample does not look Gaussian (reject H0)\n</code></pre> <pre><code>Statistics=0.538, p=0.000\nActivity does not look Gaussian in time 0 (reject H0)\nStatistics=0.632, p=0.000\nActivity does not look Gaussian in time 1 (reject H0)\nStatistics=0.614, p=0.000\nActivity does not look Gaussian in time 2 (reject H0)\nStatistics=0.607, p=0.000\nActivity does not look Gaussian in time 3 (reject H0)\nStatistics=0.608, p=0.000\nActivity does not look Gaussian in time 4 (reject H0)\n</code></pre> <pre><code>Statistics=0.573, p=0.000\nActivity of 0. unit does not look Gaussian (reject H0)\nStatistics=0.102, p=0.000\nActivity of 1. unit does not look Gaussian (reject H0)\nStatistics=0.794, p=0.000\nActivity of 2. unit does not look Gaussian (reject H0)\nStatistics=0.090, p=0.000\nActivity of 3. unit does not look Gaussian (reject H0)\nStatistics=0.119, p=0.000\nActivity of 4. unit does not look Gaussian (reject H0)\nStatistics=0.082, p=0.000\nActivity of 5. unit does not look Gaussian (reject H0)\nStatistics=0.064, p=0.000\nActivity of 6. unit does not look Gaussian (reject H0)\nStatistics=0.402, p=0.000\nActivity of 7. unit does not look Gaussian (reject H0)\nStatistics=0.510, p=0.000\nActivity of 8. unit does not look Gaussian (reject H0)\nStatistics=0.248, p=0.000\nActivity of 9. unit does not look Gaussian (reject H0)\nStatistics=0.382, p=0.000\nActivity of 10. unit does not look Gaussian (reject H0)\nStatistics=0.311, p=0.000\nActivity of 11. unit does not look Gaussian (reject H0)\nStatistics=0.076, p=0.000\nActivity of 12. unit does not look Gaussian (reject H0)\nStatistics=0.428, p=0.000\nActivity of 13. unit does not look Gaussian (reject H0)\nStatistics=0.693, p=0.000\nActivity of 14. unit does not look Gaussian (reject H0)\nStatistics=0.250, p=0.000\nActivity of 15. unit does not look Gaussian (reject H0)\nStatistics=0.553, p=0.000\nActivity of 16. unit does not look Gaussian (reject H0)\nStatistics=0.113, p=0.000\nActivity of 17. unit does not look Gaussian (reject H0)\nStatistics=0.705, p=0.000\nActivity of 18. unit does not look Gaussian (reject H0)\nStatistics=0.049, p=0.000\nActivity of 19. unit does not look Gaussian (reject H0)\nStatistics=0.084, p=0.000\nActivity of 20. unit does not look Gaussian (reject H0)\nStatistics=0.223, p=0.000\nActivity of 21. unit does not look Gaussian (reject H0)\nStatistics=0.493, p=0.000\nActivity of 22. unit does not look Gaussian (reject H0)\nStatistics=0.320, p=0.000\nActivity of 23. unit does not look Gaussian (reject H0)\nStatistics=0.850, p=0.000\nActivity of 24. unit does not look Gaussian (reject H0)\nStatistics=0.397, p=0.000\nActivity of 25. unit does not look Gaussian (reject H0)\nStatistics=0.710, p=0.000\nActivity of 26. unit does not look Gaussian (reject H0)\nStatistics=0.362, p=0.000\nActivity of 27. unit does not look Gaussian (reject H0)\nStatistics=0.599, p=0.000\nActivity of 28. unit does not look Gaussian (reject H0)\nStatistics=0.543, p=0.000\nActivity of 29. unit does not look Gaussian (reject H0)\nStatistics=0.353, p=0.000\nActivity of 30. unit does not look Gaussian (reject H0)\nStatistics=0.429, p=0.000\nActivity of 31. unit does not look Gaussian (reject H0)\nStatistics=0.562, p=0.000\nActivity of 32. unit does not look Gaussian (reject H0)\nStatistics=0.347, p=0.000\nActivity of 33. unit does not look Gaussian (reject H0)\nStatistics=0.599, p=0.000\nActivity of 34. unit does not look Gaussian (reject H0)\nStatistics=0.299, p=0.000\nActivity of 35. unit does not look Gaussian (reject H0)\nStatistics=0.682, p=0.000\nActivity of 36. unit does not look Gaussian (reject H0)\nStatistics=0.590, p=0.000\nActivity of 37. unit does not look Gaussian (reject H0)\nStatistics=0.834, p=0.000\nActivity of 38. unit does not look Gaussian (reject H0)\nStatistics=0.482, p=0.000\nActivity of 39. unit does not look Gaussian (reject H0)\nStatistics=0.364, p=0.000\nActivity of 40. unit does not look Gaussian (reject H0)\nStatistics=0.619, p=0.000\nActivity of 41. unit does not look Gaussian (reject H0)\nStatistics=0.684, p=0.000\nActivity of 42. unit does not look Gaussian (reject H0)\nStatistics=0.696, p=0.000\nActivity of 43. unit does not look Gaussian (reject H0)\nStatistics=0.695, p=0.000\nActivity of 44. unit does not look Gaussian (reject H0)\nStatistics=0.673, p=0.000\nActivity of 45. unit does not look Gaussian (reject H0)\nStatistics=0.331, p=0.000\nActivity of 46. unit does not look Gaussian (reject H0)\nStatistics=0.733, p=0.000\nActivity of 47. unit does not look Gaussian (reject H0)\nStatistics=0.620, p=0.000\nActivity of 48. unit does not look Gaussian (reject H0)\nStatistics=0.347, p=0.000\nActivity of 49. unit does not look Gaussian (reject H0)\nStatistics=0.604, p=0.000\nActivity of 50. unit does not look Gaussian (reject H0)\nStatistics=0.321, p=0.000\nActivity of 51. unit does not look Gaussian (reject H0)\nStatistics=0.310, p=0.000\nActivity of 52. unit does not look Gaussian (reject H0)\nStatistics=0.585, p=0.000\nActivity of 53. unit does not look Gaussian (reject H0)\nStatistics=0.448, p=0.000\nActivity of 54. unit does not look Gaussian (reject H0)\nStatistics=0.411, p=0.000\nActivity of 55. unit does not look Gaussian (reject H0)\nStatistics=0.858, p=0.000\nActivity of 56. unit does not look Gaussian (reject H0)\nStatistics=0.466, p=0.000\nActivity of 57. unit does not look Gaussian (reject H0)\nStatistics=0.584, p=0.000\nActivity of 58. unit does not look Gaussian (reject H0)\nStatistics=0.328, p=0.000\nActivity of 59. unit does not look Gaussian (reject H0)\nStatistics=0.695, p=0.000\nActivity of 60. unit does not look Gaussian (reject H0)\nStatistics=0.341, p=0.000\nActivity of 61. unit does not look Gaussian (reject H0)\nStatistics=0.826, p=0.000\nActivity of 62. unit does not look Gaussian (reject H0)\nStatistics=0.696, p=0.000\nActivity of 63. unit does not look Gaussian (reject H0)\nStatistics=0.558, p=0.000\nActivity of 64. unit does not look Gaussian (reject H0)\nStatistics=0.474, p=0.000\nActivity of 65. unit does not look Gaussian (reject H0)\nStatistics=0.639, p=0.000\nActivity of 66. unit does not look Gaussian (reject H0)\nStatistics=0.304, p=0.000\nActivity of 67. unit does not look Gaussian (reject H0)\nStatistics=0.901, p=0.000\nActivity of 68. unit does not look Gaussian (reject H0)\nStatistics=0.770, p=0.000\nActivity of 69. unit does not look Gaussian (reject H0)\nStatistics=0.714, p=0.000\nActivity of 70. unit does not look Gaussian (reject H0)\nStatistics=0.720, p=0.000\nActivity of 71. unit does not look Gaussian (reject H0)\nStatistics=0.796, p=0.000\nActivity of 72. unit does not look Gaussian (reject H0)\nStatistics=0.779, p=0.000\nActivity of 73. unit does not look Gaussian (reject H0)\nStatistics=0.821, p=0.000\nActivity of 74. unit does not look Gaussian (reject H0)\nStatistics=0.890, p=0.000\nActivity of 75. unit does not look Gaussian (reject H0)\nStatistics=0.538, p=0.000\nActivity of 76. unit does not look Gaussian (reject H0)\nStatistics=0.790, p=0.000\nActivity of 77. unit does not look Gaussian (reject H0)\nStatistics=0.255, p=0.000\nActivity of 78. unit does not look Gaussian (reject H0)\nStatistics=0.860, p=0.000\nActivity of 79. unit does not look Gaussian (reject H0)\nStatistics=0.305, p=0.000\nActivity of 80. unit does not look Gaussian (reject H0)\nStatistics=0.477, p=0.000\nActivity of 81. unit does not look Gaussian (reject H0)\nStatistics=0.527, p=0.000\nActivity of 82. unit does not look Gaussian (reject H0)\nStatistics=0.605, p=0.000\nActivity of 83. unit does not look Gaussian (reject H0)\nStatistics=0.529, p=0.000\nActivity of 84. unit does not look Gaussian (reject H0)\nStatistics=0.897, p=0.000\nActivity of 85. unit does not look Gaussian (reject H0)\nStatistics=0.826, p=0.000\nActivity of 86. unit does not look Gaussian (reject H0)\nStatistics=0.548, p=0.000\nActivity of 87. unit does not look Gaussian (reject H0)\nStatistics=0.548, p=0.000\nActivity of 88. unit does not look Gaussian (reject H0)\nStatistics=0.654, p=0.000\nActivity of 89. unit does not look Gaussian (reject H0)\nStatistics=0.701, p=0.000\nActivity of 90. unit does not look Gaussian (reject H0)\nStatistics=0.743, p=0.000\nActivity of 91. unit does not look Gaussian (reject H0)\nStatistics=0.739, p=0.000\nActivity of 92. unit does not look Gaussian (reject H0)\nStatistics=0.268, p=0.000\nActivity of 93. unit does not look Gaussian (reject H0)\nStatistics=0.657, p=0.000\nActivity of 94. unit does not look Gaussian (reject H0)\nStatistics=0.928, p=0.000\nActivity of 95. unit does not look Gaussian (reject H0)\nStatistics=0.608, p=0.000\nActivity of 96. unit does not look Gaussian (reject H0)\nStatistics=0.613, p=0.000\nActivity of 97. unit does not look Gaussian (reject H0)\nStatistics=0.458, p=0.000\nActivity of 98. unit does not look Gaussian (reject H0)\nStatistics=0.505, p=0.000\nActivity of 99. unit does not look Gaussian (reject H0)\nStatistics=0.474, p=0.000\nActivity of 100. unit does not look Gaussian (reject H0)\nStatistics=0.646, p=0.000\nActivity of 101. unit does not look Gaussian (reject H0)\nStatistics=0.306, p=0.000\nActivity of 102. unit does not look Gaussian (reject H0)\nStatistics=0.684, p=0.000\nActivity of 103. unit does not look Gaussian (reject H0)\nStatistics=0.664, p=0.000\nActivity of 104. unit does not look Gaussian (reject H0)\nStatistics=0.720, p=0.000\nActivity of 105. unit does not look Gaussian (reject H0)\nStatistics=0.806, p=0.000\nActivity of 106. unit does not look Gaussian (reject H0)\nStatistics=0.845, p=0.000\nActivity of 107. unit does not look Gaussian (reject H0)\nStatistics=0.874, p=0.000\nActivity of 108. unit does not look Gaussian (reject H0)\nStatistics=0.328, p=0.000\nActivity of 109. unit does not look Gaussian (reject H0)\nStatistics=0.357, p=0.000\nActivity of 110. unit does not look Gaussian (reject H0)\nStatistics=0.417, p=0.000\nActivity of 111. unit does not look Gaussian (reject H0)\nStatistics=0.647, p=0.000\nActivity of 112. unit does not look Gaussian (reject H0)\nStatistics=0.576, p=0.000\nActivity of 113. unit does not look Gaussian (reject H0)\nStatistics=0.781, p=0.000\nActivity of 114. unit does not look Gaussian (reject H0)\nStatistics=0.840, p=0.000\nActivity of 115. unit does not look Gaussian (reject H0)\nStatistics=0.669, p=0.000\nActivity of 116. unit does not look Gaussian (reject H0)\nStatistics=0.785, p=0.000\nActivity of 117. unit does not look Gaussian (reject H0)\nStatistics=0.842, p=0.000\nActivity of 118. unit does not look Gaussian (reject H0)\n</code></pre> <p></p> <p>Conclusion: Use normalization instead of standardization</p> <p>Discussion:</p> <p>Marci (2024.02.14.): ITI mean + std, csak nem z-scorenak h\u00edvjuk, mert nem normal-distr data. \u2192 Normalize based on ITI activity?</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#pca","title":"PCA","text":"<p>See references for more information.</p> <p>Date: 2024.02.06.</p> <p>Params:</p> <ul> <li>stimulus block: 5 (natural images)</li> <li>area: V1, V2</li> </ul> <p>Results:</p> <p></p> <p></p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#rrr-analysis-along-time","title":"RRR analysis along time","text":"<p>See references for more information.</p> <p>Name: RRR ANALYSIS ALONG TIME.</p> <p>Date: 2024.01.30. Modified: 2024.01.31.</p> <ul> <li>Pipeline</li> </ul> <pre><code>stages:\n  preprocess:\n    cmd: python3 -u preprocessing.py\n    params:\n      - preprocess\n    outs:\n      - data/area-responses/\n  rrr_analysis:\n    cmd: python3 -u rrr_analysis.py\n    deps:\n      - data/area-responses/\n    params:\n      - cache\n      - rrr\n    outs:\n      - results/rrr_coefficients.pickle\n  plots:\n    cmd: python3 -u rrr_plot.py\n    deps:\n      - results/rrr_coefficients.pickle\n    params:\n      - rrr\n    outs:\n      - figures/VISl-VISp_block-2_rrr-coefficients_along_time.png\n</code></pre> <p>Params:</p> <ul> <li>CV: 5</li> <li>stimulus block: 2 (receptive field mapping by gabor stimuli) and 4 (full field flashes)</li> <li>rank: 10</li> <li>stimulus duration: 0.250 s (bcs this is the average time between stimuli)</li> <li>bin size: 0.050 s (imported from ACC analysis)</li> </ul> <p>Result:</p> <p>Gabor stimuli:</p> <p></p> <p>Gabor stimuli averaged:</p> <p></p> <p>Flashes:</p> <p></p> <p>Description:</p> <p>Interpretation:</p> <p>Questions:</p> <ul> <li>What is the difference if we see V1 level stimuli or higher order stimuli?<ul> <li>TODO: stimulus block 0/5 (change detection task)</li> </ul> </li> <li>How can we order the cells? Which cell has \u201cpredictive power\u201d\u2026?</li> <li>How to choose the rank? joao (mit mond pl a mi eg\u00e9r sparse adatunk). test likelihood maximalization (score) MSE (gyakori, de sajnos gyakran lapos. startler)<ol> <li>Cross-Validation: This is a common method where the data is split into a training set and a validation set. The rank\u00a0<code>r</code>\u00a0is chosen such that it minimizes the prediction error on the validation set.</li> <li>Information Criteria: Criteria like Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can be used. These criteria trade-off the goodness of fit of the model against the complexity (in terms of the rank\u00a0<code>r</code>).</li> <li>Eigenvalue Plot: In the context of RRR, one can plot the eigenvalues obtained from the eigenvector problem. If there is a clear \u2018elbow\u2019 or \u2018knee\u2019 in this plot (a point where the decrease in eigenvalues becomes less steep), this may be a good choice for\u00a0<code>r</code>.</li> <li>Domain Knowledge: If you have specific knowledge about the underlying structure of your data or the specific problem at hand, this might inform a good choice of\u00a0<code>r</code>.</li> </ol> </li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#me","title":"Me:","text":"<ul> <li>[ ] TODO: control drifts.</li> </ul> <p>change in spike frequency</p> <ul> <li>[x] TODO: VISp\u2192VISl,  VISl\u2192VISp~~</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#gergo","title":"Gergo:","text":"<ul> <li> <p>[x] TODO: residual sz\u00e1mil\u00e1s before the analysis.~~</p> </li> <li> <p>[ ] TODO: stim params here to notion. </p> </li> <li> <p>[ ] TODO: id\u0151 t\u00e1v k\u00e9t k\u00f6z\u00f6tt!!!</p> </li> </ul> <p>frame X repeat.</p> <p>change detection task: legyen kb 100f\u00e9le k\u00e9p 10 repeattel = 1000 trial</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#corr","title":"Corr:","text":"<p>later TODO: on movies (it has autocorrelation).</p> <p>Videok autocerrelatioja .h\u00e1ny ms time bin.</p> <p>vajon a video alatt t\u00f6bb  correlatio van (pixelwide/wise) mint sima k\u00e9pn\u00e9l?</p> <p>mozg\u00e1s vajon milyen korrel\u00e1ci\u00f3t ad? annak is van dimenzionalit\u00e1sa\u2026</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#marci_1","title":"Marci:","text":"<p>$$ Y_{V2} = W_{V1} X_{V1} + W_{move} X_{move} + W_{pupil} X_{pupil} + W_{V2} X_{V2} $$ k\u00fcl\u00f6nb\u00f6z\u0151 tagokat kihagyva (hold out)  $$ R^2 $$ -ek \u00f6sszehasonl\u00edt\u00e1sa</p> <p>Ezeket \u00fagy is \u00f6ssze lehet hasonl\u00edtani, hogy pl  $$ Corr = X_{V1} \\cdot X_{move} $$ for each term/model.</p> <p>time_binek elcs\u00fasztatva, esetleg optimaliz\u00e1lni V1\u2192V2 time-lagra (R^2 kirajzol\u00e1sa vagy rank kirajzol\u00e1sa)</p>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#references","title":"References:","text":"<p>What is the effect of motor movement on V1 and V2 activity:</p> <ul> <li>Christensen, 2022: Reduced neural activity but improved coding in rodent higher-order visual cortex during locomotion</li> <li>Musall, 2019: Single-trial neural dynamics are dominated by richly varied movements</li> </ul>"},{"location":"notion/Allen%20project%20d3cfe5aab8384495b58fba8a47eeadcc/#future-plan","title":"Future plan:","text":"<ol> <li>read joao</li> <li>~~z-score~~</li> <li>~~behav data into notion~~</li> <li>~~residual~~</li> <li>~~time alignment~~</li> <li>~~Marci controls (R^2, Corr)~~</li> <li>~~choose rank~~</li> <li>control neuron freq drifts?</li> <li>~~VISp\u2192VISl,  VISl\u2192VISp~~</li> <li>change task (motor contam) OR movie</li> <li>(Bernstein)</li> </ol> <p>The Allen project appears to involve data analysis and visualization related to neuroscience. It involves the use of Python scripts for data loading, analysis, and plotting, working with large sets of data, and identifying maximum values at different parameters. However, the specific goal or research question of the project is not clearly stated in the search results.</p>"},{"location":"references/references/","title":"References","text":"<p>This page contains references to the experiments and utilities in the project.</p>"},{"location":"references/references/#table-of-contents","title":"Table of Contents","text":"<ul> <li>References</li> <li>Table of Contents</li> <li>Experiments</li> <li>Utilities</li> </ul>"},{"location":"references/references/#experiments","title":"Experiments","text":"<p>This package contains tools for analyzing neural data.</p>"},{"location":"references/references/#analyses--submodules","title":"Submodules","text":"<ul> <li><code>cca</code>: Canonical Correlation Analysis (CCA) tools.</li> <li><code>corr</code>: Correlation analysis tools.</li> <li><code>pca</code>: Principal Component Analysis (PCA) tools.</li> <li><code>cv-lag-time-selection</code>: tools for selecting the maximum mean across the CV, lag, and time dimensions.</li> <li><code>rrr_time_slice</code>: tools for performing RRRR analysis on time slices.</li> <li><code>layer_rank</code>: tools for performing rank-based analysis on neural data.</li> <li><code>data_preprocessing</code>: tools for preprocessing neural data.</li> <li><code>imbalanced_preprocessing</code>: tools for preprocessing imbalanced data.</li> <li><code>rrr</code>: Reduced Rank Regression Regularization (RRRR) tools.</li> <li><code>layer_interaction_maxValues</code>: tools for calculating the maximum values of the layer interaction matrix.</li> <li><code>machine_learning_models</code>: machine learning models for regression and feature selection.</li> </ul>"},{"location":"references/references/#utilities","title":"Utilities","text":"<p>Tool-package for data analysis and visualization (plotting).</p> <p>Main submodules:</p> <ul> <li><code>debug</code>: Debugging tool.</li> <li><code>megaplot</code>: This tool contains a class for creating and managing subplots in matplotlib.</li> <li><code>plots</code>: Plotting tool for all kinds of plots.</li> </ul> <p>More submodules:</p> <ul> <li><code>data_io</code>: Data input/output tools.</li> <li><code>download_allen</code>: Download and initialize data from the Allen Brain Observatory.</li> <li><code>layers</code>: Tools for simple layer analysis.</li> <li><code>directDownload</code>: Direct download tools for retrieving the download links for all sessions in a given manifest file from the Allen Brain Observatory.</li> <li><code>neuropixel</code>: Tools for working with Neuropixel data from the Allen Institute.</li> <li><code>ccf_volumes</code>: Tools for assigning cortical layers to channels and units based on the Allen Brain Atlas Common Coordinate Framework (CCF) volumes.</li> <li><code>utils</code>: Utility tools for various tasks.</li> <li><code>feature_functions</code>: Feature functions for data analysis.</li> </ul>"},{"location":"references/experiments/cca/","title":"Canonical Correlation Analysis","text":""},{"location":"references/experiments/cca/#analysis","title":"Analysis","text":"<p>This module makes a CCA analysis across two brain region (VISp and VISpm) from the allen cache databese using allensdk cache and the data/.vbn_s3_cache</p> <p>The functions for the analysis are in the cca.py and in the utils folder (e.g. download_allen.py and neuropixel.py)</p> <p>Parameters in code:</p> <ul> <li><code>session_id</code>: The index of the session to load.</li> </ul> <p>Input:</p> <p>None</p> <p>Output:</p> <ul> <li><code>results/cca.pickle</code>: Pickle file containing the results of the CCA analysis.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.cca</code>: Module for the CCA analysis.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.download_allen</code>: Module for downloading data from the Allen Institute API.</li> </ul>"},{"location":"references/experiments/cca/#plot","title":"Plot","text":"<p>This module plots the CCA scores.</p> <p>Parameters:</p> <p>None</p> <p>Input:</p> <ul> <li><code>results/cca_scores.csv</code>: CCA scores.</li> </ul> <p>Output:</p> <p>None</p> <p>Submodules:</p> <p>None</p>"},{"location":"references/experiments/cca/#tools","title":"Tools","text":"<p>This module contains functions for performing Canonical Correlation Analysis (CCA) on neural data. These functions are called by the cca_analysis.py, which is the main program of this analysis.</p> <p>Functions: - cca(X_train, Y_train, X_test, Y_test) -&gt; dict: Perform Canonical Correlation Analysis (CCA) on two sets of variables. - compare_two_areas(session, area_X, area_Y, log=True) -&gt; dict: Compare the responses of units in the VISp and VISpm brain areas using Canonical Correlation Analysis (CCA).</p>"},{"location":"references/experiments/cca/#analyses.cca.cca","title":"<code>cca(X_train, Y_train, X_test, Y_test)</code>","text":"<p>Perform Canonical Correlation Analysis (CCA) on two sets of variables.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>First set of variables for training, shape (n_samples, n_features1).</p> required <code>Y_train</code> <code>ndarray</code> <p>Second set of variables for training, shape (n_samples, n_features2).</p> required <code>X_test</code> <code>ndarray</code> <p>First set of variables for testing, shape (n_samples, n_features1).</p> required <code>Y_test</code> <code>ndarray</code> <p>Second set of variables for testing, shape (n_samples, n_features2).</p> required <p>Returns:</p> Name Type Description <code>cca</code> <code>dict</code> <p>A dictionary containing the following items: 'model' (sklearn.cross_decomposition.CCA): CCA object fitted on the training data. 'X_train_r' (numpy.ndarray): Transformed X_train data using CCA. 'Y_train_r' (numpy.ndarray): Transformed Y_train data using CCA. 'X_test_r' (numpy.ndarray): Transformed X_test data using CCA. 'Y_test_r' (numpy.ndarray): Transformed Y_test data using CCA.</p> Source code in <code>analyses/cca.py</code> <pre><code>def cca(X_train, Y_train, X_test, Y_test):\n    \"\"\"\n    Perform Canonical Correlation Analysis (CCA) on two sets of variables.\n\n    Parameters:\n        X_train (numpy.ndarray): First set of variables for training, shape (n_samples, n_features1).\n        Y_train (numpy.ndarray): Second set of variables for training, shape (n_samples, n_features2).\n        X_test (numpy.ndarray): First set of variables for testing, shape (n_samples, n_features1).\n        Y_test (numpy.ndarray): Second set of variables for testing, shape (n_samples, n_features2).\n\n    Returns:\n        cca (dict): A dictionary containing the following items:\n            'model' (sklearn.cross_decomposition.CCA): CCA object fitted on the training data.\n            'X_train_r' (numpy.ndarray): Transformed X_train data using CCA.\n            'Y_train_r' (numpy.ndarray): Transformed Y_train data using CCA.\n            'X_test_r' (numpy.ndarray): Transformed X_test data using CCA.\n            'Y_test_r' (numpy.ndarray): Transformed Y_test data using CCA.\n    \"\"\"\n    cca = CCA()\n    cca.fit(X_train, Y_train)\n\n    X_train_r, Y_train_r = cca.transform(X_train, Y_train)\n    X_test_r, Y_test_r = cca.transform(X_test, Y_test)\n\n    return {\n        'model': cca,\n        'X_train_r': X_train_r,\n        'Y_train_r': Y_train_r,\n        'X_test_r': X_test_r,\n        'Y_test_r': Y_test_r\n    }\n</code></pre>"},{"location":"references/experiments/cca/#analyses.cca.compare_two_areas","title":"<code>compare_two_areas(session, area_X, area_Y, log=True)</code>","text":"<p>Compare the responses of units in the VISp and VISpm brain areas using Canonical Correlation Analysis (CCA).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>EcephysSession</code> <p>The session object containing the spike times and stimulus presentations.</p> required <code>area_X</code> <code>str</code> <p>Name of the first brain area.</p> required <code>area_Y</code> <code>str</code> <p>Name of the second brain area.</p> required <code>log</code> <code>bool</code> <p>Whether to log the progress. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the results of the CCA analysis.</p> Source code in <code>analyses/cca.py</code> <pre><code>def compare_two_areas(session: EcephysSession, area_X, area_Y, log=True) -&gt; dict:\n    \"\"\"\n    Compare the responses of units in the VISp and VISpm brain areas using Canonical Correlation Analysis (CCA).\n\n    Parameters:\n        session (EcephysSession): The session object containing the spike times and stimulus presentations.\n        area_X (str): Name of the first brain area.\n        area_Y (str): Name of the second brain area.\n        log (bool, optional): Whether to log the progress. Defaults to True.\n\n    Returns:\n        dict: A dictionary containing the results of the CCA analysis.\n    \"\"\"\n\n    stimulus_block = 2\n    \"\"\"\n    stimulus block:\n    0: change detection task\n    2: receptive field mapping by gabor stimuli\n    4: full-flash\n    5: passive replay\n    \"\"\"\n\n    # Get area units\n    if log:\n        print('Get area units')\n    area_X_units = get_area_units(session, area_X) # shape (units)\n    area_Y_units = get_area_units(session, area_Y) # shape (units)\n    print('area_X_units number', area_X_units.shape[0])  # (98)\n    print('area_Y_units number', area_Y_units.shape[0])  # (111)\n\n    # Get the responses\n    if log:\n        print('Get the responses')\n    stimulus_presentations = get_stimulus_presentations(session)\n    # print(stimulus_presentations[stimulus_presentations['stimulus_block'] == stimulus_block].head(1))\n    trial_start = stimulus_presentations[stimulus_presentations['stimulus_block']\n                                         == stimulus_block]['start_time'].values\n    print('trial_start', trial_start)\n    area_X_responses = get_unit_responses(\n        area_X_units, session.spike_times, trial_start) # shape (units, timestep)\n    area_Y_responses = get_unit_responses(\n        area_Y_units, session.spike_times, trial_start) # shape (units, timestep)\n    print('area_X_responses.shape', area_X_responses.shape)  # (98, 30)\n    print('area_Y_responses.shape', area_Y_responses.shape)  # (111, 30)\n\n    # Make CCA\n    if log:\n        print('Make CCA')\n    # result = cca(area_X_responses.T, area_Y_responses.T)\n    model = CCA(n_components=params['n_components'])\n    scores = cross_val_score(model, area_X_responses.T, area_Y_responses.T, cv=params['cv'], scoring=params['scoring'])\n\n    print('Scores', scores)\n\n    return {\n        'scores': scores\n    }\n</code></pre>"},{"location":"references/experiments/control-models/","title":"Control Models","text":"<p>See experiment history for more information.</p> <p>This module compares the control models.</p> <p>Parameters:</p> <p>None</p> <p>Input:</p> <ul> <li><code>results/rrr-control.pickle</code>: RRR control models.</li> </ul> <p>Output:</p> <ul> <li><code>figures/control-models.png</code>: Plot of the control models.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module for RRR analysis.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> </ul>"},{"location":"references/experiments/crosstime/","title":"Crosstime","text":"<p>See experiment history.</p>"},{"location":"references/experiments/crosstime/#analysis","title":"Analysis","text":"<p>This module performs cross-time analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr</code>: RRR parameters.</li> <li><code>crosstime</code>: Crosstime parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;predictor&gt;-activity.pickle</code>: Predictor activity.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;target&gt;-activity.pickle</code>: Target activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/&lt;predictor&gt;-&lt;target&gt;_cross-time-RRR.pickle</code>: Cross-time RRR results.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/crosstime/#plot","title":"Plot","text":"<p>This module plots the cross-time RRR analysis.</p> <p>Parameters:</p> <ul> <li><code>load</code>:<ul> <li><code>session</code>: The session to analyze.</li> </ul> </li> <li><code>rrr-param-search</code>: The RRR parameter search parameters.</li> <li><code>rrr</code>: The RRR parameters.</li> <li><code>crosstime</code>: The cross-time parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/cross-time-RRR.pickle</code>: Pickle file containing the cross-time RRR analysis results.</li> </ul> <p>Output:</p> <ul> <li><code>figures/cross-time-RRR.png</code>: The cross-time RRR plot.</li> </ul> <p>(Also saves a copy of the plot in the <code>cache</code> directory with the session name appended to the filename.)</p> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting functions.</li> </ul>"},{"location":"references/experiments/cv-rank-time/","title":"Cross-Validation-fold Exploration","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/cv-rank-time/#analysis","title":"Analysis","text":"<p>This module performs cross-validation, rank, and time analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>:<ul> <li><code>session</code>: The session to analyze.</li> </ul> </li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr-cv-rank-time</code>: RRR cross-validation, rank, and time parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: Pickle file containing the raw activity for the VISp area.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: Pickle file containing the raw activity for the VISl area.</li> </ul> <p>Output:</p> <ul> <li><code>figures/CV-rank-time_&lt;time_bin&gt;-bin_&lt;time_step&gt;-step.png</code>: The cross-validation, rank, and time plot.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> <li><code>utils.utils</code>: Module for utility functions.</li> </ul>"},{"location":"references/experiments/cv-rank-time/#cv-rank-time-analysis.calculate_something","title":"<code>calculate_something(from_time=0.2, to_time=0.25)</code>","text":"<p>Calculate the time lag between two time series.</p> Source code in <code>cv-rank-time-analysis.py</code> <pre><code>def calculate_something(from_time=0.200, to_time=0.250):\n    '''\n    Calculate the time lag between two time series.\n    '''\n\n    # Recalculate the neural activity\n    # V1 = recalculate_neural_activity(V1, params['stimulus-duration'], params['bin-size'], params['time-step'], orig_time_step=load['step-size'])\n    V1 = raw_V1[:, :, int(from_time/load['step-size']):int(to_time/load['step-size'])].sum(axis=2)\n    X = z_score_normalize(calculate_residual_activity(V1[:,:,np.newaxis]), dims=(0, 1)).squeeze().T\n    V2 = raw_V2[:, :, int(from_time/load['step-size']):int(to_time/load['step-size'])].sum(axis=2)\n    Y = z_score_normalize(calculate_residual_activity(V2[:,:,np.newaxis]), dims=(0, 1)).squeeze().T\n\n    # Create a results array\n    results = np.zeros((len(cv), len(ranks)))\n\n    # Loop through the cross-validation and rank\n    for i, c in enumerate(cv):\n        for j, r in enumerate(ranks):\n            result = RRRR(X, Y, r, c)\n            results[i, j] = result['test_score'].mean()\n\n    # Cut off the negative values\n    results[results &lt; -0] = np.nan\n\n    print(results)\n\n    return results\n</code></pre>"},{"location":"references/experiments/cv-rank-time/#plot","title":"Plot","text":""},{"location":"references/experiments/cv-time-lag/","title":"Cross-Validation-fold, Time-lag, and Rank search","text":"<p>This is the main experiment for finding the best hyperparameters for the Reduced Rank Regression (see rrr params).</p> <p>See experiment history for more information.</p>"},{"location":"references/experiments/cv-time-lag/#analysis","title":"Analysis","text":"<p>This module searches for the optimal cv-fold and time lag in the RRR model using cross-validation.</p> <p>The script loops through the cross-validation folds, time lags, and ranks specified in the parameters and calculates the RRR model for each combination. The results are saved as a pickle file in the <code>data/rrr-results</code> directory.</p> <p>Parameters:</p> <ul> <li><code>load</code>:<ul> <li><code>stimulus-block</code>: The name of the stimulus block to analyze.</li> </ul> </li> <li><code>preprocess</code>:<ul> <li><code>step-size</code>: The step size of the time series data.</li> <li><code>stimulus-duration</code>: The duration of the stimulus block.</li> </ul> </li> <li><code>rrr</code>:<ul> <li><code>predictor</code>: The name of the brain area to use as the predictor in the RRR model.</li> <li><code>target</code>: The name of the brain area to use as the target in the RRR model.</li> <li><code>rank</code>: The rank of the RRR model.</li> </ul> </li> <li><code>rrr-param-search</code>:<ul> <li><code>cv</code>: A list of cross-validation folds to use.</li> <li><code>lag</code>: A list of time lags to use.</li> <li><code>rank</code>: A list of ranks to use.</li> <li><code>timepoints</code>: A list of timepoints to use for the analysis.</li> </ul> </li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;predictor&gt;-activity.pickle</code>: Pickle file containing the raw activity data for the predictor brain area.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;target&gt;-activity.pickle</code>: Pickle file containing the raw activity data for the target brain area.</li> </ul> <p>Output:</p> <ul> <li><code>data/rrr-results/CV-lag-time.pickle</code>: Pickle file containing the results of the cross-validation of the time lag in the RRR model. Shape: (n_cv, n_lag, n_rank, n_timepoints)</li> </ul> <p>Submodules:</p> <ul> <li><code>analysis.data_preprocessing</code>: Module for data preprocessing.</li> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/cv-time-lag/#cv-time-lag-analysis.calculate_something","title":"<code>calculate_something()</code>","text":"<p>Calculate the time lag between two time series.</p> Source code in <code>cv-time-lag-analysis.py</code> <pre><code>def calculate_something():\n    '''\n    Calculate the time lag between two time series.\n    '''\n\n    # Calculate the time length after the preprocessing by the time step and the stimulus duration\n    time_length = int(preproc['stimulus-duration'] / preproc['step-size'])\n    # Print the timelength calculation with the parameters\n    # print(f'{preproc[\"stimulus-duration\"]} / {preproc[\"step-size\"]} = {time_length}')\n\n    # Print the time length\n    # print(f'Time length: {time_length}')\n\n    # Create a results array\n    results = np.zeros((len(cv), len(time_lag), len(rank), time_length))\n\n    # Loop through the cross-validation and rank\n    for j, lag in enumerate(time_lag):\n\n        # Preprocess the area responses\n        predictor = preprocess_area_responses(full_activity_predictor)\n        target = preprocess_area_responses(full_activity_target)\n\n        # Move the activity of V2 back in time by the actual time lag\n        lagged_target = np.roll(target, -lag, axis=2)\n\n        for i, c in enumerate(cv):\n            for k, r in enumerate(rank):\n                for t, time in zip(timepoint_indices, timepoints):\n\n                    # Reduced Rank Regression\n                    # result = RRRR(V1.mean(axis=0), V2.mean(axis=0), params['rank'], cv=c) # cross-time RRRR\n\n                    result = RRRR(predictor[:,:,t].T, lagged_target[:,:,t].T, rank=r, cv=c, log=True)\n\n                    # Save the result averaged over the folds\n                    results[i, j, k, t] = result['test_score'].mean()\n\n    # Cut off the negative values\n    results[results &lt; -0] = np.nan\n\n    # Get rid of the slices that contains only zeros\n    results = results[:, :, :, np.all(results, axis=(0,1,2))]\n\n    return results\n</code></pre>"},{"location":"references/experiments/cv-time-lag/#plot","title":"Plot","text":"<p>This module plots the cross-validation, lag, and time search.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr</code>: RRR parameters.</li> <li><code>rrr-param-search</code>: RRR parameter search.</li> </ul> <p>Input:</p> <ul> <li><code>results/CV-lag-time.pickle</code>: Pickle file containing the results of the cross-validation, lag, and time search.</li> </ul> <p>Output:</p> <ul> <li><code>figures/rrr-param-search.png</code>: The cross-validation, lag, and time search plot.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting functions.</li> </ul>"},{"location":"references/experiments/histograms/","title":"Histograms","text":"<p>See experiment history for more information.</p> <p>This module calculates and plots the histograms of the V1 activity.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/5_block_VISp-activity.pickle</code>: V1 activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/5_block_VISp-histograms.pickle</code>: Histograms of the V1 activity.</li> <li><code>figures/V1-histograms_on_natural_images.png</code>: Plot of the histograms.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.utils</code>: Module for utility functions.</li> </ul>"},{"location":"references/experiments/lag-along-time/","title":"Lag along Time","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/lag-along-time/#analysis","title":"Analysis","text":"<p>This module performs lag-along-time analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr</code>: RRR parameters.</li> <li><code>best-rrr-params</code>: Best RRR parameters.</li> <li><code>rrr-param-search</code>: RRR parameter search.</li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;predictor&gt;-activity.pickle</code>: Predictor activity.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;target&gt;-activity.pickle</code>: Target activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/lags-along-time.pickle</code>: Lags along time.</li> <li><code>results/max-lags-along-time.pickle</code>: Max lags along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>analysis.data_preprocessing</code>: Module for data preprocessing.</li> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.utils</code>: Module containing utility functions.</li> </ul>"},{"location":"references/experiments/lag-along-time/#plot-multiple-time-lags","title":"Plot multiple time lags","text":"<p>This module plots the lags along time.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr-param-search</code>: RRR parameter search.</li> </ul> <p>Input:</p> <ul> <li><code>results/lags-along-time.pickle</code>: Lags along time.</li> </ul> <p>Output:</p> <ul> <li><code>figures/lags-along-time.png</code>: Plot of the lags along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/lag-along-time/#plot-the-maximum-time-lag","title":"Plot the maximum time lag","text":"<p>This module plots the max lag along time.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr-param-search</code>: RRR parameter search.</li> </ul> <p>Input:</p> <ul> <li><code>results/max-lags-along-time.pickle</code>: Max lag along time.</li> </ul> <p>Output:</p> <ul> <li><code>figures/max-lags-along-time.png</code>: Plot of the max lag along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Data I/O.</li> </ul>"},{"location":"references/experiments/layer-interaction/","title":"Layer Interaction","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/layer-interaction/#analysis","title":"Analysis","text":"<p>This module analyzes the interaction between layers in the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>crosstime</code>: Crosstime parameters.</li> <li><code>rrr</code>: RRR parameters.</li> </ul> <p>Input:</p> <ul> <li><code>session-params-old.csv</code>: Table of session parameters.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: V1 activity.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: LM activity.</li> <li><code>data/units/layer-assignments-VISp.pickle</code>: Layer assignments for V1.</li> <li><code>data/units/layer-assignments-VISl.pickle</code>: Layer assignments for LM.</li> </ul> <p>Output:</p> <ul> <li><code>results/layer-interaction_&lt;originArea&gt;-to-&lt;targetArea&gt;.pickle</code>: Layer interaction results for each layer combination between the two areas.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Reduced Rank Regression.</li> <li><code>utils.data_io</code>: Data I/O.</li> <li><code>utils.utils</code>: Utilities.</li> </ul>"},{"location":"references/experiments/layer-interaction/#plot","title":"Plot","text":"<p>This module plots the layer interaction results between two areas.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Session and stimulus-block from load parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>crosstime</code>: Crosstime parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/units/layer-assignments-&lt;originArea&gt;.pickle</code>: Layer assignments for the origin area.</li> <li><code>data/units/layer-assignments-&lt;targetArea&gt;.pickle</code>: Layer assignments for the target area.</li> <li><code>data/layer-interaction_&lt;originArea&gt;-to-&lt;targetArea&gt;.pickle</code>: Layer interaction results for the origin and target areas.</li> </ul> <p>Output:</p> <ul> <li><code>figures/layer-interaction_&lt;originArea&gt;-to-&lt;targetArea&gt;.png</code>: Layer interaction results for each layer combination between the two areas.</li> </ul> <p>(Also saved in the cache directory with the session and block number appended to the filename.)</p> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Data I/O.</li> <li><code>utils.plots</code>: Plotting utilities.</li> <li><code>utils.megaplot</code>: Megaplot.</li> </ul>"},{"location":"references/experiments/layer-rank/","title":"Layer Ranks","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/layer-rank/#analysis","title":"Analysis","text":"<p>This module calculates the RRR ranks between the layers of V1 and LM.</p> <p>Usage:</p> <pre><code>$ python layer-rank-analysis.py [-l]\n</code></pre> <p>Arguemnts:</p> <ul> <li><code>-l</code>: Log switch.</li> </ul> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>layer-rank</code>: Layer rank parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: V1 activity.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: LM activity.</li> <li><code>data/units/layer-assignments-VISp.pickle</code>: Layer assignments for V1.</li> <li><code>data/units/layer-assignments-VISl.pickle</code>: Layer assignments for LM.</li> </ul> <p>Output:</p> <ul> <li><code>results/layer-rank.pickle</code>: Layer rank results.</li> <li><code>results/layer-r2.pickle</code>: Layer R2 results.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.layer_rank</code>: Layer rank analysis.</li> <li><code>utils.data_io</code>: Data I/O.</li> <li><code>utils.utils</code>: Utilities.</li> </ul>"},{"location":"references/experiments/layer-rank/#plot","title":"Plot","text":"<p>This script plots the rank of the bottom-up and top-down connections between layers.</p> <p>Arguments:</p> <ul> <li><code>-rank</code>: Rank switch.</li> <li><code>-r2</code>: R2 switch.</li> </ul> <p>Parameters: - <code>preprocess</code>: Preprocess parameters.</p> <p>Input: - <code>results/layer-rank.pickle</code>: Layer rank results.</p> <p>Output:</p> <ul> <li><code>figures/layer-rank.png</code>: Layer rank results.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Data I/O.</li> <li><code>utils.megaplot</code>: Megaplot.</li> <li><code>utils.plots</code>: Plots.</li> <li><code>utils.utils</code>: Utilities.</li> </ul>"},{"location":"references/experiments/layer-rank/#tools","title":"Tools","text":"<p>This module contains tools for performing rank-based analysis on neural data.</p> <p>Functions:</p> <ul> <li><code>cv_search(predictor, target) -&gt; tuple</code>: Perform a RRRR search for the best cv.</li> <li><code>rank_search(predictor, target, cv, log=False) -&gt; tuple</code>: Perform a RRRR search for the best rank.</li> <li><code>calc_ranks(V1_data, LM_data, timepoints, log=False) -&gt; tuple</code>: Calculate the time lag between two time series.     tuple: A tuple containing the best rank found during the search and the maximum RRRR score.</li> </ul>"},{"location":"references/experiments/layer-rank/#analyses.layer_rank.calc_ranks","title":"<code>calc_ranks(V1_data, LM_data, timepoints, log=False)</code>","text":"<p>Calculate the ranks and R^2 scores for the given time series data.</p> <p>Parameters:</p> Name Type Description Default <code>V1_data</code> <code>numpy array</code> <p>Time series data for the V1 area.</p> required <code>LM_data</code> <code>numpy array</code> <p>Time series data for the LM area.</p> required <code>timepoints</code> <code>list</code> <p>List of time points to calculate ranks for.</p> required <code>log</code> <code>bool</code> <p>Whether to log the results or not. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>rank_results</code> <code>numpy array</code> <p>Array of calculated ranks. Shape: (nAreas(2), nLayers(6+1), nLayers(6+1), nTimepoints)</p> <code>r2_results</code> <code>numpy array</code> <p>Array of R^2 scores. Shape: (nAreas(2), nLayers(6+1), nLayers(6+1), nTimepoints)</p> Source code in <code>analyses/layer_rank.py</code> <pre><code>def calc_ranks(V1_data, LM_data, timepoints, log=False):\n    \"\"\"\n    Calculate the ranks and R^2 scores for the given time series data.\n\n    Parameters:\n        V1_data (numpy array): Time series data for the V1 area.\n        LM_data (numpy array): Time series data for the LM area.\n        timepoints (list): List of time points to calculate ranks for.\n        log (bool, optional): Whether to log the results or not. Default is False.\n\n    Returns:\n        rank_results (numpy array): Array of calculated ranks. Shape: (nAreas(2), nLayers(6+1), nLayers(6+1), nTimepoints)\n        r2_results (numpy array): Array of R^2 scores. Shape: (nAreas(2), nLayers(6+1), nLayers(6+1), nTimepoints)\n    \"\"\"\n\n    # Calculate the time length after the preprocessing by the time step and the stimulus duration\n    time_length = int(preproc['stimulus-duration'] / preproc['step-size'])\n\n    # Create a results array\n    rank_results = np.full((2, 6+1, 6+1, time_length), np.nan)\n    r2_results = np.full((2, 6+1, 6+1, time_length), np.nan)\n\n    for i, sourceArea, targetArea in zip([0, 1], [V1_data, LM_data], [LM_data, V1_data]):\n\n        # Parameters\n        sourceLayers = sourceArea['layer-assignments'].unique()\n        targetLayers = targetArea['layer-assignments'].unique()\n\n        best_cv, _ = cv_search(sourceArea['activity'], targetArea['activity'])\n\n        # Iterate over layer combinations\n        for output in sourceLayers:\n            for input in targetLayers:\n\n                # Iterate over the time points\n                for t in timepoints:\n\n                    best_rank, r2 = rank_search(sourceArea['activity'][:, :, t],\n                                                targetArea['activity'][:, :, t],\n                                                best_cv,\n                                                log=log)\n\n                    # Save the result averaged over the folds\n                    rank_results[i, output, input, t] = best_rank\n                    r2_results[i, output, input, t] = best_rank\n\n    return rank_results, r2_results\n</code></pre>"},{"location":"references/experiments/layer-rank/#analyses.layer_rank.cv_search","title":"<code>cv_search(predictor, target)</code>","text":"<p>Perform a cross-validation search for the best cv value and maximum RRRR score.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>ndarray</code> <p>The predictor data array. Shape: (n_samples, n_features, n_timepoints).</p> required <code>target</code> <code>ndarray</code> <p>The target data array. Shape: (n_samples, n_features, n_timepoints).</p> required <p>Returns:</p> Name Type Description <code>best_cv</code> <code>int</code> <p>The best cross-validation (cv) value found during the search.</p> <code>max_r2</code> <code>float</code> <p>The maximum R^2 score corresponding to the best cv value.</p> Source code in <code>analyses/layer_rank.py</code> <pre><code>def cv_search(predictor, target) -&gt; tuple:\n    \"\"\"\n    Perform a cross-validation search for the best cv value and maximum RRRR score.\n\n    Args:\n        predictor (ndarray): The predictor data array. Shape: (n_samples, n_features, n_timepoints).\n        target (ndarray): The target data array. Shape: (n_samples, n_features, n_timepoints).\n\n    Returns:\n        best_cv (int): The best cross-validation (cv) value found during the search.\n        max_r2 (float): The maximum R^2 score corresponding to the best cv value.\n    \"\"\"\n\n    # Define the cross-validation, and time\n    cvs = params['cv']\n    rank = params['bestRank']\n    timepoint = params['timepoint']\n\n    # Get the time index from the timepoint (ms) and 'step-size' (s)\n    t = int(timepoint / 1000 / preproc['step-size'])\n\n    # Create a results array\n    results = np.full((10+1), np.nan)\n\n    # Calculate the RRRR for each cv\n    for cv in cvs:\n\n        result = RRRR(predictor[:, :, t].T,\n                      target[:, :, t].T,\n                      rank=rank, cv=cv)\n\n        # Save the result averaged over the folds\n        results[cv] = result['test_score'].mean()\n\n    # Cut off the negative values\n    results[results &lt; -0] = np.nan\n\n    # Get the maximum value (max_r2) and its index (best_cv)\n    max_r2 = np.nanmax(results).round(3)\n    best_cv = np.nanargmax(results)\n\n    return best_cv, max_r2\n</code></pre>"},{"location":"references/experiments/layer-rank/#analyses.layer_rank.rank_search","title":"<code>rank_search(predictor, target, cv, log=False)</code>","text":"<p>Perform a rank search for the best rank.</p> <p>This function calculates the RRRR (Rank-Reduced Ridge Regression) for different ranks and returns the best rank found during the search.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>ndarray</code> <p>The predictor data. Shape: (n_samples, n_features).</p> required <code>target</code> <code>ndarray</code> <p>The target data. Shape: (n_samples, n_features).</p> required <code>cv</code> <code>int</code> <p>The number of cross-validation folds.</p> required <code>log</code> <code>bool</code> <p>Whether to log the results or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>best_rank</code> <code>int</code> <p>The best rank found during the search.</p> <code>max_r2</code> <code>float</code> <p>The maximum R^2 score corresponding to the best rank.</p> Source code in <code>analyses/layer_rank.py</code> <pre><code>def rank_search(predictor, target, cv, log=False) -&gt; tuple:\n    \"\"\"\n    Perform a rank search for the best rank.\n\n    This function calculates the RRRR (Rank-Reduced Ridge Regression) for different ranks and returns the best rank found during the search.\n\n    Parameters:\n        predictor (ndarray): The predictor data. Shape: (n_samples, n_features).\n        target (ndarray): The target data. Shape: (n_samples, n_features).\n        cv (int): The number of cross-validation folds.\n        log (bool): Whether to log the results or not.\n\n    Returns:\n        best_rank (int): The best rank found during the search.\n        max_r2 (float): The maximum R^2 score corresponding to the best rank.\n    \"\"\"\n\n    # Define the ranks\n    ranks = np.arange(params['minRank'], params['maxRank'], params['stepRank'])\n\n    # Create a results array\n    results = np.full((len(ranks)), np.nan)\n\n    # Calculate the RRRR for each rank\n    for i, rank in enumerate(ranks):\n\n        result = RRRR(predictor[:, :].T,\n                      target[:, :].T,\n                      rank=rank, cv=cv,\n                      success_log=log) # Switch to False!\n\n        # Save the result averaged over the folds\n        results[i] = np.nanmean(result['test_score'])\n\n    # Cut off the negative values\n    results[results &lt; -0] = np.nan\n\n    # If all nan, return nan\n    if np.all(np.isnan(results)):\n        return np.nan, np.nan\n\n    # Get the maximum value (max_r2) and its index (max_idx)\n    max_r2 = np.nanmax(results).round(3)\n    max_idx = np.nanargmax(results)\n\n    # Get the rank corresponding to the maximum value\n    max_r2_rank = max_idx * params['stepRank'] + params['minRank']\n\n    return max_r2_rank, max_r2\n</code></pre>"},{"location":"references/experiments/multiple-timeslices-layers/","title":"Multiple Time Slices: Layers","text":"<p>See experiment history for more information.</p> <p>This module analyzes the interaction between layers in the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>:<ul> <li><code>session</code>: The session to analyze.</li> </ul> </li> <li><code>preprocess</code>: The preprocess parameters.</li> <li><code>timeslice</code>: The time-slice parameters.</li> <li><code>crosstime</code>: The cross-time parameters.</li> <li><code>interaction-layers</code>: The layers to analyze.</li> </ul> <p>Input:</p> <ul> <li><code>data/units/layer-assignments-VISp.pickle</code>: Pickle file containing the layer assignments for the VISp area.</li> <li><code>data/units/layer-assignments-VISl.pickle</code>: Pickle file containing the layer assignments for the VISl area.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: Pickle file containing the raw activity for the VISp area.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: Pickle file containing the raw activity for the VISl area.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: Pickle file containing the preprocessed activity for the VISp area.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: Pickle file containing the preprocessed activity for the VISl area.</li> <li><code>data/stimulus-presentations/&lt;stimulus-block&gt;_block_image-names.pickle</code>: Pickle file containing the stimulus names.</li> </ul> <p>Output:</p> <ul> <li><code>figures/rrr-cross-time-slice-mega-plot.png</code>: The mega plot of the RRR analysis.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Reduced Rank Regression.</li> <li><code>utils.data_io</code>: Data I/O.</li> <li><code>utils.plots</code>: Plotting utilities.</li> <li><code>utils.megaplot</code>: Megaplot.</li> <li><code>utils.utils</code>: Utilities.</li> </ul>"},{"location":"references/experiments/multiple-timeslices/","title":"Multiple Time Slices","text":"<p>See experiment history for more information.</p> <p>This module analyzes the interaction between layers in the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>:<ul> <li><code>session</code>: The session to analyze.</li> </ul> </li> <li><code>preprocess</code>: The preprocess parameters.</li> <li><code>timeslice</code>: The time-slice parameters.</li> <li><code>crosstime</code>: The cross-time parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: Pickle file containing the raw activity for the VISp area.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: Pickle file containing the raw activity for the VISl area.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: Pickle file containing the preprocessed activity for the VISp area.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: Pickle file containing the preprocessed activity for the VISl area.</li> <li><code>data/stimulus-presentations/&lt;stimulus-block&gt;_block_image-names.pickle</code>: Pickle file containing the stimulus names.</li> </ul> <p>Output:</p> <ul> <li><code>figures/rrr-cross-time-slice-mega-plot.png</code>: The mega plot of the RRR analysis.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>analyses.rrr_time_slice</code>: Module containing the time-slice analysis function.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.megaplot</code>: Module for creating mega plots.</li> <li><code>utils.utils</code>: Module for utility functions.</li> <li><code>utils.plots</code>: Module for plotting functions.</li> </ul>"},{"location":"references/experiments/pca/","title":"Principal Component Analysis","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/pca/#analysis","title":"Analysis","text":"<p>This module performs PCA analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: V1 activity.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: LM activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/pca-analysis.pickle</code>: PCA analysis results.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.pca</code>: Module containing the PCA function for calculating the PCA model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/pca/#plot","title":"Plot","text":"<p>This module plots the PCA analysis results.</p> <p>Parameters:</p> <p>None</p> <p>Input:</p> <ul> <li><code>results/pca-analysis.pickle</code>: PCA analysis results.</li> </ul> <p>Output:</p> <ul> <li><code>figures/pca-analysis_explained_variance_ratio.png</code>: Plot of the explained variance ratio.</li> <li><code>figures/pca-analysis_principal-components.png</code>: Plot of the principal components.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/pca/#tools","title":"Tools","text":"<p>This module contains functions for performing PCA on neural data.</p> <p>Functions: - pca(neural_activity) -&gt; dict: Perform PCA on the given neural activity data.</p>"},{"location":"references/experiments/rank-along-time/","title":"Rank Along Time","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/rank-along-time/#analysis","title":"Analysis","text":"<p>This module performs rank-along-time analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr</code>: RRR parameters.</li> <li><code>rrr-param-search</code>: RRR parameter search.</li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;predictor&gt;-activity.pickle</code>: Predictor activity.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;target&gt;-activity.pickle</code>: Target activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/max-ranks.pickle</code>: Optimal rank along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.data_preprocessing</code>: Data preprocessing.</li> <li><code>analyses.rrr</code>: Reduced Rank Regression.</li> <li><code>utils.data_io</code>: Data I/O.</li> </ul>"},{"location":"references/experiments/rank-along-time/#plot","title":"Plot","text":"<p>This module plots the optimal rank along time.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/max-ranks.pickle</code>: Optimal rank along time.</li> </ul> <p>Output:</p> <ul> <li><code>figures/max-ranks.png</code>: Plot of the optimal rank along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Data I/O.</li> </ul>"},{"location":"references/experiments/rrr-rank/","title":"Reduced Rank Regression Rank Optimization","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/rrr-rank/#analysis","title":"Analysis","text":"<p>This module performs rank-along-time analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr</code>: RRR parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: V1 activity.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: LM activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/VISp_VISl_cross-time-test-scores.pickle</code>: RRR score along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/rrr-rank/#plot","title":"Plot","text":"<p>This module plots the RRR rank analysis.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr-plot</code>: RRR rank analysis parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/VISp_VISl_cross-time-test-scores.pickle</code>: RRR test scores.</li> </ul> <p>Output:</p> <ul> <li><code>figures/V1-V2_cross-time_RRR-rank-analysis-2DIM.png</code>: 2D plot of the RRR test scores.</li> <li><code>figures/V1-V2_cross-time_RRR-rank-analysis-timewise.png</code>: Plot of the RRR test scores along time.</li> <li><code>figures/V1-V2_cross-time_RRR-rank-analysis-averaged-over-time.png</code>: Plot of the RRR test scores averaged over time.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> </ul>"},{"location":"references/experiments/rrr-score-time/","title":"Reduced Rank Regression (RRR) Score-Time Plot","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/rrr-score-time/#analysis","title":"Analysis","text":"<p>This module performs rank-along-time analysis on the Allen Neuropixel dataset.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr</code>: RRR parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/5_block_&lt;predictor&gt;-activity.pickle</code>: Predictor activity.</li> <li><code>data/area-responses/5_block_&lt;target&gt;-activity.pickle</code>: Target activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/rrr-score-time.pickle</code>: RRR score along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/rrr-score-time/#plot","title":"Plot","text":"<p>This module plots the RRR score along time.</p> <p>Parameters:</p> <ul> <li><code>rrr</code>: RRR parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/rrr-score-time.pickle</code>: RRR score along time.</li> </ul> <p>Output:</p> <ul> <li><code>figures/rrr-score-time.png</code>: Plot of the RRR score along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> </ul>"},{"location":"references/experiments/rrr-time-slice/","title":"Reduced Rank Regression Time Slice","text":"<p>See the experiment history for more information.</p>"},{"location":"references/experiments/rrr-time-slice/#analysis","title":"Analysis","text":"<p>This module performs bidirectional time-slice analysis on the Allen Neuropixel dataset.</p> <p>The script loads the preprocessed activity for the VISp and VISl areas from the <code>data/area-responses</code> folder. It then performs the bidirectional time-slice analysis using the <code>bidirectional_time_slice</code> function from the <code>analyses.rrr_time_slice</code> module. The best parameters for the RRR model are loaded from the <code>params.yaml</code> file.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>best-rrr-params</code>: Best RRR parameters.</li> <li><code>rrr-time-slice</code>: Time-slice parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: Pickle file containing the preprocessed activity for the VISp area.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: Pickle file containing the preprocessed activity for the VISl area.</li> </ul> <p>Output:</p> <ul> <li><code>results/rrr-time-slice.pickle</code>: Pickle file containing the results of the time-slice analysis.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr_time_slice</code>: Module containing the time-slice analysis function.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/rrr-time-slice/#plot","title":"Plot","text":"<p>This module plots the results of the RRR time-slice analysis.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> <li><code>rrr-time-slice</code>: Time-slice parameters.<ul> <li><code>predictor-time</code>: Time of the predictor stimulus.</li> </ul> </li> </ul> <p>Input:</p> <ul> <li><code>results/rrr-time-slice.pickle</code>: Results of the time-slice analysis.</li> </ul> <p>Output:</p> <ul> <li><code>figures/rrr-time-slice.png</code>: Plot of the RRR time-slice analysis.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.plots</code>: Module for plotting functions.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/rrr-time-slice/#tools","title":"Tools","text":"<p>This module contains tools for performing RRRR analysis on time slices.</p> <p>Functions: - RRRR_time_slice(predictor, target, predictor_time, cv, rank, log=True) -&gt; dict: Calculate the RRRR for each time slice. - bidirectional_time_slice(V1_activity, LM_activity, session_params:pd.DataFrame, predictor_time, log=False) -&gt; dict: Perform bidirectional time slice analysis using RRRR.</p>"},{"location":"references/experiments/rrr-time-slice/#analyses.rrr_time_slice.RRRR_time_slice","title":"<code>RRRR_time_slice(predictor, target, predictor_time, cv, rank, log=True)</code>","text":"<p>Calculate the RRRR (Reduced Rank Regression Regularization) for each time slice.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>ndarray</code> <p>The predictor data with shape (N, M, T), where N is the number of samples,                  M is the number of features, and T is the number of time slices.</p> required <code>target</code> <code>ndarray</code> <p>The target data with shape (N, P, T), where N is the number of samples,               P is the number of target variables, and T is the number of time slices.</p> required <code>predictor_time</code> <code>float</code> <p>The time point of interest in milliseconds.</p> required <code>cv</code> <code>int</code> <p>The number of cross-validation folds.</p> required <code>rank</code> <code>int</code> <p>The rank of the RRRR model.</p> required <p>Returns:</p> Name Type Description <code>rrr</code> <code>dict</code> <p>A dictionary containing the mean and standard error of the RRRR scores for each time slice.   The dictionary has the following keys:   'mean': ndarray of shape (T), containing the mean RRRR scores for each time slice.   'sem': ndarray of shape (T), containing the standard error of the RRRR scores for each time slice.</p> Source code in <code>analyses/rrr_time_slice.py</code> <pre><code>def RRRR_time_slice(predictor, target, predictor_time, cv, rank, log=True):\n    \"\"\"\n    Calculate the RRRR (Reduced Rank Regression Regularization) for each time slice.\n\n    Args:\n        predictor (ndarray): The predictor data with shape (N, M, T), where N is the number of samples,\n                             M is the number of features, and T is the number of time slices.\n        target (ndarray): The target data with shape (N, P, T), where N is the number of samples,\n                          P is the number of target variables, and T is the number of time slices.\n        predictor_time (float): The time point of interest in milliseconds.\n        cv (int): The number of cross-validation folds.\n        rank (int): The rank of the RRRR model.\n\n    Returns:\n        rrr (dict): A dictionary containing the mean and standard error of the RRRR scores for each time slice.\n              The dictionary has the following keys:\n              'mean': ndarray of shape (T), containing the mean RRRR scores for each time slice.\n              'sem': ndarray of shape (T), containing the standard error of the RRRR scores for each time slice.\n    \"\"\"\n\n    T = predictor.shape[2]\n    t_pred = int(predictor_time / preprocess[\"step-size\"])\n\n    if log:\n        print('predictor_time', predictor_time)\n\n    # Init results\n    mean = np.full((T), fill_value=np.nan)\n    sem  = np.full((T), fill_value=np.nan)\n\n    # Print progress bar\n    if log:\n        printProgressBar(0, T, prefix = 'RRR analysis:', length = 50)\n\n    for x in range(T):\n\n        predictor_t = predictor[:, :, t_pred]\n        target_t = target[:, :, x]\n\n        # Calculate the RRRR\n        model = RRRR(predictor_t.T, target_t.T,\n                     rank=rank, cv=cv, success_log=False)\n\n        # Save results\n        mean[x]  =   model['test_score'].mean()\n        sem[x] = SEM(model['test_score'])\n\n        # Update progress bar\n        if log:\n            printProgressBar(x + 1, T, prefix = 'RRR analysis:', length = 50)\n\n    # Return the results\n    return {\n        'mean': mean,\n        'sem': sem\n    }\n</code></pre>"},{"location":"references/experiments/rrr-time-slice/#analyses.rrr_time_slice.bidirectional_time_slice","title":"<code>bidirectional_time_slice(V1_activity, LM_activity, session_params, predictor_time, log=False)</code>","text":"<p>Perform bidirectional time slice analysis using RRRR.</p> <p>Parameters:</p> Name Type Description Default <code>V1_activity</code> <code>ndarray</code> <p>The activity data for V1.</p> required <code>LM_activity</code> <code>ndarray</code> <p>The activity data for LM.</p> required <code>session_params</code> <code>DataFrame</code> <p>The best parameters for each prediction direction and session.</p> required <code>predictor_time</code> <code>ndarray</code> <p>The time points for the predictor activity.</p> required <p>Returns:</p> Name Type Description <code>prediction_direction</code> <code>dict</code> <p>A dictionary containing the results for each prediction direction. The keys of the dictionary are the prediction directions ('top-down', 'bottom-up', 'V1', 'LM'). The values are the results of the RRRR analysis for each prediction direction.</p> Source code in <code>analyses/rrr_time_slice.py</code> <pre><code>def bidirectional_time_slice(V1_activity, LM_activity, session_params:pd.DataFrame, predictor_time, log=False):\n\n    \"\"\"\n    Perform bidirectional time slice analysis using RRRR.\n\n    Args:\n        V1_activity (numpy.ndarray): The activity data for V1.\n        LM_activity (numpy.ndarray): The activity data for LM.\n        session_params (pd.DataFrame): The best parameters for each prediction direction and session.\n        predictor_time (numpy.ndarray): The time points for the predictor activity.\n\n    Returns:\n        prediction_direction (dict): A dictionary containing the results for each prediction direction. The keys of the dictionary are the prediction directions ('top-down', 'bottom-up', 'V1', 'LM'). The values are the results of the RRRR analysis for each prediction direction.\n\n    \"\"\"\n\n    # Define the parameters\n    abstract_areas = {\n        'top-down': {\n            'predictor': LM_activity,\n            'target': V1_activity\n        },\n        'bottom-up': {\n            'predictor': V1_activity,\n            'target': LM_activity\n        },\n        'V1': {\n            'predictor': V1_activity,\n            'target': V1_activity\n        },\n        'LM': {\n            'predictor': LM_activity,\n            'target': LM_activity\n        }\n    }\n\n    # Init results\n    prediction_direction = {}\n\n    # Iterate through the prediction directions\n    for prediction_direction in ['top-down', 'bottom-up', 'V1', 'LM']:\n\n        # Extract the data\n        predictor_activity = abstract_areas[prediction_direction]['predictor']\n        target_activity = abstract_areas[prediction_direction]['target']\n\n        # Extract the session parameters\n        if prediction_direction in ['top-down', 'bottom-up']:\n            session_key = prediction_direction\n        if prediction_direction is 'V1':\n            session_key = 'bottom-up'\n        if prediction_direction is 'LM':\n            session_key = 'top-down'\n        params = session_params[session_params['direction'] == session_key]\n        cv = params['cv'].values[0]\n        rank = params['rank'].values[0]\n\n        # Calculate the RRRR\n        result = RRRR_time_slice(\n            predictor_activity, target_activity, predictor_time, cv, rank, log=log)\n\n        # Save the results\n        prediction_direction[prediction_direction] = result\n\n    return prediction_direction\n</code></pre>"},{"location":"references/experiments/rrr/","title":"Reduced Rank Regression along Time","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/rrr/#analysis","title":"Analysis","text":"<p>This code will make a CCA analysis across two brain region (VISp and VISpm) from the allen cache databese using allensdk cache and the data/.vbn_s3_cache</p> <p>The functions for the analysis are in the cca.py and in the utils folder (e.g. download_allen.py and neuropixel.py) </p> <p>Utile functions:</p> <ul> <li>from allensdk.brain_observatory.ecephys.visualization import plot_mean_waveforms, plot_spike_counts, raster_plot</li> </ul> <p>Parameters:</p> <ul> <li><code>preprocess</code>:<ul> <li><code>stimulus-block</code>: The name of the stimulus block to analyze.</li> </ul> </li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-responses.pickle</code>: Pickle file containing the responses of the units in the VISp area.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISpm-responses.pickle</code>: Pickle file containing the responses of the units in the VISpm area.</li> </ul> <p>Output:</p> <ul> <li><code>results/rrr.pickle</code>: Pickle file containing the results of the RRR analysis.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module for the RRR analysis.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.download_allen</code>: Module for downloading data from the Allen Institute API.</li> </ul>"},{"location":"references/experiments/rrr/#plot","title":"Plot","text":"<p>This module plots the RRR results.</p> <p>Parameters:</p> <ul> <li><code>rrr</code>: RRR parameters.</li> <li><code>preprocess</code>: Preprocess parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/rrr_coefficients.pickle</code>: RRR coefficients.</li> </ul> <p>Output:</p> <ul> <li><code>figures/VISl-VISp_block-&lt;stimulus-block&gt;_rrr-coefficients_along_time.png</code>: Plot of the RRR coefficients along time.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> </ul>"},{"location":"references/experiments/rrr/#tools","title":"Tools","text":"<p>This module contains tools for performing Reduced Rank Regression (RRR) analysis on neural data.</p> <p>Functions:</p> <ul> <li>getCoeffs(model, log=False) -&gt; numpy.ndarray: Calculate the mean coefficients of a model over cross-validation folds.</li> <li>RRRR(X_data, Y_data, dataBalancing='none', rank=None, cv=None, log=False, success_log=True, warn=True) -&gt; dict: Make Reduced Rank Regression (RRR) analysis.</li> <li>RFE_CV(X_data, Y_data, rank=None, cv=None) -&gt; dict: Perform Recursive Feature Elimination (RFE) cross-validation.</li> <li>compare_two_areas(area_X_responses:np.ndarray, area_Y_responses:np.ndarray, log=False) -&gt; dict: Compare the responses of units in two brain areas using Reduced Rank Regression (RRR).</li> <li>control_models(predictor_names=['V1', 'movement', 'pupil'], response_name='V2', log=False) -&gt; np.ndarray: Perform control models analysis using Reduced Rank Regression (RRR).</li> <li>rrr_rank_analysis(V1_activity, V2_activity, max_rank=15, cv=params['cv'], log=False) -&gt; np.ndarray: Perform Reduced Rank Regression (RRR) rank analysis.</li> <li>calculate_cross_time_correlation(areaX, areaY, log=False) -&gt; np.ndarray: Calculate the cross-time correlation RRR between the responses of two brain areas.</li> <li>cross_time_rrr_coeffs(V1_activity, V2_activity, cv=None, rank=None) -&gt; np.ndarray: Calculate the cross-time RRR coefficients between two sets of activities.</li> <li>crosstime_analysis(predictor, target, cv, rank, scaling_factor=10, dataBalancing='none', ProgressBar=True) -&gt; np.ndarray: Perform cross-time analysis based on timpoints of rrr-param-search lag.</li> </ul>"},{"location":"references/experiments/rrr/#analyses.rrr.RFE_CV","title":"<code>RFE_CV(X_data, Y_data, rank=None, cv=None)</code>","text":"<p>Performs Recursive Feature Elimination (RFE) with cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>X_data</code> <code>array - like</code> <p>The input data.</p> required <code>Y_data</code> <code>array - like</code> <p>The target data.</p> required <code>rank</code> <code>int</code> <p>The number of features to select. If not provided, the default value from params will be used.</p> <code>None</code> <code>cv</code> <code>int</code> <p>The number of cross-validation folds. If not provided, the default value from params will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>optimal_features</code> <code>array - like</code> <p>The selected optimal features.</p> Source code in <code>analyses/rrr.py</code> <pre><code>def RFE_CV(X_data, Y_data, rank=None, cv=None):\n    \"\"\"\n    Performs Recursive Feature Elimination (RFE) with cross-validation.\n\n    Parameters:\n        X_data (array-like): The input data.\n        Y_data (array-like): The target data.\n        rank (int, optional): The number of features to select. If not provided, the default value from params will be used.\n        cv (int, optional): The number of cross-validation folds. If not provided, the default value from params will be used.\n\n    Returns:\n        optimal_features (array-like): The selected optimal features.\n    \"\"\"\n\n    # Set default values\n    if rank is None:\n        rank = params['rank']\n    if cv is None:\n        cv = params['cv']\n\n    # Make RFE model\n    optimal_features = custom_feature_selection(X_data, Y_data, rank, n_splits=cv)\n\n    return optimal_features\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.RRRR","title":"<code>RRRR(X_data, Y_data, dataBalancing='none', rank=None, cv=None, log=False, success_log=True, warn=True)</code>","text":"<p>Make Reduced Rank Regression (RRR) analysis.</p> <p>Parameters:</p> Name Type Description Default <code>X_data</code> <code>ndarray</code> <p>The data of the first brain area. Shape (n_samples, n_features_X)</p> required <code>Y_data</code> <code>ndarray</code> <p>The data of the second brain area. Shape (n_samples, n_features_Y)</p> required <code>dataBalancing</code> <code>str</code> <p>The type of the RRR analysis. Either 'none' or 'undersampled'. Defaults to 'none'.</p> <code>'none'</code> <code>log</code> <code>bool</code> <p>Whether to log the progress. Defaults to True.</p> <code>False</code> <code>rank</code> <code>int</code> <p>The rank of the RRR model. Defaults to None.</p> <code>None</code> <code>cv</code> <code>int</code> <p>The number of cross-validation folds. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the results of the RRR analysis. mean_coefficients (np.ndarray): The mean coefficients of the RRR model. Shape (n_features_X, n_features_Y) test_score (np.ndarray): The test scores of the RRR model. Shape (n_splits,) estimator (np.ndarray): The estimators of the RRR model. Shape (n_splits,)</p> Source code in <code>analyses/rrr.py</code> <pre><code>def RRRR(X_data, Y_data, dataBalancing='none', rank=None, cv=None, log=False, success_log=True, warn=True) -&gt; dict:\n    \"\"\"\n    Make Reduced Rank Regression (RRR) analysis.\n\n    Args:\n        X_data (np.ndarray): The data of the first brain area. Shape (n_samples, n_features_X)\n        Y_data (np.ndarray): The data of the second brain area. Shape (n_samples, n_features_Y)\n        dataBalancing (str, optional): The type of the RRR analysis. Either 'none' or 'undersampled'. Defaults to 'none'.\n        log (bool, optional): Whether to log the progress. Defaults to True.\n        rank (int, optional): The rank of the RRR model. Defaults to None.\n        cv (int, optional): The number of cross-validation folds. Defaults to None.\n\n    Returns:\n        dict: A dictionary containing the results of the RRR analysis.\n            mean_coefficients (np.ndarray): The mean coefficients of the RRR model. Shape (n_features_X, n_features_Y)\n            test_score (np.ndarray): The test scores of the RRR model. Shape (n_splits,)\n            estimator (np.ndarray): The estimators of the RRR model. Shape (n_splits,)\n    \"\"\"\n\n    # Set default values\n    if rank is None:\n        rank = params['rank']\n    if cv is None:\n        cv = params['cv']\n\n    # Make RRR model\n    model = ReducedRankRidgeRegression(rank=rank)\n\n    # Set the undersampling sample size\n    sample_size = params['sample-size']\n\n    # Perform cross-validation\n    if dataBalancing == 'none':\n        results = cross_validate(model, X_data, Y_data, cv=cv, scoring='r2', return_estimator=True)\n    elif dataBalancing == 'undersampled':\n        results = undersampled_cross_validation(model, X_data, Y_data, sample_size, k_folds=cv, warn=warn)\n    else:\n        raise ValueError('Invalid type. Use either \"none\" or \"undersampled\".')\n\n    # Log the cross-validation scores\n    if log:\n        print('Cross-validation scores:', results['test_score'])\n\n    # Get the coefficients\n    results['mean_coefficients'] = getCoeffs(results['estimator'], log=log)\n\n    # Get the adjusted R2 score\n    results['adjusted_r2'] = 1 - (1 - results['test_score']) * (Y_data.shape[0] - 1) / (Y_data.shape[0] - X_data.shape[1] - 1)\n    if log:\n        print('R2:', results['test_score'].mean())\n        print('Adjusted R2:', results['adjusted_r2'].mean())\n\n    # If mean of the scores is not negative, then print the cv, rank and the mean of the scores\n    if success_log and np.mean(results['test_score']) &gt; 0:\n        print(f'CV: {cv}, Rank: {rank}, Mean test score: {np.mean(results[\"test_score\"])}')\n\n    # Negative scores are not meaningful, so set them to nan\n    # results['test_score'][results['test_score'] &lt; 0] = np.nan\n    if np.mean(results['test_score']) &lt; 0:\n        results['test_score'] = np.array([np.nan])\n\n    return results\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.calculate_cross_time_correlation","title":"<code>calculate_cross_time_correlation(areaX, areaY, log=False)</code>","text":"<p>Calculate the cross-time correlation RRR between the responses of two brain areas. The output will be a matrix of shape (T, T) where T is the number of time points.</p> Source code in <code>analyses/rrr.py</code> <pre><code>def calculate_cross_time_correlation(areaX, areaY, log=False) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the cross-time correlation RRR between the responses of two brain areas. The output will be a matrix of shape (T, T) where T is the number of time points.\n    \"\"\"\n\n    # Get the number of neurons, trials, and time points\n    N, K, T = areaX.shape\n\n    # Initialize the cross-time correlation coefficients\n    cross_time_r2 = np.zeros((T, T))\n\n    # Iterate over time\n    for t in range(T):\n        for s in range(T):\n            X = areaX[:, :, t].T\n            Y = areaY[:, :, s].T\n\n            # Calculate rrr ridge using your rrrr function\n            models = RRRR(X, Y, rank=params['rank'], cv=params['cv'])\n\n            # Calculate the mean of the test scores above the cv-folds\n            cross_time_r2[t, s] = np.mean(models['test_score'])\n\n    if log:\n        print('cross_time_r2\\n', cross_time_r2)\n\n    # The values must be nan where the time of V1 is greater than V2\n    cross_time_r2[np.tril_indices(cross_time_r2.shape[0], k=-1)] = np.nan\n\n    # The negative values must be nan, because they are not meaningful\n    cross_time_r2[cross_time_r2 &lt; 0] = np.nan\n\n    return cross_time_r2\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.compare_two_areas","title":"<code>compare_two_areas(area_X_responses, area_Y_responses, log=False)</code>","text":"<p>Compare the responses of units in two brain areas using Reduced Rank Regression (RRR).</p> <p>Parameters:</p> Name Type Description Default <code>area_X_responses</code> <code>ndarray</code> <p>The responses of units in the first brain area. Shape (n_units_X, n_trials, n_timepoints)</p> required <code>area_Y_responses</code> <code>ndarray</code> <p>The responses of units in the second brain area. Shape (n_units_Y, n_trials, n_timepoints)</p> required <code>log</code> <code>bool</code> <p>Whether to log the progress. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>return</code> <code>dict</code> <p>A dictionary containing the results of the RRR analysis. coefficients (np.ndarray): The coefficients of the RRR model. Shape (n_units_X, n_units_Y, n_timepoints)</p> Source code in <code>analyses/rrr.py</code> <pre><code>def compare_two_areas(area_X_responses:np.ndarray, area_Y_responses:np.ndarray, log=False) -&gt; dict:\n    \"\"\"\n    Compare the responses of units in two brain areas using Reduced Rank Regression (RRR).\n\n    Parameters:\n        area_X_responses (np.ndarray): The responses of units in the first brain area. Shape (n_units_X, n_trials, n_timepoints)\n        area_Y_responses (np.ndarray): The responses of units in the second brain area. Shape (n_units_Y, n_trials, n_timepoints)\n        log (bool, optional): Whether to log the progress. Defaults to True.\n\n    Returns:\n        return (dict): A dictionary containing the results of the RRR analysis.\n            coefficients (np.ndarray): The coefficients of the RRR model. Shape (n_units_X, n_units_Y, n_timepoints)\n    \"\"\"\n\n    # Parameters\n    binSize = preprocess['bin-size']\n    duration = preprocess['stimulus-duration']\n    time_length = int(duration/binSize)\n    n_area_X_units, n_area_Y_units = area_X_responses.shape[0], area_Y_responses.shape[0]\n\n    # Make RRR\n    if log:\n        print('Make RRR...')\n    coefficients = np.zeros((n_area_X_units, n_area_Y_units, time_length))\n    for time in range(time_length):\n        coefficients[:, :, time]= RRRR(area_X_responses[:, :, time].T, area_Y_responses[:, :, time].T, log=False).T\n\n    if log:\n        print('coefficients.shape', coefficients.shape)\n\n    return {\n        'coefficients': coefficients\n    }\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.control_models","title":"<code>control_models(predictor_names=['V1', 'movement', 'pupil'], response_name='V2', log=False)</code>","text":"<p>Perform control models analysis using Reduced Rank Regression (RRR).</p> <p>This function loads the necessary data, transforms the behavioral data to match the neuronal data, and performs RRR analysis at each time point. The analysis is performed using the V1 neuronal activity as predictors and V2 neuronal activity as the target variable. The behavioral data (movement and pupil) can also be included as predictors.</p> <p>Parameters:</p> Name Type Description Default <code>predictor_names</code> <code>list</code> <p>Define, which data to concatenate into the predictors. The options are: 'V1', 'movement', 'pupil'.</p> <code>['V1', 'movement', 'pupil']</code> <code>response_name</code> <code>str</code> <p>The name of the outcome variable. Default is 'V2'.</p> <code>'V2'</code> <code>log</code> <code>bool</code> <p>Whether to log the progress. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>results</code> <code>ndarray</code> <p>Array of shape (T, cv) containing the RRR test scores at each time point.</p> Source code in <code>analyses/rrr.py</code> <pre><code>def control_models(predictor_names=['V1', 'movement', 'pupil'], response_name='V2', log=False) -&gt; np.ndarray:\n    '''\n    Perform control models analysis using Reduced Rank Regression (RRR).\n\n    This function loads the necessary data, transforms the behavioral data to match the neuronal data,\n    and performs RRR analysis at each time point. The analysis is performed using the V1 neuronal activity\n    as predictors and V2 neuronal activity as the target variable. The behavioral data (movement and pupil)\n    can also be included as predictors.\n\n    Parameters:\n        predictor_names (list): Define, which data to concatenate into the predictors. The options are: 'V1', 'movement', 'pupil'.\n        response_name (str): The name of the outcome variable. Default is 'V2'.\n        log (bool): Whether to log the progress. Default is False.\n\n    Returns:\n        results (numpy.ndarray): Array of shape (T, cv) containing the RRR test scores at each time point.\n    '''\n\n    # Load the data\n    V1 = load_pickle(\"5_block_VISp-activity\", path=\"data/area-responses\") # shape (Neurons, Trials, Time)\n    V2 = load_pickle(\"5_block_VISl-activity\", path=\"data/area-responses\") # shape (Neurons, Trials, Time)\n    movement = load_pickle(\"5_block_running-speed\", path=\"data/behav-responses\") # shape (Trials, Time)\n    pupil = load_pickle(\"5_block_pupil-area\", path=\"data/behav-responses\") # shape (Trials, Time)\n\n    # Transform the behav data to amtch the V1 and V2 data\n    movement = movement[np.newaxis, :, :]\n    pupil = pupil[np.newaxis, :, :]\n\n    # Get the number of neurons, trials, and time\n    N_1, K, T = V1.shape\n    N_2 = V2.shape[0]\n    if log:\n        print(f'V1 shape: {V1.shape}, V2 shape: {V2.shape}, movement shape: {movement.shape}, pupil shape: {pupil.shape}')\n\n    # Init the results bs shape (T, cv)\n    results = np.zeros((params['cv'], T))\n\n    # Loop through the time\n    for t in range(T):\n\n        # Make DataFrame from the data\n        X_V1  = pd.DataFrame(V1[:, :, t].T, columns=[f'V1_{i}' for i in range(V1.shape[0])])\n        X_mov = pd.DataFrame(movement[:, :, t].T, columns=['movement'])\n        X_pup = pd.DataFrame(pupil[:, :, t].T, columns=['pupil'])\n        Y_V2  = pd.DataFrame(V2[:, :, t].T, columns=[f'V2_{i}' for i in range(V2.shape[0])])\n\n        # Calculate mean values\n        X_V1_mean = X_V1.mean()\n        X_mov_mean = X_mov.mean()\n        X_pup_mean = X_pup.mean()\n        Y_V2_mean = Y_V2.mean()\n\n        # If any of the mean values are NaN, print a warning specifying which one\n        if log:\n            if X_V1_mean.isna().any():\n                print('X_V1_mean contains NaNs')\n            if X_mov_mean.isna().any():\n                print('X_mov_mean contains NaNs')\n            if X_pup_mean.isna().any():\n                print('X_pup_mean contains NaNs')\n            if Y_V2_mean.isna().any():\n                print('Y_V2_mean contains NaNs')\n\n        # Replace the NaNs with the mean\n        X_V1.fillna(X_V1_mean, inplace=True)\n        X_mov.fillna(X_mov_mean, inplace=True)\n        X_pup.fillna(X_pup_mean, inplace=True)\n        Y_V2.fillna(Y_V2_mean, inplace=True)\n\n        # Create a dictionary of the dataframes\n        dfs = {'V1': X_V1, 'movement': X_mov, 'pupil': X_pup, 'V2': Y_V2}\n\n        # Create a list from the dataframes if they are listed in the predictor names list\n        predictors = [df for name, df in dfs.items() if name in predictor_names]\n\n        # Concatenate the data\n        X = pd.concat(predictors, axis=1).values\n\n        # Get the outcome variable\n        Y = dfs[response_name].values\n\n        # if x contains nan, raise an error\n        if np.isnan(X).any():\n            raise ValueError('X contains NaNs')\n\n        # Make Reduced Rank Reegression\n        scores = RRRR(X, Y, rank=params['rank'], cv=params['cv'], log=log)\n\n        # Append the scores to the results\n        results[:, t] = scores['test_score']\n\n    return results\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.cross_time_rrr_coeffs","title":"<code>cross_time_rrr_coeffs(V1_activity, V2_activity, cv=None, rank=None)</code>","text":"<p>Calculate the cross-time RRR coefficients between two sets of activities.</p> <p>Parameters:</p> Name Type Description Default <code>V1_activity</code> <code>ndarray</code> <p>Array of activities for the first set.</p> required <code>V2_activity</code> <code>ndarray</code> <p>Array of activities for the second set.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of cross-time RRR coefficients. Shape (timelength_X, timelength_Y)</p> Source code in <code>analyses/rrr.py</code> <pre><code>def cross_time_rrr_coeffs(V1_activity, V2_activity, cv=None, rank=None) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the cross-time RRR coefficients between two sets of activities.\n\n    Parameters:\n        V1_activity (np.ndarray): Array of activities for the first set.\n        V2_activity (np.ndarray): Array of activities for the second set.\n\n    Returns:\n        np.ndarray: Array of cross-time RRR coefficients. Shape (timelength_X, timelength_Y)\n\n    \"\"\"\n    return RRRR(V1_activity.mean(axis=0), V2_activity.mean(\n        axis=0), cv=cv, rank=rank, log=True)\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.crosstime_analysis","title":"<code>crosstime_analysis(predictor, target, cv, rank, scaling_factor=10, dataBalancing='none', ProgressBar=True)</code>","text":"<p>Perform cross-time analysis based on timpoints of rrr-param-search lag.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>ndarray</code> <p>Array of shape (neurons, trials, timepoints) representing the activity of the predictor area.</p> required <code>target</code> <code>ndarray</code> <p>Array of shape (neurons, trials, timepoints) representing the activity of the target area.</p> required <code>cv</code> <code>int</code> <p>Number of cross-validation folds to use.</p> required <code>rank</code> <code>int</code> <p>Rank of the reduced-rank regression model.</p> required <code>scaling_factor</code> <code>int</code> <p>Scaling factor for the time points. Defaults to 1.</p> <code>10</code> <code>dataBalancing</code> <code>str</code> <p>Type of data balancing to use. Defaults to 'none'.</p> <code>'none'</code> <code>ProgressBar</code> <code>bool</code> <p>Whether to display a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>crosstimeMatrix</code> <code>ndarray</code> <p>Array of shape (timepoints, timepoints) representing the cross-time RRR scores.</p> Source code in <code>analyses/rrr.py</code> <pre><code>def crosstime_analysis(predictor, target, cv, rank, scaling_factor=10, dataBalancing='none', ProgressBar=True):\n    \"\"\"\n    Perform cross-time analysis based on timpoints of rrr-param-search lag.\n\n    Parameters:\n        predictor (ndarray): Array of shape (neurons, trials, timepoints) representing the activity of the predictor area.\n        target (ndarray): Array of shape (neurons, trials, timepoints) representing the activity of the target area.\n        cv (int): Number of cross-validation folds to use.\n        rank (int): Rank of the reduced-rank regression model.\n        scaling_factor (int, optional): Scaling factor for the time points. Defaults to 1.\n        dataBalancing (str, optional): Type of data balancing to use. Defaults to 'none'.\n        ProgressBar (bool, optional): Whether to display a progress bar. Defaults to True.\n\n    Returns:\n        crosstimeMatrix (np.ndarray): Array of shape (timepoints, timepoints) representing the cross-time RRR scores.\n    \"\"\"\n    predictor_orig = predictor\n    target_orig = target\n\n    # Get the time bin\n    time_bin = int(preprocess[\"bin-size\"] * 1000) # in ms\n\n    # Define the parameters\n    xseries = np.arange(0, 200, scaling_factor)\n    yseries = np.arange(0, 200, scaling_factor)\n\n    # Init results\n    crosstimeMatrix = np.full((len(xseries), len(yseries)), fill_value=np.nan)\n\n    # If sample_size is greater than the number of samples, then log a warning and return empty results\n    if dataBalancing == 'undersampled':\n        y_length = target_orig.shape[0]\n        if params['sample-size'] &gt; y_length:\n            print(f\"Waring: sample_size ({params['sample-size']}) is greater than the number of samples ({y_length}). Returning empty results.\")\n            return crosstimeMatrix\n\n    # In case of undersampling lower boundary of layer 5 -&gt; do not calculate the other layers\n    # if dataBalancing == 'none': # TODO: wipe this out\n    #     return results\n\n    # Print progressbar\n    progress_bar_id = 'crosstime analysis'\n    if type(ProgressBar) == str:\n        progress_bar_id = ProgressBar\n        ProgressBar = True\n    if ProgressBar: manager.progress_bar(progress_bar_id, 0, len(xseries))\n\n    for x, t_x in enumerate(xseries):\n        for y, t_y in enumerate(yseries):\n\n            # Preprocess the data (the trial duration is only one bin now).\n            predictor = preprocess_area_responses(predictor_orig[:, :, t_x : t_x + time_bin], \n                                                  stimulus_duration=preprocess[\"bin-size\"], \n                                                  step_size=preprocess[\"step-size\"]).squeeze()\n            target    = preprocess_area_responses(target_orig[:, :, t_y : t_y + time_bin], \n                                                  stimulus_duration=preprocess[\"bin-size\"], \n                                                  step_size=preprocess[\"step-size\"]).squeeze()\n\n            # Calculate the RRRR\n            res = RFE_CV(predictor.T, target.T, rank=rank, cv=cv)\n\n            # Save results\n            crosstimeMatrix[x, y] = res\n\n        # Print progressbar\n        if ProgressBar: manager.progress_bar(progress_bar_id, x+1, len(xseries))\n\n    return crosstimeMatrix\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.getCoeffs","title":"<code>getCoeffs(model, log=False)</code>","text":"<p>Calculate the mean coefficients of a model over cross-validation folds.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>list</code> <p>A list of estimators representing the model.</p> required <code>log</code> <code>bool</code> <p>Whether to print the shape of the coefficients. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>mean_coefficients</code> <code>ndarray</code> <p>The mean coefficients of the model.</p> Note <p>If the model is None, None is returned.</p> Source code in <code>analyses/rrr.py</code> <pre><code>def getCoeffs(model, log=False):\n    \"\"\"\n    Calculate the mean coefficients of a model over cross-validation folds.\n\n    Parameters:\n        model (list): A list of estimators representing the model.\n        log (bool, optional): Whether to print the shape of the coefficients. Default is False.\n\n    Returns:\n        mean_coefficients (numpy.ndarray): The mean coefficients of the model.\n\n    Note:\n        If the model is None, None is returned.\n    \"\"\"\n\n    # If the model is None, return None\n    if model[0] is None:\n        return None\n\n    # Concatenate the coefficients over the cross-validation folds\n    coefficients = np.array(\n        [estimator.coef_ for estimator in model])\n    if log:\n        print('coefficients.shape', coefficients.shape)\n\n    # Calculate the mean of the coefficients\n    mean_coefficients = np.mean(coefficients, axis=0)\n\n    # Append the mean coefficients to the results\n    return mean_coefficients.T\n</code></pre>"},{"location":"references/experiments/rrr/#analyses.rrr.rrr_rank_analysis","title":"<code>rrr_rank_analysis(V1_activity, V2_activity, max_rank=15, cv=params['cv'], log=False)</code>","text":"<p>Perform Reduced Rank Regression (RRR) rank analysis.</p> Source code in <code>analyses/rrr.py</code> <pre><code>def rrr_rank_analysis(V1_activity, V2_activity, max_rank=15, cv=params['cv'], log=False):\n    \"\"\"\n    Perform Reduced Rank Regression (RRR) rank analysis.\n    \"\"\"\n\n    # Get the number of neurons, trials, and time points\n    N, K_V1, T = V1_activity.shape\n\n    # Define the range of ranks to iterate over\n    ranks = range(1, max_rank+1)\n\n    # Initialize the errors\n    test_scores = np.zeros((max_rank, T))\n\n    # Iterate over time\n    for t in range(T):\n        for rank in ranks:\n            X = V1_activity[:, :, t].T\n            Y = V2_activity[:, :, t].T\n\n            # Calculate rrr ridge using your rrrr function\n            models = RRRR(X, Y, rank=rank, cv=cv)\n\n            # Calculate the mean of the test scores above the cv-folds\n            test_score = np.mean(models['test_score'])\n\n            # Save the test score\n            test_scores[rank-1, t] = test_score\n\n            if log:\n                print(f\"Rank: {rank}, Time: {t}, Test Score: {test_score}\")\n\n    # If test scores are negative, set them to nan\n    test_scores[test_scores &lt; 0] = np.nan\n\n    return test_scores\n</code></pre>"},{"location":"references/experiments/time-lag-search/","title":"Time lag search","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/time-lag-search/#analysis","title":"Analysis","text":"<p>This module searches for the optimal time lag in the RRR model using cross-validation.</p> <p>The script loops through the time lags specified in the parameters and calculates the RRR model for each time lag. The results are saved as a pickle file in the <code>data/results</code> directory.</p> <p>Parameters:</p> <ul> <li><code>load</code>:<ul> <li><code>stimulus-block</code>: The name of the stimulus block to analyze.</li> </ul> </li> <li><code>preprocess</code>: Preprocessing parameters.</li> <li><code>rrr</code>: RRR parameters.</li> <li><code>best-rrr-params</code>: Best RRR parameters.</li> <li><code>rrr-param-search</code>: RRR parameter search.</li> </ul> <p>Input:</p> <ul> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;predictor&gt;-activity.pickle</code>: Pickle file containing the raw activity data for the predictor brain area.</li> <li><code>data/raw-area-responses/&lt;stimulus-block&gt;_block_&lt;target&gt;-activity.pickle</code>: Pickle file containing the raw activity data for the target brain area.</li> </ul> <p>Output:</p> <ul> <li><code>data/results/time-lag-search.pickle</code>: Pickle file containing the results of the time lag search.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>analyses.data_preprocessing</code>: Module for data preprocessing.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/time-lag-search/#plot","title":"Plot","text":"<p>This module plots the time-lag search.</p> <p>Parameters:</p> <ul> <li><code>rrr-param-search</code>: The RRR parameter search parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/time-lag-search.pickle</code>: Pickle file containing the time-lag search results.</li> </ul> <p>Output:</p> <ul> <li><code>figures/time-lag-search.png</code>: The time-lag search plot.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul>"},{"location":"references/experiments/time-lag/","title":"Time-lag search","text":"<p>See experiment history for more information.</p>"},{"location":"references/experiments/time-lag/#analysis","title":"Analysis","text":"<p>This module calculates the time lag between the V1 and V2 activity.</p> <p>The script loads the V1 and V2 activity data, calculates the RRR between the two areas, and finds the time lag that maximizes the RRR coefficients.</p> <p>Parameters:</p> <ul> <li><code>load</code>: Load parameters.</li> <li><code>preprocess</code>: Preprocessing parameters.</li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISp-activity.pickle</code>: V1 activity.</li> <li><code>data/area-responses/&lt;stimulus-block&gt;_block_VISl-activity.pickle</code>: V2 activity.</li> </ul> <p>Output:</p> <ul> <li><code>results/VISp_VISl_cross-time-coeffs.pickle</code>: Pickle file containing the RRR coefficients between V1 and V2.</li> <li><code>results/VISp_VISl_cross-time-lag.pickle</code>: Pickle file containing the time lag between V1 and V2.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> </ul> <p>This module searches for the optimal cv-fold and time lag in the RRR model using cross-validation.</p> <p>The script loops through the cross-validation folds, time lags, and ranks specified in the parameters and calculates the RRR model for each combination. The results are saved as a pickle file in the <code>data/rrr-results</code> directory.</p> <p>Parameters:</p> <ul> <li><code>rrr-cv-rank-time</code>:<ul> <li><code>cv</code>: A list of cross-validation folds to use.</li> <li><code>ranks</code>: A list of ranks to use.</li> <li><code>duration</code>: The duration of the time window.</li> <li><code>time-bin</code>: The time bin size.</li> <li><code>time-step</code>: The time step size.</li> </ul> </li> </ul> <p>Input:</p> <ul> <li><code>data/area-responses/5_block_VISp-activity.pickle</code>: Pickle file containing the raw activity data for the VISp brain area.</li> <li><code>data/area-responses/5_block_VISl-activity.pickle</code>: Pickle file containing the raw activity data for the VISl brain area.</li> </ul> <p>Output:</p> <ul> <li><code>data/rrr-results/CV-rank.pickle</code>: Pickle file containing the results of the cross-validation of the rank in the RRR model. Shape: (n_cv, n_rank)</li> <li><code>figures/CV-rank_cross-time.png</code>: Plot of the cross-validation of the rank in the RRR model.</li> </ul> <p>Submodules:</p> <ul> <li><code>analyses.rrr</code>: Module containing the RRRR function for calculating the RRR model.</li> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> </ul>"},{"location":"references/experiments/time-lag/#cv-rank-cross-time.calculate_something","title":"<code>calculate_something()</code>","text":"<p>Calculate the time lag between two time series.</p> Source code in <code>cv-rank-cross-time.py</code> <pre><code>def calculate_something():\n    '''\n    Calculate the time lag between two time series.\n    '''\n\n    # Create a results array\n    results = np.zeros((len(cv), len(ranks)))\n\n    # Loop through the cross-validation and rank\n    for i, c in enumerate(cv):\n        for j, r in enumerate(ranks):\n            result = cross_time_rrr_coeffs(X, Y, c, r)\n            results[i, j] = result['test_score'].mean()\n\n    # Cut off the negative values\n    results[results &lt; -0] = np.nan\n\n    print(results)\n\n    return results\n</code></pre>"},{"location":"references/experiments/time-lag/#plot","title":"Plot","text":"<p>This module plots the time lag between V1 and LM.</p> <p>Parameters:</p> <ul> <li><code>preprocess</code>: Preprocess parameters.</li> </ul> <p>Input:</p> <ul> <li><code>results/VISp_VISl_cross-time-coeffs.pickle</code>: Cross time correlation coefficients.</li> </ul> <p>Output:</p> <ul> <li><code>figures/Time_lag_between_V1_LM.png</code>: Plot of the time lag between V1 and LM.</li> </ul> <p>Submodules:</p> <ul> <li><code>utils.data_io</code>: Module for loading and saving data.</li> <li><code>utils.plots</code>: Module for plotting data.</li> </ul>"},{"location":"references/utilities/ccf-volumes/","title":"Common Coordinate Framework (CCF) Volumes","text":"<p>This submodule contains tools for assigning cortical layers to channels and units based on the Allen Brain Atlas Common Coordinate Framework (CCF) volumes.</p> <p>Created on Tue Apr  7 08:41:07 2020</p> <p>Author: joshs</p> <p>References:</p> <ul> <li>Cortical Layers using CCFv3 in Neuropixels Data</li> <li>CCFv3 Annotation Volume</li> </ul> <p>Functions:</p> <ul> <li>get_layer_name(acronym) -&gt; int: Get the layer number from the given acronym.</li> <li>get_structure_ids(df, annotations) -&gt; np.ndarray: Get the structure IDs for the given DataFrame.</li> <li>cortical_depth_calculation(channels) -&gt; pd.DataFrame: Calculate the cortical depth for the given channels.</li> <li>layer_assignment_to_channels(channels) -&gt; pd.DataFrame: Assign cortical layers to the given channels.</li> <li>cortical_layer_assignment(channels, units) -&gt; pd.DataFrame: Assign cortical layers to the given units.</li> </ul>"},{"location":"references/utilities/ccf-volumes/#utils.ccf_volumes.cortical_depth_calculation","title":"<code>cortical_depth_calculation(channels)</code>","text":"<p>Calculate the cortical depth for the given channels.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>DataFrame</code> <p>The DataFrame containing channel information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with a new column 'cortical_depth' that represents the cortical depth of each channel.</p> Source code in <code>utils/ccf_volumes.py</code> <pre><code>def cortical_depth_calculation(channels) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the cortical depth for the given channels.\n\n    Parameters:\n        channels (pd.DataFrame): The DataFrame containing channel information.\n\n    Returns:\n        pd.DataFrame: The DataFrame with a new column 'cortical_depth' that represents the cortical depth of each channel.\n    \"\"\"\n    channels = channels[channels.anterior_posterior_ccf_coordinate &gt; 0]\n\n    x = (channels.anterior_posterior_ccf_coordinate.values / 10).astype('int')\n    y = (channels.dorsal_ventral_ccf_coordinate.values / 10).astype('int')\n    z = (channels.left_right_ccf_coordinate.values / 10).astype('int')\n\n    cortical_depth = streamlines[x, y, z]\n\n    channels['cortical_depth'] = 0\n\n    channels.loc[channels.anterior_posterior_ccf_coordinate &gt; 0, 'cortical_depth'] = cortical_depth\n\n    return channels\n</code></pre>"},{"location":"references/utilities/ccf-volumes/#utils.ccf_volumes.cortical_layer_assignment","title":"<code>cortical_layer_assignment(channels, units)</code>","text":"<p>Assigns cortical layers to units based on their ecephys_channel_id.</p> <p>ATTENTION! Please keep in mind that these layer assignments are only estimates, and not definitive labels. </p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>DataFrame</code> <p>The DataFrame containing channel information.</p> required <code>units</code> <code>DataFrame</code> <p>The DataFrame containing unit information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with an additional 'layer' column containing the cortical layer assignments for each unit.</p> Source code in <code>utils/ccf_volumes.py</code> <pre><code>def cortical_layer_assignment(channels, units) -&gt; pd.DataFrame:\n    \"\"\"\n    Assigns cortical layers to units based on their ecephys_channel_id.\n\n    **ATTENTION! Please keep in mind that these layer assignments are only estimates, and not definitive labels**. \n\n    Args:\n        channels (pd.DataFrame): The DataFrame containing channel information.\n        units (pd.DataFrame): The DataFrame containing unit information.\n\n    Returns:\n        pd.DataFrame: The DataFrame with an additional 'layer' column containing the cortical layer assignments for each unit.\n    \"\"\"\n\n    # Get the channels with the layer assignment\n    channels = layer_assignment_to_channels(channels)\n\n    # Get the channel ids for the units\n    unit_channel_ids = units.peak_channel_id\n\n    # Filter channel_ids to only those that are in the channels index\n    filtered_channel_ids = unit_channel_ids[unit_channel_ids.isin(channels.index)]\n\n    # Get the layer assignments for the filtered units\n    layer_assignments = channels.loc[filtered_channel_ids].cortical_layer.values\n\n    # Initialize a new column in units with a default value\n    units['layer'] = np.nan\n\n    # Update only the filtered units\n    units.loc[units['peak_channel_id'].isin(filtered_channel_ids), 'layer'] = layer_assignments\n\n    return units\n</code></pre>"},{"location":"references/utilities/ccf-volumes/#utils.ccf_volumes.get_layer_name","title":"<code>get_layer_name(acronym)</code>","text":"<p>Get the layer number from the given acronym.</p> <p>Parameters:</p> Name Type Description Default <code>acronym</code> <code>str</code> <p>The acronym representing a cortical layer.</p> required <p>Returns:</p> Name Type Description <code>layer_number</code> <code>int</code> <p>The layer number corresponding to the given acronym.</p> Source code in <code>utils/ccf_volumes.py</code> <pre><code>def get_layer_name(acronym):\n    \"\"\"\n    Get the layer number from the given acronym.\n\n    Parameters:\n        acronym (str): The acronym representing a cortical layer.\n\n    Returns:\n        layer_number (int): The layer number corresponding to the given acronym.\n    \"\"\"\n    try:\n        layer = int(re.findall(r'\\d+', acronym)[0])\n        if layer == 3:\n            layer = 0\n        return layer\n    except IndexError:\n        return 0\n</code></pre>"},{"location":"references/utilities/ccf-volumes/#utils.ccf_volumes.get_structure_ids","title":"<code>get_structure_ids(df, annotations)</code>","text":"<p>Get the structure IDs for the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing coordinate values.</p> required <code>annotations</code> <code>ndarray</code> <p>The annotation volume.</p> required <p>Returns:</p> Name Type Description <code>structure_ids</code> <code>ndarray</code> <p>The structure IDs corresponding to the given DataFrame.</p> Source code in <code>utils/ccf_volumes.py</code> <pre><code>def get_structure_ids(df, annotations):\n    \"\"\"\n    Get the structure IDs for the given DataFrame.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame containing coordinate values.\n        annotations (np.ndarray): The annotation volume.\n\n    Returns:\n        structure_ids (np.ndarray): The structure IDs corresponding to the given DataFrame.\n    \"\"\"\n    x = (df.anterior_posterior_ccf_coordinate.values / 10).astype('int')\n    y = (df.dorsal_ventral_ccf_coordinate.values / 10).astype('int')\n    z = (df.left_right_ccf_coordinate.values / 10).astype('int')\n\n    x[x &lt; 0] = 0\n    y[y &lt; 0] = 0\n    z[z &lt; 0] = 0\n\n    structure_ids = annotations[x, y, z] - 1 # annotation volume is Matlab-indexed\n\n    return structure_ids\n</code></pre>"},{"location":"references/utilities/ccf-volumes/#utils.ccf_volumes.layer_assignment_to_channels","title":"<code>layer_assignment_to_channels(channels)</code>","text":"<p>Assign cortical layers to the given channels.</p> <p>ATTENTION! Please keep in mind that these layer assignments are only estimates, and not definitive labels. </p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>DataFrame</code> <p>The DataFrame containing channel information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with a new column 'cortical_layer' that assigns each channel to a cortical layer.</p> Source code in <code>utils/ccf_volumes.py</code> <pre><code>def layer_assignment_to_channels(channels) -&gt; pd.DataFrame:\n    \"\"\"\n    Assign cortical layers to the given channels.\n\n    ATTENTION! Please keep in mind that these layer assignments are only estimates, and not definitive labels. \n\n    Parameters:\n        channels (pd.DataFrame): The DataFrame containing channel information.\n\n    Returns:\n        pd.DataFrame: The DataFrame with a new column 'cortical_layer' that assigns each channel to a cortical layer.\n    \"\"\"\n    channels = cortical_depth_calculation(channels)\n\n    structure_ids = get_structure_ids(channels, annotations)\n    structure_acronyms = structure_tree.loc[structure_ids].acronym\n\n    layers = [get_layer_name(acronym) for acronym in structure_acronyms]\n\n    channels['cortical_layer'] = layers\n\n    return channels\n</code></pre>"},{"location":"references/utilities/data-io/","title":"Data I/O","text":"<p>This submodule contains tools for saving and loading data.</p> <p>Tools:</p> <ul> <li>path_name(path: str, name: str) -&gt; str: Returns the full path name.</li> <li>save_csv(data: List[List[Any]], name: str, path: str = \"results\") -&gt; None: Saves the data to a CSV file.</li> <li>save_pickle(data: Any, name: str, path: str = \"results\") -&gt; None: Saves the data to a pickle file.</li> <li>save_fig(fig: matplotlib.figure.Figure, name: str, path: str = \"figures\") -&gt; None: Saves the figure to a PNG file.</li> <li>save_based_on_type(data: Any, name: str, path: str = \"results\") -&gt; None: Saves the data based on its type.</li> <li>save_dict_items(dictionary: Dict[str, Any], name: str = \"\", path: str = \"results\", log: bool = True) -&gt; None: Saves the items in a dictionary.</li> <li>load_csv(name: str, path: str = \"\") -&gt; pd.DataFrame: Loads the data from a CSV file.</li> <li>load_pickle(name: str, path: str = \"results\") -&gt; Any: Loads the data from a pickle file.</li> </ul>"},{"location":"references/utilities/data-io/#utils.data_io.load_csv","title":"<code>load_csv(name, path='')</code>","text":"<p>Loads the data from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to the file. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The loaded data.</p> Source code in <code>utils/data_io.py</code> <pre><code>def load_csv(name: str, path: str = \"\") -&gt; pd.DataFrame:\n    \"\"\"\n    Loads the data from a CSV file.\n\n    Args:\n        name (str): The name of the file.\n        path (str, optional): The path to the file. Defaults to \"\".\n\n    Returns:\n        pd.DataFrame: The loaded data.\n    \"\"\"\n    name = path_name(path, name)\n    data = pd.read_csv(f\"{name}.csv\")\n    return data\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.load_pickle","title":"<code>load_pickle(name, path='results')</code>","text":"<p>Loads the data from a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to the file. Defaults to \"results\".</p> <code>'results'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded data.</p> Source code in <code>utils/data_io.py</code> <pre><code>def load_pickle(name: str, path: str = \"results\") -&gt; Any:\n    \"\"\"\n    Loads the data from a pickle file.\n\n    Args:\n        name (str): The name of the file.\n        path (str, optional): The path to the file. Defaults to \"results\".\n\n    Returns:\n        Any: The loaded data.\n    \"\"\"\n    name = path_name(path, name)\n    with open(f\"{name}.pickle\", \"rb\") as f:\n        data = pickle.load(f)\n    return data\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.path_name","title":"<code>path_name(path, name)</code>","text":"<p>Returns the full path name by concatenating the path and name.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file.</p> required <code>name</code> <code>str</code> <p>The name of the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full path name.</p> Source code in <code>utils/data_io.py</code> <pre><code>def path_name(path: str, name: str) -&gt; str:\n    \"\"\"\n    Returns the full path name by concatenating the path and name.\n\n    Args:\n        path (str): The path to the file.\n        name (str): The name of the file.\n\n    Returns:\n        str: The full path name.\n    \"\"\"\n    if path != \"\":\n        if path[-1] != \"/\":\n            path = path + \"/\"\n        name = f'{path}{name}'\n    return name\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.save_based_on_type","title":"<code>save_based_on_type(data, name, path='results')</code>","text":"<p>Saves the data based on its type.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to be saved.</p> required <code>name</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to save the file. Defaults to \"results\".</p> <code>'results'</code> Source code in <code>utils/data_io.py</code> <pre><code>def save_based_on_type(data: Any, name: str, path: str = \"results\") -&gt; None:\n    \"\"\"\n    Saves the data based on its type.\n\n    Args:\n        data (Any): The data to be saved.\n        name (str): The name of the file.\n        path (str, optional): The path to save the file. Defaults to \"results\".\n    \"\"\"\n    if type(data) == np.ndarray:\n        save_csv(data, name, path=path)\n    else:\n        save_pickle(data, name, path=path)\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.save_csv","title":"<code>save_csv(data, name, path='results')</code>","text":"<p>Saves the data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[List[Any]]</code> <p>The data to be saved.</p> required <code>name</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to save the file. Defaults to \"results\".</p> <code>'results'</code> Source code in <code>utils/data_io.py</code> <pre><code>def save_csv(data: List[List[Any]], name: str, path: str = \"results\") -&gt; None:\n    \"\"\"\n    Saves the data to a CSV file.\n\n    Args:\n        data (List[List[Any]]): The data to be saved.\n        name (str): The name of the file.\n        path (str, optional): The path to save the file. Defaults to \"results\".\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    name = path_name(path, name)\n    with open(f\"{name}.csv\", \"w\") as f:\n        writer = csv.writer(f, delimiter=\",\")\n        writer.writerows(data)\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.save_dict_items","title":"<code>save_dict_items(dictionary, name='', path='results', log=True)</code>","text":"<p>Saves the items in a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>Dict[str, Any]</code> <p>The dictionary containing the items to be saved.</p> required <code>name</code> <code>str</code> <p>The name prefix for the saved files. Defaults to \"\".</p> <code>''</code> <code>path</code> <code>str</code> <p>The path to save the files. Defaults to \"results\".</p> <code>'results'</code> <code>log</code> <code>bool</code> <p>Whether to log the saving process. Defaults to True.</p> <code>True</code> Source code in <code>utils/data_io.py</code> <pre><code>def save_dict_items(dictionary: Dict[str, Any], name: str = \"\", path: str = \"results\", log: bool = True) -&gt; None:\n    \"\"\"\n    Saves the items in a dictionary.\n\n    Args:\n        dictionary (Dict[str, Any]): The dictionary containing the items to be saved.\n        name (str, optional): The name prefix for the saved files. Defaults to \"\".\n        path (str, optional): The path to save the files. Defaults to \"results\".\n        log (bool, optional): Whether to log the saving process. Defaults to True.\n    \"\"\"\n    if name != \"\":\n        name = name + \"_\"\n    for key, value in dictionary.items():\n        if log:\n            print(\"Saving\", key, type(value))\n        save_pickle(value, f\"{name}{key}\", path=path)\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.save_fig","title":"<code>save_fig(fig, name, path='figures')</code>","text":"<p>Saves the figure to a PNG file.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to be saved.</p> required <code>name</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to save the file. Defaults to \"figures\".</p> <code>'figures'</code> Source code in <code>utils/data_io.py</code> <pre><code>def save_fig(fig: matplotlib.figure.Figure, name: str, path: str = \"figures\") -&gt; None:\n    \"\"\"\n    Saves the figure to a PNG file.\n\n    Args:\n        fig (matplotlib.figure.Figure): The figure to be saved.\n        name (str): The name of the file.\n        path (str, optional): The path to save the file. Defaults to \"figures\".\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    fig.savefig(f\"{path}/{name}.png\")\n</code></pre>"},{"location":"references/utilities/data-io/#utils.data_io.save_pickle","title":"<code>save_pickle(data, name, path='results')</code>","text":"<p>Saves the data to a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to be saved.</p> required <code>name</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to save the file. Defaults to \"results\".</p> <code>'results'</code> Source code in <code>utils/data_io.py</code> <pre><code>def save_pickle(data: Any, name: str, path: str = \"results\") -&gt; None:\n    \"\"\"\n    Saves the data to a pickle file.\n\n    Args:\n        data (Any): The data to be saved.\n        name (str): The name of the file.\n        path (str, optional): The path to save the file. Defaults to \"results\".\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    name = path_name(path, name)\n    with open(f\"{name}.pickle\", \"wb\") as f:\n        pickle.dump(data, f)\n</code></pre>"},{"location":"references/utilities/debug/","title":"Debug tools","text":"<p>This submodule provides different tools for debugging variables and their values.</p> <p>It includes functions for generating shape names based on array dimensions, getting modified shape names, and retrieving array heads by dimension pairs. The module also includes a class called <code>debug</code> that allows for convenient printing of variable names and values for debugging purposes.</p> <p>Functions: - shape_name_short(arr, i, j): Generates a shape name based on the dimensions of the input array. - shape_name_long(arr, i, j): Returns a string representation of the shape of the input array with modified dimensions. - get_head_by_dimension_pairs(arr, m=0, n=5, log=True): Returns a list of shape names and corresponding array heads for all dimension pairs in the input array. - hasharr(arr): Calculate the hash value of a numpy array.</p> <p>Classes: - debug: A class for debugging variables and their values.</p> <p>Example usage:</p> <p>arr = np.zeros((3, 4, 5)) shape_name_short(arr, 0, 2) shape_name_long(arr, 0, 2) get_head_by_dimension_pairs(arr, m=0, n=2) debug(1, 2, 3, first=1, last=3) hasharr(arr)</p>"},{"location":"references/utilities/debug/#utils.debug.debug","title":"<code>debug</code>","text":"<p>A class for debugging variables and their values.</p> <p>This class provides a convenient way to print the names and values of variables for debugging purposes. It supports printing numpy arrays with different dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable arguments to be debugged.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to set the parameters of the debug instance.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>first</code> <code>int</code> <p>The starting index for slicing arrays. Default is 0.</p> <code>last</code> <code>int</code> <p>The ending index for slicing arrays. Default is 5. If negative, all elements are included.</p> <p>Attributes:</p> Name Type Description <code>first</code> <code>int</code> <p>The starting index for slicing arrays.</p> <code>last</code> <code>int</code> <p>The ending index for slicing arrays.</p> Example <p>debug(1, 2, 3, first=1, last=3)</p> Source code in <code>utils/debug.py</code> <pre><code>class debug:\n    \"\"\"\n    A class for debugging variables and their values.\n\n    This class provides a convenient way to print the names and values of variables for debugging purposes.\n    It supports printing numpy arrays with different dimensions.\n\n    Args:\n        *args: Variable arguments to be debugged.\n        **kwargs: Keyword arguments to set the parameters of the debug instance.\n\n    Keyword Args:\n        first (int): The starting index for slicing arrays. Default is 0.\n        last (int): The ending index for slicing arrays. Default is 5. If negative, all elements are included.\n\n    Attributes:\n        first (int): The starting index for slicing arrays.\n        last (int): The ending index for slicing arrays.\n\n    Example:\n        &gt;&gt;&gt; debug(1, 2, 3, first=1, last=3)\n        # Output:\n        # var_name: 1, var_content: 2\n        # var_name: 2, var_content: 3\n    \"\"\"\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Prints the names and values of the variables passed as arguments.\n\n        Args:\n            *args: Variable arguments to be debugged.\n            **kwargs: Keyword arguments to set the parameters of the debug instance.\n        \"\"\"\n        n = 5\n        frame = inspect.currentframe()\n        frame = inspect.getouterframes(frame)[1]\n        code_context = inspect.getframeinfo(frame[0]).code_context[0].strip()\n        arg_names = code_context[code_context.find('(') + 1:-1].split(',')\n        arg_names = [name.strip() for name in arg_names]\n        for i, var in enumerate(args):\n            var_name = arg_names[i]\n            var_content = var\n            if type(var_content) == np.ndarray:\n                if var_content.ndim == 1:\n                    ic(var_name, var_content)\n                    continue\n                elif var_content.ndim == 2:\n                    var_head = var_content[self.first:, self.first:]\n                    var_shape = var_content.shape\n                    ic(var_name, var_shape, var_head)\n                    continue\n                else:\n                    ic(var_name)\n                    var_head = get_head_by_dimension_pairs(var_content, m=self.first, n=self.last)\n            else:\n                ic(var_name, var_content)\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initializes a debug instance and calls the __call__ method.\n\n        Args:\n            *args: Variable arguments to be debugged.\n            **kwargs: Keyword arguments to set the parameters of the debug instance.\n        \"\"\"\n        self.params(**kwargs)\n        self.__call__(*args, **kwargs)\n\n    def params(self, first=0, last=5):\n        \"\"\"\n        Sets the parameters of the debug instance.\n\n        Args:\n            first (int): The starting index for slicing arrays.\n            last (int): The ending index for slicing arrays.\n        \"\"\"\n        if first &lt; 0:\n            last = None\n        self.first = first\n        self.last = last\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.debug--output","title":"Output:","text":""},{"location":"references/utilities/debug/#utils.debug.debug--var_name-1-var_content-2","title":"var_name: 1, var_content: 2","text":""},{"location":"references/utilities/debug/#utils.debug.debug--var_name-2-var_content-3","title":"var_name: 2, var_content: 3","text":""},{"location":"references/utilities/debug/#utils.debug.debug.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Prints the names and values of the variables passed as arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable arguments to be debugged.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to set the parameters of the debug instance.</p> <code>{}</code> Source code in <code>utils/debug.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"\n    Prints the names and values of the variables passed as arguments.\n\n    Args:\n        *args: Variable arguments to be debugged.\n        **kwargs: Keyword arguments to set the parameters of the debug instance.\n    \"\"\"\n    n = 5\n    frame = inspect.currentframe()\n    frame = inspect.getouterframes(frame)[1]\n    code_context = inspect.getframeinfo(frame[0]).code_context[0].strip()\n    arg_names = code_context[code_context.find('(') + 1:-1].split(',')\n    arg_names = [name.strip() for name in arg_names]\n    for i, var in enumerate(args):\n        var_name = arg_names[i]\n        var_content = var\n        if type(var_content) == np.ndarray:\n            if var_content.ndim == 1:\n                ic(var_name, var_content)\n                continue\n            elif var_content.ndim == 2:\n                var_head = var_content[self.first:, self.first:]\n                var_shape = var_content.shape\n                ic(var_name, var_shape, var_head)\n                continue\n            else:\n                ic(var_name)\n                var_head = get_head_by_dimension_pairs(var_content, m=self.first, n=self.last)\n        else:\n            ic(var_name, var_content)\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.debug.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes a debug instance and calls the call method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable arguments to be debugged.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to set the parameters of the debug instance.</p> <code>{}</code> Source code in <code>utils/debug.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initializes a debug instance and calls the __call__ method.\n\n    Args:\n        *args: Variable arguments to be debugged.\n        **kwargs: Keyword arguments to set the parameters of the debug instance.\n    \"\"\"\n    self.params(**kwargs)\n    self.__call__(*args, **kwargs)\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.debug.params","title":"<code>params(first=0, last=5)</code>","text":"<p>Sets the parameters of the debug instance.</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>int</code> <p>The starting index for slicing arrays.</p> <code>0</code> <code>last</code> <code>int</code> <p>The ending index for slicing arrays.</p> <code>5</code> Source code in <code>utils/debug.py</code> <pre><code>def params(self, first=0, last=5):\n    \"\"\"\n    Sets the parameters of the debug instance.\n\n    Args:\n        first (int): The starting index for slicing arrays.\n        last (int): The ending index for slicing arrays.\n    \"\"\"\n    if first &lt; 0:\n        last = None\n    self.first = first\n    self.last = last\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.get_head_by_dimension_pairs","title":"<code>get_head_by_dimension_pairs(arr, m=0, n=5, log=True)</code>","text":"<p>Returns a list of shape names and corresponding array heads for all dimension pairs in the input array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array.</p> required <code>m</code> <code>int</code> <p>The starting index for slicing arrays. Default is 0.</p> <code>0</code> <code>n</code> <code>int</code> <p>The number of elements to include in the sliced arrays. Default is 5.</p> <code>5</code> <code>log</code> <code>bool</code> <p>Whether to log the shape names and array heads. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of shape names and corresponding array heads.</p> Example <p>import numpy as np arr = np.zeros((3, 4, 5)) get_head_by_dimension_pairs(arr, m=0, n=2) ['(&gt;2&lt;, 4, 5)', array([[[0., 0., 0., 0., 0.],                        [0., 0., 0., 0., 0.]],                       [[0., 0., 0., 0., 0.],                        [0., 0., 0., 0., 0.]],                       [[0., 0., 0., 0., 0.],                        [0., 0., 0., 0., 0.]]])]</p> Source code in <code>utils/debug.py</code> <pre><code>def get_head_by_dimension_pairs(arr, m=0, n=5, log=True):\n    \"\"\"\n    Returns a list of shape names and corresponding array heads for all dimension pairs in the input array.\n\n    Args:\n        arr (ndarray): The input array.\n        m (int): The starting index for slicing arrays. Default is 0.\n        n (int): The number of elements to include in the sliced arrays. Default is 5.\n        log (bool): Whether to log the shape names and array heads. Default is True.\n\n    Returns:\n        list: A list of shape names and corresponding array heads.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; arr = np.zeros((3, 4, 5))\n        &gt;&gt;&gt; get_head_by_dimension_pairs(arr, m=0, n=2)\n        ['(&gt;2&lt;, 4, 5)', array([[[0., 0., 0., 0., 0.],\n                               [0., 0., 0., 0., 0.]],\n                              [[0., 0., 0., 0., 0.],\n                               [0., 0., 0., 0., 0.]],\n                              [[0., 0., 0., 0., 0.],\n                               [0., 0., 0., 0., 0.]]])]\n    \"\"\"\n    result = []\n    for i in range(arr.ndim):\n        for j in range(i+1, arr.ndim):\n            index = [0] * arr.ndim\n            index[i] = slice(m, m+n)\n            index[j] = slice(m, m+n)\n            index = tuple(index)\n            var_head = arr[index]\n            var_dimensions = shape_name_long(arr, i, j)\n            result.append(var_dimensions)\n            result.append(var_head)\n            if log:\n                ic(var_dimensions, var_head)\n    return result\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.hasharr","title":"<code>hasharr(arr)</code>","text":"<p>Calculate the hash value of a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The hash value of the array.</p> Source code in <code>utils/debug.py</code> <pre><code>def hasharr(arr: np.ndarray):\n    \"\"\"\n    Calculate the hash value of a numpy array.\n\n    Parameters:\n        arr (numpy.ndarray): The input array.\n\n    Returns:\n        str: The hash value of the array.\n\n    \"\"\"\n    if not isinstance(arr, np.ndarray):\n        arr = arr.to_numpy()\n    hash_value = hashlib.blake2b(arr.tobytes(), digest_size=20).hexdigest()\n    return hash_value\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.shape_name_long","title":"<code>shape_name_long(arr, i, j)</code>","text":"<p>Returns a string representation of the shape of the input array with modified dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array.</p> required <code>i</code> <code>int</code> <p>The index of the first dimension to modify.</p> required <code>j</code> <code>int</code> <p>The index of the second dimension to modify.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the modified shape of the input array.</p> Example <p>import numpy as np arr_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) shape_name_long(arr_3d, 0, 2) '(&gt;2&lt;, 2, &gt;2&lt;)'</p> Source code in <code>utils/debug.py</code> <pre><code>def shape_name_long(arr, i, j):\n    \"\"\"\n    Returns a string representation of the shape of the input array with modified dimensions.\n\n    Args:\n        arr (ndarray): The input array.\n        i (int): The index of the first dimension to modify.\n        j (int): The index of the second dimension to modify.\n\n    Returns:\n        str: A string representation of the modified shape of the input array.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; arr_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        &gt;&gt;&gt; shape_name_long(arr_3d, 0, 2)\n        '(&gt;2&lt;, 2, &gt;2&lt;)'\n    \"\"\"\n    ith_dim, jth_dim = arr.shape[i], arr.shape[j]\n    ith_name, jth_name = f'&gt;{ith_dim}&lt;', f'&gt;{jth_dim}&lt;'\n    shape_name = []\n    for k in range(arr.ndim):\n        if k == i:\n            shape_name.append(ith_name)\n        elif k == j:\n            shape_name.append(jth_name)\n        else:\n            shape_name.append(arr.shape[k])\n    shape_name = \", \".join(map(str, shape_name))\n    shape_name = f'({shape_name})'\n    return shape_name\n</code></pre>"},{"location":"references/utilities/debug/#utils.debug.shape_name_short","title":"<code>shape_name_short(arr, i, j)</code>","text":"<p>Generates a shape name based on the dimensions of the input array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array.</p> required <code>i</code> <code>int</code> <p>The index of the first dimension to mark with 'X' in the shape name.</p> required <code>j</code> <code>int</code> <p>The index of the second dimension to mark with 'X' in the shape name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The generated shape name.</p> Example <p>import numpy as np arr = np.zeros((3, 4, 5)) shape_name_short(arr, 0, 2) 'shape_name_short (str): head_of_dims_XOX'</p> Source code in <code>utils/debug.py</code> <pre><code>def shape_name_short(arr, i, j):\n    \"\"\"\n    Generates a shape name based on the dimensions of the input array.\n\n    Args:\n        arr (ndarray): The input array.\n        i (int): The index of the first dimension to mark with 'X' in the shape name.\n        j (int): The index of the second dimension to mark with 'X' in the shape name.\n\n    Returns:\n        str: The generated shape name.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; arr = np.zeros((3, 4, 5))\n        &gt;&gt;&gt; shape_name_short(arr, 0, 2)\n        'shape_name_short (str): head_of_dims_XOX'\n    \"\"\"\n    shape_name = ['O'] * arr.ndim\n    shape_name[i] = 'X'  # f'&gt;{arr.shape[i]}&lt;'\n    shape_name[j] = 'X'  # f'&gt;{arr.shape[j]}&lt;'\n    shape_name = \"\".join(shape_name)\n    shape_name = f'head_of_dims_{shape_name}'\n    return shape_name\n</code></pre>"},{"location":"references/utilities/direct-download/","title":"Direct Download","text":"<p>The Direct Download utility allows you to download the dataset directly from an URL.</p> <p>Utility tool for retrieving the download links for all sessions in a given manifest file.</p> <p>Functions: - retrieve_link(session_id): Retrieves the download link for the given session ID. - get_download_links(manifest_path): Retrieves the download links for all sessions in the given manifest file.</p>"},{"location":"references/utilities/direct-download/#utils.directDownload.get_download_links","title":"<code>get_download_links(manifest_path)</code>","text":"<p>Retrieves the download links for all sessions in the given manifest file.</p> <p>Parameters:</p> Name Type Description Default <code>manifest_path</code> <code>str</code> <p>The path to the manifest file.</p> required <p>Returns:</p> Name Type Description <code>download_links</code> <code>list</code> <p>A list of download links for each session.</p> Source code in <code>utils/directDownload.py</code> <pre><code>def get_download_links(manifest_path):\n    \"\"\"\n    Retrieves the download links for all sessions in the given manifest file.\n\n    Parameters:\n        manifest_path (str): The path to the manifest file.\n\n    Returns:\n        download_links (list): A list of download links for each session.\n    \"\"\"\n\n    cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n\n    sessions = cache.get_session_table()\n\n    download_links = [retrieve_link(session_id)\n                    for session_id in sessions.index.values]\n\n    for session_id, link in zip(sessions.index.values, download_links):\n        print(f\"Session ID: {session_id}, Download Link: {link}\")\n\n    return download_links\n</code></pre>"},{"location":"references/utilities/direct-download/#utils.directDownload.retrieve_link","title":"<code>retrieve_link(session_id)</code>","text":"<p>Retrieves the download link for the given session ID.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>int</code> <p>The ID of the session.</p> required <p>Returns:</p> Name Type Description <code>download_link</code> <code>str</code> <p>The download link for the session.</p> Source code in <code>utils/directDownload.py</code> <pre><code>def retrieve_link(session_id):\n    \"\"\"\n    Retrieves the download link for the given session ID.\n\n    Parameters:\n        session_id (int): The ID of the session.\n\n    Returns:\n        download_link (str): The download link for the session.\n    \"\"\"\n\n    well_known_files = build_and_execute(\n        (\n            \"criteria=model::WellKnownFile\"\n            \",rma::criteria,well_known_file_type[name$eq'EcephysNwb']\"\n            \"[attachable_type$eq'EcephysSession']\"\n            r\"[attachable_id$eq{{session_id}}]\"\n        ),\n        engine=rma_engine.get_rma_tabular,\n        session_id=session_id\n    )\n\n    return 'http://api.brain-map.org/' + well_known_files['download_link'].iloc[0]\n</code></pre>"},{"location":"references/utilities/megaplot/","title":"Megaplot","text":"<p>This submodule contains a tool named <code>megaplot</code> for creating and managing subplots in matplotlib.</p> <p>Classes:</p> <ul> <li>SubPlot: A class for creating and managing subplots in matplotlib.</li> <li>megaplot: A class used to make matplotlib subplot and its changes more convenient.</li> </ul> <p>Functions in megaplot:</p> <ul> <li>SubPlot.init(self, ncols, nrows, name, constrained_layout=True): Initializes a SubPlot object with the specified number of columns and rows.</li> <li>SubPlot.ax(self, x, y) -&gt; Axes: Returns the Axes object at the specified position.</li> <li>SubPlot.addrow(self): Adds a new row to the subplot grid.</li> <li>megaplot.init(self, argv, fig=None, gs=None, nrows=None, ncols=None, title=None, xlabel=None, ylabel=None, constrained_layout=False, log=True, *kwargs): Initializes a megaplot object with the specified parameters.</li> <li>megaplot.getitem(self, items) -&gt; Axes: Returns the Axes object at the specified position.</li> <li>megaplot.setitem(self, items, artist): Sets the Axes object at the specified position.</li> <li>megaplot.len(self): Returns the number of Axes objects in the megaplot.</li> <li>megaplot.iter(self): Returns an iterator for iterating over the Axes objects in the megaplot.</li> <li>megaplot.show(self): Displays the figure.</li> <li>megaplot.save(self, filename, type='png', path='outputs', makedir=False, log=None, **kwargs): Saves the figure to a file.</li> <li>megaplot.append2pdf(self, pdf:PdfPages): Appends the figure to an existing PDF file.</li> <li>SubPlot.ncols: Number of columns in the subplot grid.</li> <li>SubPlot.nrows: Number of rows in the subplot grid.</li> <li>SubPlot.fig: The matplotlib figure object.</li> <li>SubPlot.gs: The matplotlib GridSpec object.</li> <li>SubPlot.size: The size of the subplot grid.</li> <li>SubPlot.row_names: Names of the rows in the subplot grid.</li> <li>SubPlot.col_names: Names of the columns in the subplot grid.</li> <li>megaplot.nrows: Number of rows in the subplot grid.</li> <li>megaplot.ncols: Number of columns in the subplot grid.</li> <li>megaplot.log: Whether to print logs.</li> <li>megaplot.row_names: Names of the rows in the subplot grid.</li> <li>megaplot.col_names: Names of the columns in the subplot grid.</li> <li>megaplot.fig: The matplotlib figure object.</li> <li>megaplot.table: The table containing the indices of the Axes objects in the figure.</li> </ul>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot","title":"<code>megaplot</code>","text":"Source code in <code>utils/megaplot.py</code> <pre><code>class megaplot():\n\n    def __init__(self, *argv, fig=None, gs=None, nrows=None, ncols=None, title=None, xlabel=None, ylabel=None, constrained_layout=False, log=True, **kwargs):\n        \"\"\"\n        A class used to make matplotlib subplot and its changes more convenient.\n\n        Args:\n            *argv (tuple): Tuple of (nrows, ncols) if provided as separate arguments.\n            fig (pyplot.figure): The figure to use. If not provided, a new figure will be created.\n            gs (GridSpec): The GridSpec to use. If not provided, a new GridSpec will be created.\n            nrows (int): Number of rows in the subplot grid. If not provided, it will be determined based on the fig or gs.\n            ncols (int): Number of columns in the subplot grid. If not provided, it will be determined based on the fig or gs.\n            title (str): The title of the figure.\n            xlabel (str): The label for the x-axis.\n            ylabel (str): The label for the y-axis.\n            constrained_layout (bool): Whether to use constrained layout for the figure.\n            log (bool): Whether to print logs.\n            **kwargs (dict): Additional keyword arguments to be passed to the figure creation.\n\n        Attributes:\n            nrows (int): Number of rows in the subplot grid. Do not change directly!\n            ncols (int): Number of columns in the subplot grid. Do not change directly!\n            log (bool): Whether to print logs.\n            row_names (list of str): Names of the rows.\n            col_names (list of str): Names of the columns.\n            fig (pyplot.figure): The figure.\n            table (numpy.array): The table containing the indices of the fig.axes.\n\n        Methods:\n            expand(nrows, ncols): Increases the number of rows and/or columns in the subplot grid.\n            transpose(): Transpose the subplot grid.\n            ax(x, y): Get a specific axes at the given position (x, y).\n            rownames(positions, names): Set names for some of the rows.\n            colnames(positions, names): Set names for some of the columns.\n            show(): Show the figure.\n            savefig(filename, type='png', path='outputs', makedir=False, log=None, **kwargs): Save the figure.\n            append2pdf(pdf): Save the figure to an existing PDF.\n\n        Example:\n            ```python\n            # Create 2x2 figure and two Axes at [0,:] and [1,0]\n            myfig = subplot(2,2,constrained_layout=True,title='title',xlabel='axis')\n            ax = myfig[0,:]\n            ax.plot([2,1,3])\n            ax = myfig[1,0]\n            ax.plot([2,1,3])\n\n            # myfig.show()\n\n            # Expand figure size to 3x3 (rows=3, cols+=1)\n            myfig.expand(3,myfig.ncols+1)\n\n            # Create a new Axes in the new figure named 'new Axes'\n            ax = myfig[:,2]\n            ax.plot([3,1,2])\n            ax.set_title('new Axes')\n\n            # Print myfig.table in the terminal\n            print(myfig.table)\n            # myfig.show()\n\n            # Modify Axes at [1,0]\n            ax = myfig[1,0]\n            ax.plot([3,3,2])\n            ax.set_title('modified Axes')\n\n            # Create a new Axes ([0,1]) in an alternative mode on top of an existing (our first Axes)\n            ax = myfig.ax(0,1)\n            ax.plot([10,2])\n            ax.set_title('displaced Axes')\n\n            # Create titles for the rows and the columns\n            myfig.rownames([0,1,2],['a','b','new row'])\n            myfig.colnames([0,1,2],['a','b','new column'])\n\n            # Transpose the whole figure (change columns and rows)\n            myfig.transpose()\n\n            # Show our figure\n            myfig.show()\n\n            # Save our figure\n            myfig.savefig('test',type='pdf',path='test')\n            ```\n        \"\"\"\n        if len(argv)==2:\n            nrows=argv[0]\n            ncols=argv[1]\n\n        if fig != None:\n\n            # Set figure\n            self.fig = fig\n\n            # Set nrows, nfigures\n            # nrows, ncols = self.gettable(nrows,ncols)\n\n            # Set GridSpec, nrows, ncols\n            if gs != None:\n                self.gs = gs\n                if not nrows: nrows = gs.nrows\n                if not ncols: ncols = gs.ncols\n            else: self.gs = self.fig.add_gridspec(nrows,ncols)\n\n            # Resize figure\n            self.expand(nrows, ncols)\n\n        else:\n\n            # Set figure\n            self.fig = plt.figure(figsize=[6.4*ncols, 4.8*nrows], constrained_layout=constrained_layout)\n\n            # Set list of geometry\n            self.geometry = []\n            self.table = np.zeros((nrows,ncols),dtype=np.int64)\n\n            # Set GridSpec\n            if gs != None: self.gs = gs\n            elif nrows==0 or ncols==0: pass\n            else: self.gs = self.fig.add_gridspec(nrows,ncols)\n\n        # Update nrows and ncols\n        self.nrows = nrows\n        self.ncols = ncols\n        self.log = log\n\n        # Init objects\n        # self.shareXaxis = self.getitem()\n        self.suptexts = [None,None,None]\n        if title:\n            self.suptexts[0] = title\n        if xlabel:\n            self.suptexts[1] = xlabel\n        if ylabel:\n            self.suptexts[2] = ylabel\n        self.row_names = [None for ___ in range(nrows)]\n        self.col_names = [None for ___ in range(ncols)]\n\n    def __getitem__(self, items) -&gt; Axes:\n\n        # Check if the values are lesser then self.nrows and self.ncols\n        if abs(items[0]) &gt;= self.nrows: raise ValueError(f'The first index ({items[0]}) is not smaller than nrows {self.nrows}')\n        if abs(items[1]) &gt;= self.ncols: raise ValueError(f'The second index ({items[1]}) is not smaller than ncols {self.ncols}')\n\n\n        items = self.getCoordinates(items) # Transform items to concrete bounds\n\n        if items in self.geometry:\n            return self.get_axes(items)\n        elif self.table.__getitem__(items).any() &gt; 0:\n            return self.get_multiple_axeses(items)\n        else:\n            return self.addaxes(items, log=False)  # only log if self.log == True\n\n    def __setitem__(self, items, artist): # could this even work?\n        # ax = self.fig.add_subplot(self.gs.__getitem__(items))\n        # plt.sca(ax)\n        print(artist, type(artist))\n        print(artist.axes, type(artist.axes))\n\n        ax = artist.axes\n\n        self.fig.axes.append(ax)\n        self.geometry.append(items)\n        self.table = self.table_update(self.table, items, len(self.fig.axes))\n        ax.set_subplotspec(self.gs.__getitem__(items))\n\n    def __len__(self):\n        return len(self.geometry)\n\n    def __iter__(self):\n        self.cnt = 0\n        return self\n\n    def __next__(self):\n        if self.cnt &lt; self.__len__():\n            ret =  self.fig.axes[self.cnt]\n            self.cnt += 1\n            return ret\n        else:\n            raise StopIteration\n\n    class area():\n        def __init__(self, items:tuple):\n            \"items: tuple of two slices\"\n            self.x = items[0]\n            self.y = items[1]\n        def __iter__(self):\n            self.cntx = self.x.start\n            self.cnty = self.y.start\n            self.stop = False\n            return self\n        def __next__(self):\n\n            ret = (self.cntx,self.cnty)\n\n            # If out of bounds, stop iteration\n            if self.cnty == self.y.stop: raise StopIteration\n\n            # Next tile\n            self.cntx += 1\n            if self.cntx == self.x.stop:\n                self.cntx = self.x.start\n                self.cnty += 1\n\n            # Raise error if ran out\n            if self.cntx &gt; self.x.stop or self.cnty &gt; self.y.stop: raise ValueError()\n\n            return ret\n\n    class getitem():\n        def __init__(self,func):\n            self.items = None\n            self.func = func\n        def __getitem__(self,items):\n            self.items = items\n            self.func\n\n    def show(self) -&gt; None:\n        self.final()\n        plt.show()\n        self.reverse_final()\n\n    def save(self, filename, type='png', path='outputs', makedir=False, log=None, **kwargs):\n        \"\"\"\n        ### Params:\n        filename: string of the file name\n        type: 'png' or 'pdf'\n        path: path to the file\n        makedir: whether to make directories if not exist\n        log (bool | None): whether to log\n        \"\"\"\n\n        self.final()\n\n        # Path\n        if path[-1] != '/': slash = '/'\n        else: slash = ''\n\n        # Mkdir\n        if makedir:\n            import os\n            os.makedirs(path,exist_ok=True)\n\n        # Create and log pathnfile\n        if '/' in filename: pathnfile = filename\n        else: pathnfile = path+slash+filename\n        if log or (log==None and self.log): print(f'Save {pathnfile}')\n\n        if type == 'png':\n            if not '.' in pathnfile: pathnfile = pathnfile + '.png'\n            plt.savefig(pathnfile, **kwargs)\n\n        if type == 'pdf':\n            if not '.' in pathnfile: pathnfile = pathnfile + '.pdf'\n            from matplotlib.backends.backend_pdf import PdfPages\n            with PdfPages(pathnfile) as pdf: pdf.savefig(bbox_inches='tight')\n\n        self.reverse_final()\n\n    def append2pdf(self,pdf:PdfPages):\n        \"\"\"pdf: the handler of the file\"\"\"\n        self.final()\n        pdf.savefig(bbox_inches='tight')\n        self.reverse_final()\n\n    mathtextdict = {'{':'',\n                    '}':'',\n                    '_': ' '}\n\n    def reverse_final(self):\n        \"\"\"\n        Reverse the changes made by the final() method.\n        \"\"\"\n        # Reset rows\n        if self.ytitles:\n            for cnt, name in enumerate(self.row_names):\n                if name != None and self.table[cnt,0] &gt; 0:\n                    ax = self.fig.axes[self.table[cnt, 0]-1]\n                    ax.set_ylabel(self.ytitles.pop(0))\n\n        # Reset columns\n        if self.xtitles:\n            for cnt, name in enumerate(self.col_names):\n                if name != None and self.table[cnt, 0] &gt; 0:\n                    ax = self.fig.axes[self.table[0, cnt]-1]\n                    ax.set_title(self.xtitles.pop(0))\n\n        # Reset fig attributes\n        self.fig.suptitle('')\n        self.fig.supxlabel('')\n        self.fig.supylabel('')\n\n\n    def final(self):\n\n        self.ytitles = []\n        self.xtitles = []\n\n        # Rows\n        for cnt, name in enumerate(self.row_names):\n            if name != None and self.table[cnt,0] &gt; 0:\n                ax = self.fig.axes[self.table[cnt,0]-1]\n                title = ax.get_ylabel()\n                self.ytitles.append(title)\n                name = str(name)\n                name = ''.join(self.mathtextdict.get(c,c) for c in name) # delete chars, that corrupt the string in mathtex\n                name = ' '.join(rf'$\\bf{{{word}}}$' for word in str(name).split()) # or instead: name = ' '.join(rf'$**{word}**$' for word in str(name).split()) # bold by mathtex\n                ax.set_ylabel(f'{name}\\n{title}')\n\n        # Cols\n        for cnt, name in enumerate(self.col_names):\n            if name != None and self.table[0,cnt] &gt; 0:\n                ax = self.fig.axes[self.table[0,cnt]-1]\n                title = ax.get_title()\n                self.xtitles.append(title)\n                name = str(name)\n                name = ''.join(self.mathtextdict.get(c,c) for c in name) # delete chars, that corrupt the string in mathtex\n                name = ' '.join(rf'$\\bf{{{word}}}$' for word in str(name).split()) # bold by mathtex\n                ax.set_title(f'{name}\\n{title}')\n\n        # Suptexts\n        if self.suptexts[0]:\n            text = self.suptexts[0]\n            text = str(text)\n            text = ''.join(self.mathtextdict.get(c,c) for c in text) # delete chars, that corrupt the string in mathtex\n            text = ' '.join(rf'$\\bf{{{word}}}$' for word in str(text).split()) # bold by mathtex\n            self.fig.suptitle(text,fontsize=self.textsize('suptitle'))\n        if self.suptexts[1]:\n            text = self.suptexts[1]\n            self.fig.supxlabel(text,fontsize=self.textsize('supaxis'))\n        if self.suptexts[2]:\n            text = self.suptexts[2]\n            self.fig.supylabel(text,fontsize=self.textsize('supaxis'))\n\n    def get_axes(self,items) -&gt; Axes:\n        which_axes = self.geometry.index(items)\n        return self.fig.axes[which_axes]\n\n    def get_multiple_axeses(self, items): # check existing axeses in the specified area\n\n        l = []\n\n        for tile in self.area(self.getCoordinates(items)):\n\n            if self.table.item(tile) &gt; 0:\n                l.append(self.fig.axes[int(self.table.item(tile))-1])\n\n        return tuple(l)\n\n    def expand(self, nrows, ncols):\n        \"\"\"\n        https://github.com/matplotlib/matplotlib/issues/7225/\n\n        None, if not to change\n        \"\"\"\n\n        # Initialize\n        if nrows == None: nrows = self.nrows\n        if ncols == None: ncols = self.ncols\n        oldrows, oldcols = self.nrows, self.ncols\n\n        # Conditions:\n        if nrows &lt;= self.nrows and ncols &lt;= self.ncols: return\n        if nrows &lt; self.nrows or ncols &lt; self.ncols: raise ValueError('nrows and ncols must be equal or greater than original nrows and ncols')\n\n        # Prepare gridspec &amp; table &amp; fig to the Axis rearrange\n        gs = gridspec.GridSpec(nrows, ncols)\n        if self.nrows==0 or self.ncols==0: # init if it was 0 so far\n            # gs = self.fig.add_gridspec(nrows,ncols)\n            gs.tight_layout(self.fig)\n        table = np.zeros((nrows, ncols),dtype=np.int64)\n        self.fig.set_size_inches(6.4*ncols, 4.8*nrows)\n        # self.fig.set_figheight(self.nrows*4.8)\n        # self.fig.set_figwidth(self.ncols*6.4)\n\n        # Axes rearrange\n        for i, ax, geo in zip(range(len(self.geometry)), self.fig.axes, self.geometry):\n            pos = gs[geo].get_position(self.fig) # Bbox\n            ax.set_position(pos)\n            ax.set_subplotspec(gs[geo])\n            table = self.table_update(table, geo, i+1)\n\n        # Constrained layout\n        if plt.rcParams['figure.constrained_layout.use'] == False:\n            plt.rcParams['figure.constrained_layout.use'] = True\n\n        self.nrows, self.ncols = nrows, ncols\n        self.gs = gs\n        self.table = table\n        self.row_names = [___ for ___ in self.row_names] + [None for ___ in range(nrows - oldrows)]\n        self.col_names = [___ for ___ in self.col_names] + [None for ___ in range(ncols - oldcols)]\n\n    def table_update(self, table, items, nth_axes, log=True):\n        \"\"\"add items tuple to the table. warn if already occupied\"\"\"\n\n        # Iterate through gridspec tiles of the designated area\n        for tile in self.area(self.getCoordinates(items)):\n\n            # Check if occupied\n            if table.__getitem__(tile) == 1 and (log or self.log):\n                warnings.warn(f'{items} is already in use') # occupied\n\n            # Set table element to number of axes\n            table.__setitem__(tile,int(nth_axes)) # self.table[tile[0],tile[1]] = 1\n\n        return table\n\n    def addaxes(self, items, log=True, **kwargs):\n\n        self.geometry.append(self.getCoordinates(items))\n\n        self.table = self.table_update(self.table, items, len(self.fig.axes)+1, log=log) # len(axes)+1 bcs u have not added yet the axes to the fig\n\n        return self.fig.add_subplot(self.gs.__getitem__(items), **kwargs)\n\n    def transformSizeToFitSubplots(self, sl, max:int): # Transorm slice to fit the subplots by nrwos and ncols\n        if type(sl) == int: sl = slice(sl,sl+1)\n\n        if type(sl) == slice:\n\n            start, stop = sl.start, sl.stop\n\n            if stop == None: stop = max\n            if start == None: start = 0\n\n            if stop &lt; 0:\n                stop += max\n            if start &lt; 0:\n                start += max\n                if stop &lt; start: stop += max # is sl.stop == sl.start+1\n\n            # Check\n            if stop &gt; max: raise ValueError(f'stop {stop} is greater than max {max}')\n\n            return slice(start,stop)\n\n        else: raise TypeError('I can only concretize only on int or slice')\n\n    def getCoordinates(self, items:tuple) -&gt; tuple:\n\n        x = self.transformSizeToFitSubplots(items[0],self.nrows)\n        y = self.transformSizeToFitSubplots(items[1],self.ncols)\n\n        return (x,y)\n\n    # def toConcreteSlices(self, items) -&gt; tuple:\n    #     l=[]\n    #     for i, max in zip(items, [self.nrows, self.ncols]):\n    #         if type(i) == int: ret = self.concretize(slice(i,i+1), max)\n    #         elif type(i) == slice: ret = self.concretize(i, max)\n    #         else: raise TypeError('elements of \"items\" tuple must be int or slice')\n    #         l.append(ret)\n    #     return tuple(l)\n\n    class shareXax():\n        def __init__(self):\n            # super().__init__()\n            pass\n        def __getitem__(super, items):\n            super.cols = items\n\n    def shareXaxisold(self, *axeses): # join\n        \"\"\"\n        ### Example\n        myfunc.shareXaxis(myfunc[:,:])\n        \"\"\"\n\n        # left = []\n        # right = []\n\n        # for axes in axeses:\n        #     if type(axes) == Axes:\n        #         left.append(axes.get_xlim())\n        #     elif type(axes) == tuple or type(axes) == list:\n        #         left.append(self.gs.__getitem__(axes).get_position(self.fig))\n        #     else: raise TypeError('there must be Axes, tuple or lsit in joinXaxis')\n\n        # Determine values\n        # maximum = max(right)\n        # minimum = min(left)\n\n        # print(axeses)\n        # print(axeses[0])\n        if(type(axeses) == tuple) and (len(axeses) == 1): axeses = axeses[0]\n        axeses[0].get_shared_x_axes()#.join(axeses)\n\n    def transpose(self):\n\n        # Initialize\n        nrows = self.ncols # change\n        ncols = self.nrows # change\n\n        # Prepare gridspec &amp; table &amp; fig to the Axis rearrange\n        gs = gridspec.GridSpec(nrows, ncols)\n        if self.nrows==0 or self.ncols==0: # init if it was 0 so far\n            gs.tight_layout(self.fig)\n        table = np.zeros((nrows, ncols),dtype=np.int64)\n        self.fig.set_size_inches(6.4*ncols, 4.8*nrows)\n\n        # Axes rearrange\n        for i, ax, geo in zip(range(len(self.geometry)), self.fig.axes, self.geometry):\n            pos = gs[(geo[1],geo[0])].get_position(self.fig) # Bbox # change\n            ax.set_position(pos)\n            ax.set_subplotspec(gs[(geo[1],geo[0])]) # change\n            table = self.table_update(table, (geo[1],geo[0]), i+1) # change\n\n        self.nrows, self.ncols = nrows, ncols\n        self.gs = gs\n        self.table = table\n        self.row_names, self.col_names = [___ for ___ in self.col_names], [___ for ___ in self.row_names] # change\n\n    def ax(self,row,column,log=True,**kwargs) -&gt; Axes:\n        \"\"\"\n        ### Parameters\n        log (bool): whether to print warning message, if another axes overlaps the location\n        \"\"\"\n\n        items = (row,column)\n\n        items = self.getCoordinates(items) # Transform items to concrete bounds\n\n        if items in self.geometry:\n            return self.get_axes(items)\n        else:\n            return self.addaxes(items, log=log, **kwargs)\n\n    def rownames(self, positions:list, names:list):\n\n        # Check\n        if type(positions) != list and type(positions) != range: positions = [positions]\n        if type(names) != list: names = [names]\n        if len(positions) != len(names): raise ValueError('length of position and length of names must be the same')\n\n        for position, name in zip(positions, names):\n            if name != None:\n                # ax = self.__getitem__((position,0))\n                self.row_names[position] = name\n\n    def colnames(self, positions:list, names:list):\n\n        # Check\n        if type(positions) != list and type(positions) != range: positions = [positions]\n        if type(names) != list: names = [names]\n        if len(positions) != len(names): raise ValueError('length of position and length of names must be the same')\n\n        for position, name in zip(positions, names):\n            if name != None:\n                # ax = self.__getitem__((0,position))\n                self.col_names[position] = name\n\n    def textsize(self, type):\n        d = {'suptitle':2,\n             'supaxis':1,\n             'title':1,\n             'axis':.5\n             }\n        return d[type] * sqrt(self.nrows**2 + self.ncols**2) * 3\n\n    class colorbar():\n        def __init__(self):\n            pass\n        def __getitem__(self, items):\n            area = self.area(items)\n            return self.fig.colorbar(self.fig.axes[0].collections[0], ax=items, orientation='vertical', label=None)\n\n    def add_colorbar(self, imshow: Union[AxesImage, ContourSet], orientation: str = 'vertical', label: str = None):\n        \"\"\"\n        Add colorbar axes to the side of the figure or below the figure.\n\n        Parameters:\n            imshow (AxesImage or ContourSet): The image or contour set to which the colorbar corresponds.\n            orientation (str): The orientation and position of the colorbar. Can be 'vertical' or 'horizontal'. Default is 'vertical'.\n            label (str): The label of the colorbar. Default is None.\n        \"\"\"\n\n        # Initialize subplot. It must be AxesImage or ContourSet.\n        if (type(imshow) is not AxesImage) and \\\n            (type(imshow) is not ContourSet):\n            raise ValueError('imshow must be AxesImage or ContourSet')\n\n        # Add colorbar axes\n        # if orientation == 'vertical':\n        #     self.expand(self.nrows, self.ncols+1)\n        #     location = (slice(None),-1)\n        #     print('location', location)\n        #     print('nrows, ncols', self.nrows, self.ncols)\n        #     location = self.getCoordinates(location) # Transform items to concrete bounds\n        # elif orientation == 'horizontal':\n        #     self.expand(self.nrows+1, self.ncols)\n        #     location = (-1,slice(None))\n        #     location = self.getCoordinates(location)\n\n        if orientation == 'vertical':\n            # Get the horizontal position of the colorbar\n            col = self.ncols\n            h_pos = 0.9 + (col)*0.01\n            dx = 0.95 - h_pos\n            # Add colorbar axes\n            cbar_ax = self.fig.add_axes([h_pos, 0.15, dx, 0.7])\n            # Add colorbar\n            cbar = self.fig.colorbar(imshow, cax=cbar_ax, orientation=orientation, label=label)\n\n        elif orientation == 'horizontal':\n            raise NotImplementedError('Horizontal colorbar is not implemented yet, bcs of the xlabel would overlap with the colorbar.')\n            # Get the vertical position of the colorbar\n            row = self.nrows\n            dy = row/10\n            # Add colorbar axes\n            cbar_ax = self.fig.add_axes([0.15, 0.05, 0.7, dy])\n            # Add colorbar\n            cbar = self.fig.colorbar(imshow, cax=cbar_ax, orientation=orientation, label=label)\n\n        return cbar\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.area","title":"<code>area</code>","text":"Source code in <code>utils/megaplot.py</code> <pre><code>class area():\n    def __init__(self, items:tuple):\n        \"items: tuple of two slices\"\n        self.x = items[0]\n        self.y = items[1]\n    def __iter__(self):\n        self.cntx = self.x.start\n        self.cnty = self.y.start\n        self.stop = False\n        return self\n    def __next__(self):\n\n        ret = (self.cntx,self.cnty)\n\n        # If out of bounds, stop iteration\n        if self.cnty == self.y.stop: raise StopIteration\n\n        # Next tile\n        self.cntx += 1\n        if self.cntx == self.x.stop:\n            self.cntx = self.x.start\n            self.cnty += 1\n\n        # Raise error if ran out\n        if self.cntx &gt; self.x.stop or self.cnty &gt; self.y.stop: raise ValueError()\n\n        return ret\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.area.__init__","title":"<code>__init__(items)</code>","text":"<p>items: tuple of two slices</p> Source code in <code>utils/megaplot.py</code> <pre><code>def __init__(self, items:tuple):\n    \"items: tuple of two slices\"\n    self.x = items[0]\n    self.y = items[1]\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.__init__","title":"<code>__init__(*argv, fig=None, gs=None, nrows=None, ncols=None, title=None, xlabel=None, ylabel=None, constrained_layout=False, log=True, **kwargs)</code>","text":"<p>A class used to make matplotlib subplot and its changes more convenient.</p> <p>Parameters:</p> Name Type Description Default <code>*argv</code> <code>tuple</code> <p>Tuple of (nrows, ncols) if provided as separate arguments.</p> <code>()</code> <code>fig</code> <code>figure</code> <p>The figure to use. If not provided, a new figure will be created.</p> <code>None</code> <code>gs</code> <code>GridSpec</code> <p>The GridSpec to use. If not provided, a new GridSpec will be created.</p> <code>None</code> <code>nrows</code> <code>int</code> <p>Number of rows in the subplot grid. If not provided, it will be determined based on the fig or gs.</p> <code>None</code> <code>ncols</code> <code>int</code> <p>Number of columns in the subplot grid. If not provided, it will be determined based on the fig or gs.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the figure.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>The label for the x-axis.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>The label for the y-axis.</p> <code>None</code> <code>constrained_layout</code> <code>bool</code> <p>Whether to use constrained layout for the figure.</p> <code>False</code> <code>log</code> <code>bool</code> <p>Whether to print logs.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the figure creation.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>nrows</code> <code>int</code> <p>Number of rows in the subplot grid. Do not change directly!</p> <code>ncols</code> <code>int</code> <p>Number of columns in the subplot grid. Do not change directly!</p> <code>log</code> <code>bool</code> <p>Whether to print logs.</p> <code>row_names</code> <code>list of str</code> <p>Names of the rows.</p> <code>col_names</code> <code>list of str</code> <p>Names of the columns.</p> <code>fig</code> <code>figure</code> <p>The figure.</p> <code>table</code> <code>array</code> <p>The table containing the indices of the fig.axes.</p> <p>Functions:</p> Name Description <code>expand</code> <p>Increases the number of rows and/or columns in the subplot grid.</p> <code>transpose</code> <p>Transpose the subplot grid.</p> <code>ax</code> <p>Get a specific axes at the given position (x, y).</p> <code>rownames</code> <p>Set names for some of the rows.</p> <code>colnames</code> <p>Set names for some of the columns.</p> <code>show</code> <p>Show the figure.</p> <code>savefig</code> <p>Save the figure.</p> <code>append2pdf</code> <p>Save the figure to an existing PDF.</p> Example <pre><code># Create 2x2 figure and two Axes at [0,:] and [1,0]\nmyfig = subplot(2,2,constrained_layout=True,title='title',xlabel='axis')\nax = myfig[0,:]\nax.plot([2,1,3])\nax = myfig[1,0]\nax.plot([2,1,3])\n\n# myfig.show()\n\n# Expand figure size to 3x3 (rows=3, cols+=1)\nmyfig.expand(3,myfig.ncols+1)\n\n# Create a new Axes in the new figure named 'new Axes'\nax = myfig[:,2]\nax.plot([3,1,2])\nax.set_title('new Axes')\n\n# Print myfig.table in the terminal\nprint(myfig.table)\n# myfig.show()\n\n# Modify Axes at [1,0]\nax = myfig[1,0]\nax.plot([3,3,2])\nax.set_title('modified Axes')\n\n# Create a new Axes ([0,1]) in an alternative mode on top of an existing (our first Axes)\nax = myfig.ax(0,1)\nax.plot([10,2])\nax.set_title('displaced Axes')\n\n# Create titles for the rows and the columns\nmyfig.rownames([0,1,2],['a','b','new row'])\nmyfig.colnames([0,1,2],['a','b','new column'])\n\n# Transpose the whole figure (change columns and rows)\nmyfig.transpose()\n\n# Show our figure\nmyfig.show()\n\n# Save our figure\nmyfig.savefig('test',type='pdf',path='test')\n</code></pre> Source code in <code>utils/megaplot.py</code> <pre><code>def __init__(self, *argv, fig=None, gs=None, nrows=None, ncols=None, title=None, xlabel=None, ylabel=None, constrained_layout=False, log=True, **kwargs):\n    \"\"\"\n    A class used to make matplotlib subplot and its changes more convenient.\n\n    Args:\n        *argv (tuple): Tuple of (nrows, ncols) if provided as separate arguments.\n        fig (pyplot.figure): The figure to use. If not provided, a new figure will be created.\n        gs (GridSpec): The GridSpec to use. If not provided, a new GridSpec will be created.\n        nrows (int): Number of rows in the subplot grid. If not provided, it will be determined based on the fig or gs.\n        ncols (int): Number of columns in the subplot grid. If not provided, it will be determined based on the fig or gs.\n        title (str): The title of the figure.\n        xlabel (str): The label for the x-axis.\n        ylabel (str): The label for the y-axis.\n        constrained_layout (bool): Whether to use constrained layout for the figure.\n        log (bool): Whether to print logs.\n        **kwargs (dict): Additional keyword arguments to be passed to the figure creation.\n\n    Attributes:\n        nrows (int): Number of rows in the subplot grid. Do not change directly!\n        ncols (int): Number of columns in the subplot grid. Do not change directly!\n        log (bool): Whether to print logs.\n        row_names (list of str): Names of the rows.\n        col_names (list of str): Names of the columns.\n        fig (pyplot.figure): The figure.\n        table (numpy.array): The table containing the indices of the fig.axes.\n\n    Methods:\n        expand(nrows, ncols): Increases the number of rows and/or columns in the subplot grid.\n        transpose(): Transpose the subplot grid.\n        ax(x, y): Get a specific axes at the given position (x, y).\n        rownames(positions, names): Set names for some of the rows.\n        colnames(positions, names): Set names for some of the columns.\n        show(): Show the figure.\n        savefig(filename, type='png', path='outputs', makedir=False, log=None, **kwargs): Save the figure.\n        append2pdf(pdf): Save the figure to an existing PDF.\n\n    Example:\n        ```python\n        # Create 2x2 figure and two Axes at [0,:] and [1,0]\n        myfig = subplot(2,2,constrained_layout=True,title='title',xlabel='axis')\n        ax = myfig[0,:]\n        ax.plot([2,1,3])\n        ax = myfig[1,0]\n        ax.plot([2,1,3])\n\n        # myfig.show()\n\n        # Expand figure size to 3x3 (rows=3, cols+=1)\n        myfig.expand(3,myfig.ncols+1)\n\n        # Create a new Axes in the new figure named 'new Axes'\n        ax = myfig[:,2]\n        ax.plot([3,1,2])\n        ax.set_title('new Axes')\n\n        # Print myfig.table in the terminal\n        print(myfig.table)\n        # myfig.show()\n\n        # Modify Axes at [1,0]\n        ax = myfig[1,0]\n        ax.plot([3,3,2])\n        ax.set_title('modified Axes')\n\n        # Create a new Axes ([0,1]) in an alternative mode on top of an existing (our first Axes)\n        ax = myfig.ax(0,1)\n        ax.plot([10,2])\n        ax.set_title('displaced Axes')\n\n        # Create titles for the rows and the columns\n        myfig.rownames([0,1,2],['a','b','new row'])\n        myfig.colnames([0,1,2],['a','b','new column'])\n\n        # Transpose the whole figure (change columns and rows)\n        myfig.transpose()\n\n        # Show our figure\n        myfig.show()\n\n        # Save our figure\n        myfig.savefig('test',type='pdf',path='test')\n        ```\n    \"\"\"\n    if len(argv)==2:\n        nrows=argv[0]\n        ncols=argv[1]\n\n    if fig != None:\n\n        # Set figure\n        self.fig = fig\n\n        # Set nrows, nfigures\n        # nrows, ncols = self.gettable(nrows,ncols)\n\n        # Set GridSpec, nrows, ncols\n        if gs != None:\n            self.gs = gs\n            if not nrows: nrows = gs.nrows\n            if not ncols: ncols = gs.ncols\n        else: self.gs = self.fig.add_gridspec(nrows,ncols)\n\n        # Resize figure\n        self.expand(nrows, ncols)\n\n    else:\n\n        # Set figure\n        self.fig = plt.figure(figsize=[6.4*ncols, 4.8*nrows], constrained_layout=constrained_layout)\n\n        # Set list of geometry\n        self.geometry = []\n        self.table = np.zeros((nrows,ncols),dtype=np.int64)\n\n        # Set GridSpec\n        if gs != None: self.gs = gs\n        elif nrows==0 or ncols==0: pass\n        else: self.gs = self.fig.add_gridspec(nrows,ncols)\n\n    # Update nrows and ncols\n    self.nrows = nrows\n    self.ncols = ncols\n    self.log = log\n\n    # Init objects\n    # self.shareXaxis = self.getitem()\n    self.suptexts = [None,None,None]\n    if title:\n        self.suptexts[0] = title\n    if xlabel:\n        self.suptexts[1] = xlabel\n    if ylabel:\n        self.suptexts[2] = ylabel\n    self.row_names = [None for ___ in range(nrows)]\n    self.col_names = [None for ___ in range(ncols)]\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.add_colorbar","title":"<code>add_colorbar(imshow, orientation='vertical', label=None)</code>","text":"<p>Add colorbar axes to the side of the figure or below the figure.</p> <p>Parameters:</p> Name Type Description Default <code>imshow</code> <code>AxesImage or ContourSet</code> <p>The image or contour set to which the colorbar corresponds.</p> required <code>orientation</code> <code>str</code> <p>The orientation and position of the colorbar. Can be 'vertical' or 'horizontal'. Default is 'vertical'.</p> <code>'vertical'</code> <code>label</code> <code>str</code> <p>The label of the colorbar. Default is None.</p> <code>None</code> Source code in <code>utils/megaplot.py</code> <pre><code>def add_colorbar(self, imshow: Union[AxesImage, ContourSet], orientation: str = 'vertical', label: str = None):\n    \"\"\"\n    Add colorbar axes to the side of the figure or below the figure.\n\n    Parameters:\n        imshow (AxesImage or ContourSet): The image or contour set to which the colorbar corresponds.\n        orientation (str): The orientation and position of the colorbar. Can be 'vertical' or 'horizontal'. Default is 'vertical'.\n        label (str): The label of the colorbar. Default is None.\n    \"\"\"\n\n    # Initialize subplot. It must be AxesImage or ContourSet.\n    if (type(imshow) is not AxesImage) and \\\n        (type(imshow) is not ContourSet):\n        raise ValueError('imshow must be AxesImage or ContourSet')\n\n    # Add colorbar axes\n    # if orientation == 'vertical':\n    #     self.expand(self.nrows, self.ncols+1)\n    #     location = (slice(None),-1)\n    #     print('location', location)\n    #     print('nrows, ncols', self.nrows, self.ncols)\n    #     location = self.getCoordinates(location) # Transform items to concrete bounds\n    # elif orientation == 'horizontal':\n    #     self.expand(self.nrows+1, self.ncols)\n    #     location = (-1,slice(None))\n    #     location = self.getCoordinates(location)\n\n    if orientation == 'vertical':\n        # Get the horizontal position of the colorbar\n        col = self.ncols\n        h_pos = 0.9 + (col)*0.01\n        dx = 0.95 - h_pos\n        # Add colorbar axes\n        cbar_ax = self.fig.add_axes([h_pos, 0.15, dx, 0.7])\n        # Add colorbar\n        cbar = self.fig.colorbar(imshow, cax=cbar_ax, orientation=orientation, label=label)\n\n    elif orientation == 'horizontal':\n        raise NotImplementedError('Horizontal colorbar is not implemented yet, bcs of the xlabel would overlap with the colorbar.')\n        # Get the vertical position of the colorbar\n        row = self.nrows\n        dy = row/10\n        # Add colorbar axes\n        cbar_ax = self.fig.add_axes([0.15, 0.05, 0.7, dy])\n        # Add colorbar\n        cbar = self.fig.colorbar(imshow, cax=cbar_ax, orientation=orientation, label=label)\n\n    return cbar\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.append2pdf","title":"<code>append2pdf(pdf)</code>","text":"<p>pdf: the handler of the file</p> Source code in <code>utils/megaplot.py</code> <pre><code>def append2pdf(self,pdf:PdfPages):\n    \"\"\"pdf: the handler of the file\"\"\"\n    self.final()\n    pdf.savefig(bbox_inches='tight')\n    self.reverse_final()\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.ax","title":"<code>ax(row, column, log=True, **kwargs)</code>","text":""},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.ax--parameters","title":"Parameters","text":"<p>log (bool): whether to print warning message, if another axes overlaps the location</p> Source code in <code>utils/megaplot.py</code> <pre><code>def ax(self,row,column,log=True,**kwargs) -&gt; Axes:\n    \"\"\"\n    ### Parameters\n    log (bool): whether to print warning message, if another axes overlaps the location\n    \"\"\"\n\n    items = (row,column)\n\n    items = self.getCoordinates(items) # Transform items to concrete bounds\n\n    if items in self.geometry:\n        return self.get_axes(items)\n    else:\n        return self.addaxes(items, log=log, **kwargs)\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.expand","title":"<code>expand(nrows, ncols)</code>","text":"<p>https://github.com/matplotlib/matplotlib/issues/7225/</p> <p>None, if not to change</p> Source code in <code>utils/megaplot.py</code> <pre><code>def expand(self, nrows, ncols):\n    \"\"\"\n    https://github.com/matplotlib/matplotlib/issues/7225/\n\n    None, if not to change\n    \"\"\"\n\n    # Initialize\n    if nrows == None: nrows = self.nrows\n    if ncols == None: ncols = self.ncols\n    oldrows, oldcols = self.nrows, self.ncols\n\n    # Conditions:\n    if nrows &lt;= self.nrows and ncols &lt;= self.ncols: return\n    if nrows &lt; self.nrows or ncols &lt; self.ncols: raise ValueError('nrows and ncols must be equal or greater than original nrows and ncols')\n\n    # Prepare gridspec &amp; table &amp; fig to the Axis rearrange\n    gs = gridspec.GridSpec(nrows, ncols)\n    if self.nrows==0 or self.ncols==0: # init if it was 0 so far\n        # gs = self.fig.add_gridspec(nrows,ncols)\n        gs.tight_layout(self.fig)\n    table = np.zeros((nrows, ncols),dtype=np.int64)\n    self.fig.set_size_inches(6.4*ncols, 4.8*nrows)\n    # self.fig.set_figheight(self.nrows*4.8)\n    # self.fig.set_figwidth(self.ncols*6.4)\n\n    # Axes rearrange\n    for i, ax, geo in zip(range(len(self.geometry)), self.fig.axes, self.geometry):\n        pos = gs[geo].get_position(self.fig) # Bbox\n        ax.set_position(pos)\n        ax.set_subplotspec(gs[geo])\n        table = self.table_update(table, geo, i+1)\n\n    # Constrained layout\n    if plt.rcParams['figure.constrained_layout.use'] == False:\n        plt.rcParams['figure.constrained_layout.use'] = True\n\n    self.nrows, self.ncols = nrows, ncols\n    self.gs = gs\n    self.table = table\n    self.row_names = [___ for ___ in self.row_names] + [None for ___ in range(nrows - oldrows)]\n    self.col_names = [___ for ___ in self.col_names] + [None for ___ in range(ncols - oldcols)]\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.reverse_final","title":"<code>reverse_final()</code>","text":"<p>Reverse the changes made by the final() method.</p> Source code in <code>utils/megaplot.py</code> <pre><code>def reverse_final(self):\n    \"\"\"\n    Reverse the changes made by the final() method.\n    \"\"\"\n    # Reset rows\n    if self.ytitles:\n        for cnt, name in enumerate(self.row_names):\n            if name != None and self.table[cnt,0] &gt; 0:\n                ax = self.fig.axes[self.table[cnt, 0]-1]\n                ax.set_ylabel(self.ytitles.pop(0))\n\n    # Reset columns\n    if self.xtitles:\n        for cnt, name in enumerate(self.col_names):\n            if name != None and self.table[cnt, 0] &gt; 0:\n                ax = self.fig.axes[self.table[0, cnt]-1]\n                ax.set_title(self.xtitles.pop(0))\n\n    # Reset fig attributes\n    self.fig.suptitle('')\n    self.fig.supxlabel('')\n    self.fig.supylabel('')\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.save","title":"<code>save(filename, type='png', path='outputs', makedir=False, log=None, **kwargs)</code>","text":""},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.save--params","title":"Params:","text":"<p>filename: string of the file name type: 'png' or 'pdf' path: path to the file makedir: whether to make directories if not exist log (bool | None): whether to log</p> Source code in <code>utils/megaplot.py</code> <pre><code>def save(self, filename, type='png', path='outputs', makedir=False, log=None, **kwargs):\n    \"\"\"\n    ### Params:\n    filename: string of the file name\n    type: 'png' or 'pdf'\n    path: path to the file\n    makedir: whether to make directories if not exist\n    log (bool | None): whether to log\n    \"\"\"\n\n    self.final()\n\n    # Path\n    if path[-1] != '/': slash = '/'\n    else: slash = ''\n\n    # Mkdir\n    if makedir:\n        import os\n        os.makedirs(path,exist_ok=True)\n\n    # Create and log pathnfile\n    if '/' in filename: pathnfile = filename\n    else: pathnfile = path+slash+filename\n    if log or (log==None and self.log): print(f'Save {pathnfile}')\n\n    if type == 'png':\n        if not '.' in pathnfile: pathnfile = pathnfile + '.png'\n        plt.savefig(pathnfile, **kwargs)\n\n    if type == 'pdf':\n        if not '.' in pathnfile: pathnfile = pathnfile + '.pdf'\n        from matplotlib.backends.backend_pdf import PdfPages\n        with PdfPages(pathnfile) as pdf: pdf.savefig(bbox_inches='tight')\n\n    self.reverse_final()\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.shareXaxisold","title":"<code>shareXaxisold(*axeses)</code>","text":""},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.shareXaxisold--example","title":"Example","text":"<p>myfunc.shareXaxis(myfunc[:,:])</p> Source code in <code>utils/megaplot.py</code> <pre><code>def shareXaxisold(self, *axeses): # join\n    \"\"\"\n    ### Example\n    myfunc.shareXaxis(myfunc[:,:])\n    \"\"\"\n\n    # left = []\n    # right = []\n\n    # for axes in axeses:\n    #     if type(axes) == Axes:\n    #         left.append(axes.get_xlim())\n    #     elif type(axes) == tuple or type(axes) == list:\n    #         left.append(self.gs.__getitem__(axes).get_position(self.fig))\n    #     else: raise TypeError('there must be Axes, tuple or lsit in joinXaxis')\n\n    # Determine values\n    # maximum = max(right)\n    # minimum = min(left)\n\n    # print(axeses)\n    # print(axeses[0])\n    if(type(axeses) == tuple) and (len(axeses) == 1): axeses = axeses[0]\n    axeses[0].get_shared_x_axes()#.join(axeses)\n</code></pre>"},{"location":"references/utilities/megaplot/#utils.megaplot.megaplot.table_update","title":"<code>table_update(table, items, nth_axes, log=True)</code>","text":"<p>add items tuple to the table. warn if already occupied</p> Source code in <code>utils/megaplot.py</code> <pre><code>def table_update(self, table, items, nth_axes, log=True):\n    \"\"\"add items tuple to the table. warn if already occupied\"\"\"\n\n    # Iterate through gridspec tiles of the designated area\n    for tile in self.area(self.getCoordinates(items)):\n\n        # Check if occupied\n        if table.__getitem__(tile) == 1 and (log or self.log):\n            warnings.warn(f'{items} is already in use') # occupied\n\n        # Set table element to number of axes\n        table.__setitem__(tile,int(nth_axes)) # self.table[tile[0],tile[1]] = 1\n\n    return table\n</code></pre>"},{"location":"references/utilities/neuropixel/","title":"Preprocessing Tools","text":"<p>This module contains functions for downloading and initializing data from the Allen Brain Observatory.</p> <p>Functions: - cacheData(location=\"\") -&gt; EcephysProjectCache: Caches data from the Allen Brain Observatory. - completeDownloadData(sessions, cache, output_dir, download_lfp=False) -&gt; None: Downloads the complete dataset for analysis.</p> <p>This submodule contains tools for working with Neuropixel data from the Allen Institute.</p> <p>Functions:</p> <ul> <li>get_table(cache, session_id, table_name) -&gt; DataFrame: Get a table from the cache.</li> <li>dict_from_dataframe(df, name) -&gt; dict: Create a dictionary from a DataFrame.</li> <li>AllenTables(cache, session_id, layer_assignment=False) -&gt; AllenTables: Create an AllenTables object.</li> <li>get_unit_channels(session, log_all_areas=False) -&gt; DataFrame: Get the unit channels for the given session.</li> <li>makePSTH(spikes, startTimes, windowDur, binSize=0.001) -&gt; tuple: Compute the Peri-Stimulus Time Histogram (PSTH).</li> <li>get_stimulus_presentations(session) -&gt; DataFrame: Get the stimulus presentations for the given session.</li> <li>get_area_units(units: pd.DataFrame, area_of_interest) -&gt; DataFrame: Get the units in a specific area of interest.</li> <li>get_area_change_responses(session, area_of_interest) -&gt; np.ndarray: Get the change responses for the units in the area of interest.</li> <li>get_area_receptive_fields(spike_times, stimulus_presentations, area_of_interest) -&gt; list: Get the receptive fields for the units in the area of interest.</li> <li>optotagging(opto_table, spike_times, area_of_interest) -&gt; np.ndarray: Perform optotagging analysis on the units in the area of interest.</li> <li>get_response_magnitudes(opto_response) -&gt; np.ndarray: Calculate the response magnitudes of optogenetic stimulation.</li> <li>get_average_unit_responses(units, spike_times, trial_start, duration=0.03, binSize=0.001) -&gt; np.ndarray: Calculate the unit responses for each unit in the given units DataFrame.</li> <li>get_unit_responses(units, spike_times, trial_start, trial_end, stepSize=0.010, binSize=0.050, progressbar=True) -&gt; np.ndarray: Calculate the unit responses for each trial and time bin.</li> </ul> <p>Classes:</p> <ul> <li>AllenTables: A class that represents tables related to Allen Institute's Neuropixel data.</li> </ul> <p>Info:</p> <p>This module provides functions and a class for working with Neuropixel data from the Allen Institute. It includes functions for retrieving specific tables from the cache, creating dictionaries from DataFrames, and performing various analyses on the data. The AllenTables class represents tables related to Neuropixel data and provides methods for creating tables and columns, assigning cortical layer information to units, and retrieving dataframes based on keys. The module also includes functions for computing the Peri-Stimulus Time Histogram (PSTH), getting unit channels, stimulus presentations, units in a specific area of interest, change responses for units in an area of interest, receptive fields for units in an area of interest, performing optotagging analysis, calculating response magnitudes of optogenetic stimulation, and calculating unit responses for each unit in a given DataFrame.</p> <p>Acronyms:</p> <ul> <li>VISp: Primary Visual Area (V1)</li> <li>VISpl: Posterolateral visual area</li> <li>VISli: Laterointermediate area (visually guided behav)</li> <li>VISl: Lateral visual area (V2, LM)</li> <li>VISal: Anteromedial visual area</li> <li>VISlla: Laterolateral anterior visual area</li> <li>VISrl: Rostrolateral visual area (visually guided behav)</li> <li>VISam: Anteromedial visual area</li> <li>VISpm: Posteromedial visual area (V4-V5, MT; PMC: associative area)</li> <li>VISm: Medial visual area (V6, \"medial motion area\", see: https://pages.ucsd.edu/~msereno/papers/V6Motion09.pdf)</li> <li>VISmma: Mediomedial anterior visual area</li> <li>VISmmp: Mediomedial posterior visual area</li> </ul> <p>For more information, refer to the AllenSDK documentation: https://allensdk.readthedocs.io/en/latest/_static/examples/nb/ecephys_quickstart.html</p>"},{"location":"references/utilities/neuropixel/#utils.download_allen.cacheData","title":"<code>cacheData(location='')</code>","text":"<p>Caches data from the Allen Brain Observatory.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>The location where the data will be cached. If not provided, the default location will be used.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>cache</code> <code>EcephysProjectCache</code> <p>The cache object containing the cached data.</p> Source code in <code>utils/download_allen.py</code> <pre><code>def cacheData(location=\"\"):\n    \"\"\"\n    Caches data from the Allen Brain Observatory.\n\n    Args:\n        location (str): The location where the data will be cached. If not provided, the default location will be used.\n\n    Returns:\n        cache (EcephysProjectCache): The cache object containing the cached data.\n    \"\"\"\n\n    # Set the cache location\n    if location == \"\":\n        location = params['location']\n    print(\"Cache location:\", params['location'])\n    output_dir = Path(location)\n\n    # Confirm the allensdk version\n    print(f\"Your allensdk version is: {allensdk.__version__}\")\n\n    # Create the cache object\n    cache = EcephysProjectCache(manifest=os.path.join(output_dir, \"manifest.json\"))\n\n    return cache\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.download_allen.completeDownloadData","title":"<code>completeDownloadData(sessions, cache, output_dir, download_lfp=False)</code>","text":"<p>Downloads the complete dataset for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>sessions</code> <code>DataFrame</code> <p>A DataFrame containing session information.</p> required <code>cache</code> <code>Cache</code> <p>An object representing the cache.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the downloaded data will be stored.</p> required <code>download_lfp</code> <code>bool</code> <p>Whether to download LFP data files. Defaults to False.</p> <code>False</code> Notes <ul> <li>This function downloads the complete dataset by iterating over each session in the provided DataFrame.</li> <li>It checks for the presence of the complete file to ensure that the download is not interrupted due to an unreliable connection.</li> <li>Make sure that you have enough space available in your cache directory before running this code.   You'll need around 855 GB for the whole dataset, and 147 GB if you're not downloading the LFP data files.</li> </ul> Source code in <code>utils/download_allen.py</code> <pre><code>def completeDownloadData(sessions, cache, output_dir, download_lfp=False):\n    '''\n    Downloads the complete dataset for analysis.\n\n    Parameters:\n        sessions (DataFrame): A DataFrame containing session information.\n        cache (Cache): An object representing the cache.\n        output_dir (str): The directory where the downloaded data will be stored.\n        download_lfp (bool, optional): Whether to download LFP data files. Defaults to False.\n\n    Notes:\n        - This function downloads the complete dataset by iterating over each session in the provided DataFrame.\n        - It checks for the presence of the complete file to ensure that the download is not interrupted due to an unreliable connection.\n        - Make sure that you have enough space available in your cache directory before running this code.\n          You'll need around 855 GB for the whole dataset, and 147 GB if you're not downloading the LFP data files.\n    '''\n    for session_id, row in sessions.iterrows():\n\n        truncated_file = True\n        directory = os.path.join(output_dir + '/session_' + str(session_id))\n\n        while truncated_file:\n            session = cache.get_session_data(session_id)\n            try:\n                print(session.specimen_name)\n                truncated_file = False\n            except OSError:\n                shutil.rmtree(directory)\n                print(\" Truncated spikes file, re-downloading\")\n\n        print('Downloaded session ' + str(session_id))\n\n        if download_lfp:\n            for probe_id, probe in session.probes.iterrows():\n\n                print(' ' + probe.description)\n                truncated_lfp = True\n\n                while truncated_lfp:\n                    try:\n                        lfp = session.get_lfp(probe_id)\n                        truncated_lfp = False\n                    except OSError:\n                        fname = directory + '/probe_' + str(probe_id) + '_lfp.nwb'\n                        os.remove(fname)\n                        print(\"  Truncated LFP file, re-downloading\")\n                    except ValueError:\n                        print(\"  LFP file not found.\")\n                        truncated_lfp = False\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.AllenTables","title":"<code>AllenTables</code>","text":"<p>A class that represents tables related to Allen Institute's Neuropixel data.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>object</code> <p>The cache object used to retrieve data.</p> <code>session_id</code> <code>int</code> <p>The ID of the session.</p> <p>Methods:</p> Name Description <code>make_tables</code> <p>Creates a dictionary of tables.</p> <code>make_columns</code> <p>Creates a dictionary of columns.</p> <code>layer_assignment</code> <p>Assigns cortical layer information to the units based on the channels and units tables.</p> <code>__init__</code> <p>Initializes the AllenTables object.</p> <code>__getitem__</code> <p>Retrieves a dataframe based on the given key.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>class AllenTables():\n    \"\"\"\n    A class that represents tables related to Allen Institute's Neuropixel data.\n\n    Attributes:\n        cache (object): The cache object used to retrieve data.\n        session_id (int): The ID of the session.\n\n    Methods:\n        make_tables: Creates a dictionary of tables.\n        make_columns: Creates a dictionary of columns.\n        layer_assignment: Assigns cortical layer information to the units based on the channels and units tables.\n        __init__: Initializes the AllenTables object.\n        __getitem__: Retrieves a dataframe based on the given key.\n    \"\"\"\n\n    def make_tables(self):\n        \"\"\"\n        Creates a dictionary of tables.\n\n        Returns:\n            tables (dict): A dictionary of tables.\n        \"\"\"\n        self.tables = {\n            'session': self.session,\n            # 'behavior': self.behavior,\n            'probes': self.probes,\n            'units': self.units,\n            'channels': self.channels\n        }\n        return self.tables\n\n    def make_columns(self):\n        \"\"\"\n        Creates a dictionary of columns.\n\n        Returns:\n            tables (dict): A dictionary of columns where the keys are the column names and the values are the assigned name.\n        \"\"\"\n        self.columns = {}\n\n        for name, table in self.tables.items():\n            # Get the columns from the dataframe\n            new_columns = dict_from_dataframe(table, name)\n\n            # Merge the new columns with the existing columns\n            for key, value in new_columns.items():\n                if key in self.columns:\n                    # Key exists in dictionary\n                    self.columns[key].append(value)\n                else:\n                    # Key does not exist in dictionary, add it\n                    self.columns[key] = [value]\n\n        return self.tables\n\n    def layer_assignment(self):\n        \"\"\"\n        Assigns cortical layer information to the units based on the channels and units tables.\n\n        This method uses the cortical_layer_assignment function to assign the cortical layer information\n        to the units in the tables. The assigned layer information is then appended to the 'layer' column\n        in the columns list.\n\n        \"\"\"\n        # self.units = cortical_layer_assignment(self.tables.channels, self.tables.units)\n        # self.columns.append('layer')\n\n    def __init__(self, cache, session_id, layer_assignment=False):\n        \"\"\"\n        Initializes the AllenTables object.\n\n        Args:\n            cache (object): The cache object used to retrieve data.\n            session_id (int): The ID of the session.\n            layer_assignment (bool, optional): Whether to assign cortical layers to the units. Defaults to False.\n\n        Note:\n            The elapsed time for this function is approximately 0.198 seconds.\n        \"\"\"\n        self.cache = cache\n        self.session_id = session_id\n\n        session = cache.get_session_data(cache.get_session_table().index.values[session_id])\n\n        # get the metadata tables\n        self.session = pd.DataFrame(cache.get_session_table().iloc[session_id])\n        self.session['ecephys_session_id'] = session_id\n        # self.behavior = cache.get_behavior_session_table()[cache.get_behavior_session_table()['ecephys_session_id'] == session_id]\n        self.probes = session.probes\n        self.units = session.units\n        self.channels = session.channels\n\n        # Make the tables and columns utilities\n        self.make_tables()\n        self.make_columns()\n        self.table_names = list(self.tables.keys())\n\n        # Assign cortical layers to the units\n        if layer_assignment:\n            raise NotImplementedError(\"The layer_assignment method is not implemented yet.\") # TODO: move this class to a new file, bcs the import of ccf files are too slow. Then just uncomment the lines in the layer_assignment function\n            self.layer_assignment()\n\n    def __getitem__(self, key: str)-&gt; pd.DataFrame:\n        \"\"\"\n        Retrieves a dataframe based on the given key.\n\n        Args:\n            key (str): The key to search for in the tables.\n\n        Returns:\n            pd.DataFrame: The dataframe containing the columns with the given key.\n        \"\"\"\n        # Make a mask for the self.tables with the key\n        key_mask = [name for name, table in self.tables.items() if key in table.columns]\n\n        # If there are no tables with the key, return None\n        if len(key_mask) == 0:\n            print(f'No tables with the key: {key}')\n            return None\n\n        # Add the key to each list of the table if it is not already there\n        commonColNames = jointColumns\n        for tab in commonColNames:\n            if key not in commonColNames[tab]:\n                commonColNames[tab].append(key)\n\n        # Get the columns with the key from each table\n        subtables = [self.tables[name][commonColNames[name]].reset_index() for name in key_mask]\n\n        # Merge the columns\n        mergedColumns = mergeDataframes(subtables)\n\n        if len(mergedColumns) == 1:\n            return mergedColumns[0]\n        else:\n            # TODO: Do this whole procedure in __getitem__ twice (or more), to merge the distant columns. (In the first time it can be only a list with more than one merged subtables)\n            raise ValueError('The mergedColumns is not a single dataframe')\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.AllenTables.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieves a dataframe based on the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to search for in the tables.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataframe containing the columns with the given key.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def __getitem__(self, key: str)-&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves a dataframe based on the given key.\n\n    Args:\n        key (str): The key to search for in the tables.\n\n    Returns:\n        pd.DataFrame: The dataframe containing the columns with the given key.\n    \"\"\"\n    # Make a mask for the self.tables with the key\n    key_mask = [name for name, table in self.tables.items() if key in table.columns]\n\n    # If there are no tables with the key, return None\n    if len(key_mask) == 0:\n        print(f'No tables with the key: {key}')\n        return None\n\n    # Add the key to each list of the table if it is not already there\n    commonColNames = jointColumns\n    for tab in commonColNames:\n        if key not in commonColNames[tab]:\n            commonColNames[tab].append(key)\n\n    # Get the columns with the key from each table\n    subtables = [self.tables[name][commonColNames[name]].reset_index() for name in key_mask]\n\n    # Merge the columns\n    mergedColumns = mergeDataframes(subtables)\n\n    if len(mergedColumns) == 1:\n        return mergedColumns[0]\n    else:\n        # TODO: Do this whole procedure in __getitem__ twice (or more), to merge the distant columns. (In the first time it can be only a list with more than one merged subtables)\n        raise ValueError('The mergedColumns is not a single dataframe')\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.AllenTables.__init__","title":"<code>__init__(cache, session_id, layer_assignment=False)</code>","text":"<p>Initializes the AllenTables object.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>object</code> <p>The cache object used to retrieve data.</p> required <code>session_id</code> <code>int</code> <p>The ID of the session.</p> required <code>layer_assignment</code> <code>bool</code> <p>Whether to assign cortical layers to the units. Defaults to False.</p> <code>False</code> Note <p>The elapsed time for this function is approximately 0.198 seconds.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def __init__(self, cache, session_id, layer_assignment=False):\n    \"\"\"\n    Initializes the AllenTables object.\n\n    Args:\n        cache (object): The cache object used to retrieve data.\n        session_id (int): The ID of the session.\n        layer_assignment (bool, optional): Whether to assign cortical layers to the units. Defaults to False.\n\n    Note:\n        The elapsed time for this function is approximately 0.198 seconds.\n    \"\"\"\n    self.cache = cache\n    self.session_id = session_id\n\n    session = cache.get_session_data(cache.get_session_table().index.values[session_id])\n\n    # get the metadata tables\n    self.session = pd.DataFrame(cache.get_session_table().iloc[session_id])\n    self.session['ecephys_session_id'] = session_id\n    # self.behavior = cache.get_behavior_session_table()[cache.get_behavior_session_table()['ecephys_session_id'] == session_id]\n    self.probes = session.probes\n    self.units = session.units\n    self.channels = session.channels\n\n    # Make the tables and columns utilities\n    self.make_tables()\n    self.make_columns()\n    self.table_names = list(self.tables.keys())\n\n    # Assign cortical layers to the units\n    if layer_assignment:\n        raise NotImplementedError(\"The layer_assignment method is not implemented yet.\") # TODO: move this class to a new file, bcs the import of ccf files are too slow. Then just uncomment the lines in the layer_assignment function\n        self.layer_assignment()\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.AllenTables.layer_assignment","title":"<code>layer_assignment()</code>","text":"<p>Assigns cortical layer information to the units based on the channels and units tables.</p> <p>This method uses the cortical_layer_assignment function to assign the cortical layer information to the units in the tables. The assigned layer information is then appended to the 'layer' column in the columns list.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def layer_assignment(self):\n    \"\"\"\n    Assigns cortical layer information to the units based on the channels and units tables.\n\n    This method uses the cortical_layer_assignment function to assign the cortical layer information\n    to the units in the tables. The assigned layer information is then appended to the 'layer' column\n    in the columns list.\n\n    \"\"\"\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.AllenTables.make_columns","title":"<code>make_columns()</code>","text":"<p>Creates a dictionary of columns.</p> <p>Returns:</p> Name Type Description <code>tables</code> <code>dict</code> <p>A dictionary of columns where the keys are the column names and the values are the assigned name.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def make_columns(self):\n    \"\"\"\n    Creates a dictionary of columns.\n\n    Returns:\n        tables (dict): A dictionary of columns where the keys are the column names and the values are the assigned name.\n    \"\"\"\n    self.columns = {}\n\n    for name, table in self.tables.items():\n        # Get the columns from the dataframe\n        new_columns = dict_from_dataframe(table, name)\n\n        # Merge the new columns with the existing columns\n        for key, value in new_columns.items():\n            if key in self.columns:\n                # Key exists in dictionary\n                self.columns[key].append(value)\n            else:\n                # Key does not exist in dictionary, add it\n                self.columns[key] = [value]\n\n    return self.tables\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.AllenTables.make_tables","title":"<code>make_tables()</code>","text":"<p>Creates a dictionary of tables.</p> <p>Returns:</p> Name Type Description <code>tables</code> <code>dict</code> <p>A dictionary of tables.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def make_tables(self):\n    \"\"\"\n    Creates a dictionary of tables.\n\n    Returns:\n        tables (dict): A dictionary of tables.\n    \"\"\"\n    self.tables = {\n        'session': self.session,\n        # 'behavior': self.behavior,\n        'probes': self.probes,\n        'units': self.units,\n        'channels': self.channels\n    }\n    return self.tables\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.dict_from_dataframe","title":"<code>dict_from_dataframe(df, name)</code>","text":"<p>Create a dictionary from a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to convert.</p> required <code>name</code> <code>str</code> <p>The name to assign to each column in the dictionary.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where the keys are the column names and the values are the assigned name.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def dict_from_dataframe(df: pd.DataFrame, name: str) -&gt; dict:\n    \"\"\"\n    Create a dictionary from a DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to convert.\n        name (str): The name to assign to each column in the dictionary.\n\n    Returns:\n        dict: A dictionary where the keys are the column names and the values are the assigned name.\n    \"\"\"\n    return {col_name: name for col_name in df.columns}\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_area_change_responses","title":"<code>get_area_change_responses(session, area_of_interest)</code>","text":"<p>Calculate the change responses for units in a specific area of interest.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The session object.</p> required <code>area_of_interest</code> <code>str or list[str]</code> <p>The acronym of the area of interest.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the change responses.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_area_change_responses(session, area_of_interest) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the change responses for units in a specific area of interest.\n\n    Args:\n        session (Session): The session object.\n        area_of_interest (str or list[str]): The acronym of the area of interest.\n\n    Returns:\n        np.ndarray: An array containing the change responses.\n    \"\"\"\n    stimulus_presentations = get_stimulus_presentations(session)\n\n    area_change_responses = []\n    area_units = get_area_units(area_of_interest=area_of_interest)\n    spike_times = session.spike_times\n    time_before_change = 1\n    duration = 2.5\n    for iu, unit in area_units.iterrows():\n        unit_spike_times = spike_times[iu]\n        unit_change_response, bins = makePSTH(unit_spike_times,\n                                                stimulus_presentations['start_time'].values - time_before_change,\n                                                duration, binSize=0.01)\n        area_change_responses.append(unit_change_response)\n    area_change_responses = np.array(area_change_responses)\n\n    return area_change_responses\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_area_receptive_fields","title":"<code>get_area_receptive_fields(spike_times, stimulus_presentations, area_of_interest)</code>","text":"<p>Get the receptive fields for units in a specific area of interest by Gabors. (There are many trials, and only in the second block are gabors for the receptive fields.)</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>dict</code> <p>A dictionary mapping unit IDs to their corresponding spike times.</p> required <code>stimulus_presentations</code> <code>DataFrame</code> <p>A DataFrame containing the stimulus presentations.</p> required <code>area_of_interest</code> <code>str or list[str]</code> <p>The acronym of the area of interest.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the receptive fields.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_area_receptive_fields(spike_times, stimulus_presentations, area_of_interest) -&gt; list:\n    \"\"\"\n    Get the receptive fields for units in a specific area of interest by Gabors. (There are many trials, and only in the second block are gabors for the receptive fields.)\n\n    Args:\n        spike_times (dict): A dictionary mapping unit IDs to their corresponding spike times.\n        stimulus_presentations (DataFrame): A DataFrame containing the stimulus presentations.\n        area_of_interest (str or list[str]): The acronym of the area of interest.\n\n    Returns:\n        list: A list containing the receptive fields.\n    \"\"\"\n    rf_stim_table = stimulus_presentations[stimulus_presentations['stimulus_name'].str.contains('gabor')]\n\n    # position categories/ypes of gabor along azimuth\n    xs = np.sort(rf_stim_table.position_x.unique())\n    # positions of gabor along elevation\n    ys = np.sort(rf_stim_table.position_y.unique())\n\n    def find_rf(spikes, xs, ys) -&gt; np.ndarray:\n        unit_rf = np.zeros([ys.size, xs.size])\n        for ix, x in enumerate(xs):\n            for iy, y in enumerate(ys):\n                stim_times = rf_stim_table[(rf_stim_table.position_x == x)\n                                            &amp; (rf_stim_table.position_y == y)]['start_time'].values\n                unit_response, bins = makePSTH(spikes,\n                                                stim_times + 0.01,\n                                                0.2, binSize=0.001)\n                unit_rf[iy, ix] = unit_response.mean()\n        return unit_rf\n\n    area_rfs = []\n    area_units = get_area_units(area_of_interest=area_of_interest)\n    for iu, unit in area_units.iterrows():\n        unit_spike_times = spike_times[iu]\n        unit_rf = find_rf(unit_spike_times, xs, ys)\n        area_rfs.append(unit_rf)\n\n    return area_rfs\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_area_units","title":"<code>get_area_units(units, area_of_interest)</code>","text":"<p>Retrieve the units in a specific area of interest.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>DataFrame</code> <p>The DataFrame containing the units.</p> required <code>area_of_interest</code> <code>str or list[str]</code> <p>The acronym of the area of interest.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the units in the specified area.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_area_units(units: pd.DataFrame, area_of_interest) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieve the units in a specific area of interest.\n\n    Args:\n        units (pd.DataFrame): The DataFrame containing the units.\n        area_of_interest (str or list[str]): The acronym of the area of interest.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the units in the specified area.\n    \"\"\"\n\n    # Filter the units based on the area of interest\n    good_unit_filter = ((units['snr'] &gt; 1) &amp;\n                        (units['isi_violations'] &lt; 1) &amp;\n                        (units['firing_rate'] &gt; 0.1))\n    good_units = units.loc[good_unit_filter]\n\n    # Get the units in the area of interest\n    if type(area_of_interest) == str:\n        area_units = good_units[good_units['ecephys_structure_acronym'] == area_of_interest]\n    elif type(area_of_interest) == list:\n        area_units = good_units[good_units['ecephys_structure_acronym'].isin(area_of_interest)]\n    else:\n        raise ValueError('area_of_interest must be a string or a list of strings')\n\n    return area_units\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_average_unit_responses","title":"<code>get_average_unit_responses(units, spike_times, trial_start, duration=0.03, binSize=0.001)</code>","text":"<p>Calculate the average unit responses for each unit in the given units DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>DataFrame</code> <p>DataFrame containing information about the units.</p> required <code>spike_times</code> <code>list</code> <p>List of spike times for each unit.</p> required <code>trial_start</code> <code>float</code> <p>Start time of the trial.</p> required <code>duration</code> <code>float</code> <p>Total duration of trial for PSTH in seconds. Default is 0.03.</p> <code>0.03</code> <code>binSize</code> <code>float</code> <p>Bin size for PSTH in seconds. Default is 0.001.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>response</code> <code>ndarray</code> <p>Array containing the average unit responses, shape (units, duration/binSize)</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_average_unit_responses(units, spike_times, trial_start, duration=0.03, binSize=0.001):\n    \"\"\"\n    Calculate the average unit responses for each unit in the given units DataFrame.\n\n    Args:\n        units (DataFrame): DataFrame containing information about the units.\n        spike_times (list): List of spike times for each unit.\n        trial_start (float): Start time of the trial.\n        duration (float): Total duration of trial for PSTH in seconds. Default is 0.03.\n        binSize (float): Bin size for PSTH in seconds. Default is 0.001.\n\n    Returns:\n        response (numpy.ndarray): Array containing the average unit responses, shape (units, duration/binSize)\n    \"\"\"\n    response = []\n    unit_id = []\n    for iu, unit in units.iterrows():\n        unit_spike_times = spike_times[iu]\n        unit_response, bins = makePSTH(unit_spike_times,\n                                        trial_start, duration,\n                                        binSize=binSize)\n\n        response.append(unit_response)\n        unit_id.append(iu)\n\n    return np.array(response)\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_response_magnitudes","title":"<code>get_response_magnitudes(opto_response)</code>","text":"<p>Calculate the response magnitudes of optogenetic stimulation.</p> <p>Parameters:</p> Name Type Description Default <code>opto_response</code> <code>ndarray</code> <p>Array containing the optogenetic response data.</p> required <p>Returns:</p> Name Type Description <code>response_magnitudes</code> <code>ndarray</code> <p>Array of response magnitudes calculated for each trial.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_response_magnitudes(opto_response):\n    \"\"\"\n    Calculate the response magnitudes of optogenetic stimulation.\n\n    Args:\n        opto_response (numpy.ndarray): Array containing the optogenetic response data.\n\n    Returns:\n        response_magnitudes (numpy.ndarray): Array of response magnitudes calculated for each trial.\n    \"\"\"\n    baseline_window = slice(0, 9)  # baseline epoch\n    response_window = slice(11, 18)  # laser epoch\n\n    response_magnitudes = np.mean(opto_response[:, response_window], axis=1) \\\n                        - np.mean(opto_response[:, baseline_window], axis=1)\n\n    return response_magnitudes\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_stimulus_presentations","title":"<code>get_stimulus_presentations(session)</code>","text":"<p>Retrieve the stimulus presentations for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The session object.</p> required <p>Returns:</p> Name Type Description <code>stimulus_presentations</code> <code>DataFrame</code> <p>A DataFrame containing the stimulus presentations.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_stimulus_presentations(session):\n    \"\"\"\n    Retrieve the stimulus presentations for a given session.\n\n    Args:\n        session (Session): The session object.\n\n    Returns:\n        stimulus_presentations (DataFrame): A DataFrame containing the stimulus presentations.\n    \"\"\"\n    stimulus_presentations = session.stimulus_presentations\n    return stimulus_presentations\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_table","title":"<code>get_table(cache, session_id, table_name)</code>","text":"<p>Get a table from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>EcephysProjectCache</code> <p>The cache object.</p> required <code>session_id</code> <code>int</code> <p>The session identifier.</p> required <code>table_name</code> <code>str</code> <p>The name of the table to retrieve.</p> required <p>Returns:</p> Name Type Description <code>session_table</code> <code>DataFrame</code> <p>The table data.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_table(cache, session_id, table_name):\n    \"\"\"\n    Get a table from the cache.\n\n    Args:\n        cache (EcephysProjectCache): The cache object.\n        session_id (int): The session identifier.\n        table_name (str): The name of the table to retrieve.\n\n    Returns:\n        session_table (DataFrame): The table data.\n    \"\"\"\n    return cache.get_session_data(session_id)[table_name]\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_unit_channels","title":"<code>get_unit_channels(session, log_all_areas=False)</code>","text":"<p>Retrieve the unit channels for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The session object.</p> required <code>log_all_areas</code> <code>bool</code> <p>Whether to log all brain areas recorded during the session. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the unit channels.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_unit_channels(session, log_all_areas=False) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieve the unit channels for a given session.\n\n    Args:\n        session (Session): The session object.\n        log_all_areas (bool, optional): Whether to log all brain areas recorded during the session. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the unit channels.\n    \"\"\"\n\n    # Merging unit and channel dataframes will give us CCF coordinates for each unit\n    units = session.get_units()\n    channels = session.get_channels()\n    unit_channels = units.merge(channels, left_on='peak_channel_id', right_index=True)\n\n    # Which brain structures were recorded during this session\n    brain_areas_recorded = unit_channels.value_counts('ecephys_structure_acronym')\n\n    if log_all_areas:\n        print(brain_areas_recorded, '\\n', brain_areas_recorded)\n\n    return unit_channels\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.get_unit_responses","title":"<code>get_unit_responses(units, spike_times, trial_start, trial_end, stepSize=0.01, binSize=0.05, progressbar=True)</code>","text":"<p>Calculate the unit responses for each trial and time bin.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>DataFrame</code> <p>A DataFrame containing unit data.</p> required <code>spike_times</code> <code>dict</code> <p>A dictionary mapping unit IDs to their corresponding spike times.</p> required <code>trial_start</code> <code>array - like</code> <p>An array-like object containing the start times of each trial.</p> required <code>trial_end</code> <code>array - like</code> <p>An array-like object containing the end times of each trial.</p> required <code>stepSize</code> <code>float</code> <p>The size of the time step. Defaults to 0.010.</p> <code>0.01</code> <code>binSize</code> <code>float</code> <p>The size of the time bin. Defaults to 0.050.</p> <code>0.05</code> <code>progressbar</code> <code>bool</code> <p>Whether to display a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tensor</code> <code>ndarray</code> <p>A 3-dimensional numpy array containing the unit responses for each trial and time bin.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def get_unit_responses(units, spike_times, trial_start, trial_end, stepSize=0.010, binSize=0.050, progressbar=True):\n    \"\"\"\n    Calculate the unit responses for each trial and time bin.\n\n    Args:\n        units (DataFrame): A DataFrame containing unit data.\n        spike_times (dict): A dictionary mapping unit IDs to their corresponding spike times.\n        trial_start (array-like): An array-like object containing the start times of each trial.\n        trial_end (array-like): An array-like object containing the end times of each trial.\n        stepSize (float, optional): The size of the time step. Defaults to 0.010.\n        binSize (float, optional): The size of the time bin. Defaults to 0.050.\n        progressbar (bool, optional): Whether to display a progress bar. Defaults to True.\n\n    Returns:\n        tensor (ndarray): A 3-dimensional numpy array containing the unit responses for each trial and time bin.\n    \"\"\"\n    n_unit = len(units)\n    n_trial = len(trial_start)\n    trial_length = np.mean(trial_end - trial_start)\n    n_step = int(trial_length / stepSize)\n    n_bin = int(trial_length / binSize)\n\n    tensor = np.zeros((n_unit, n_trial, n_step))\n\n    if progressbar:\n        printProgressBar(0, n_unit, prefix='Units:', length=50)\n\n    for i, unit_ID in enumerate([unit_ID for unit_ID, unit_data, in units.iterrows()]):\n        # Get the spike times for the unit\n        unit_spike_times = spike_times[unit_ID]\n\n        # Loop through trials and time\n        for j, (start, end) in enumerate(zip(trial_start, trial_end)):  # Trials\n            for k, time in enumerate(np.arange(start, end, stepSize)): # Time\n\n                # Check if k is out of range. (This can happen because of different floating point rounding in int() and np.arange() functions i guess.)\n                if k == n_bin:\n                    break\n\n                # Find the bin indices\n                bin_start_idx = np.searchsorted(unit_spike_times, time)\n                bin_end_idx = np.searchsorted(unit_spike_times, time+binSize)\n\n                # Get the spikes in the time bin\n                spikes_in_timebin = unit_spike_times[bin_start_idx:bin_end_idx]\n\n                # Count the number of spikes in the time bin\n                tensor[i, j, k] = len(spikes_in_timebin)\n\n        if progressbar:\n            printProgressBar(i + 1, n_unit, prefix='Units:', length=50)\n\n    # Count the number of spikes in the tensor\n    count = np.count_nonzero(tensor)\n    print('Spike count in the data:', count)\n\n    return tensor\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.makePSTH","title":"<code>makePSTH(spikes, startTimes, windowDur, binSize=0.001)</code>","text":"<p>Compute the Peri-Stimulus Time Histogram (PSTH).</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>array - like</code> <p>Array of spike times.</p> required <code>startTimes</code> <code>array - like</code> <p>Array of stimulus start times.</p> required <code>windowDur</code> <code>float</code> <p>Duration of the time window to consider for the PSTH.</p> required <code>binSize</code> <code>float</code> <p>Size of the time bins for the histogram. Defaults to 0.001.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>return</code> <code>tuple[NDArray[float64], NDArray[floating[Any]]]</code> <p>A tuple containing the PSTH counts and the bin edges.</p> Notes <p>The PSTH is a histogram that represents the firing rate of neurons in response to a stimulus over time. It is computed by dividing the time window into bins and counting the number of spikes that fall into each bin.</p> Source code in <code>utils/neuropixel.py</code> <pre><code>def makePSTH(spikes, startTimes, windowDur, binSize=0.001):\n    \"\"\"\n\n    Compute the Peri-Stimulus Time Histogram (PSTH).\n\n    Args:\n        spikes (array-like): Array of spike times.\n        startTimes (array-like): Array of stimulus start times.\n        windowDur (float): Duration of the time window to consider for the PSTH.\n        binSize (float, optional): Size of the time bins for the histogram. Defaults to 0.001.\n\n    Returns:\n        return (tuple[NDArray[float64], NDArray[floating[Any]]]): A tuple containing the PSTH counts and the bin edges.\n\n    Notes:\n        The PSTH is a histogram that represents the firing rate of neurons in response to a stimulus over time.\n        It is computed by dividing the time window into bins and counting the number of spikes that fall into each bin.\n    \"\"\"\n    bins = np.arange(0, windowDur+binSize, binSize)\n    counts = np.zeros(bins.size-1)\n\n    # enumerate through trials\n    for i, start in enumerate(startTimes):\n        startInd = np.searchsorted(spikes, start)\n        endInd = np.searchsorted(spikes, start+windowDur)\n        counts = counts + np.histogram(spikes[startInd:endInd]-start, bins)[0]\n\n    counts = counts/startTimes.size\n    return counts/binSize, bins\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.optotagging","title":"<code>optotagging(opto_table, spike_times, area_of_interest)</code>","text":"<p>Perform optotagging analysis on units in a specific area of interest.</p> <p>Parameters:</p> Name Type Description Default <code>opto_table</code> <code>DataFrame</code> <p>A DataFrame containing the optotagging stimulus table.</p> required <code>spike_times</code> <code>dict</code> <p>A dictionary mapping unit IDs to their corresponding spike times.</p> required <code>area_of_interest</code> <code>str or list[str]</code> <p>The acronym of the area of interest.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the optogenetic responses.</p> <p>Notes:</p> <p>Since this is an SST mouse, we should see putative SST+ interneurons that are activated during our optotagging protocol. Let's load the optotagging stimulus table and plot PSTHs triggered on the laser onset. For more examples and useful info about optotagging, you can check out the Visual Coding Neuropixels Optagging notebook here (though note that not all the functionality in the visual coding SDK will work for this dataset).</p> <p>We use 2 different laser waveforms: a short square pulse that's 10 ms long and a half-period cosine that's 1 second long. We drive each at three light levels, giving us 6 total conditions</p> <p>most units don't respond to the short laser pulse. Note that the activity occurring at the onset and offset of the laser is artifactual and should be excluded from analysis!</p> <pre><code>opto_table = session.optotagging_table\n</code></pre> Source code in <code>utils/neuropixel.py</code> <pre><code>def optotagging(opto_table, spike_times, area_of_interest) -&gt; np.ndarray:\n    \"\"\"\n    Perform optotagging analysis on units in a specific area of interest.\n\n    Args:\n        opto_table (DataFrame): A DataFrame containing the optotagging stimulus table.\n        spike_times (dict): A dictionary mapping unit IDs to their corresponding spike times.\n        area_of_interest (str or list[str]): The acronym of the area of interest.\n\n    Returns:\n        np.ndarray: An array containing the optogenetic responses.\n\n    Notes:\n\n    Since this is an SST mouse, we should see putative SST+ interneurons that are activated during our optotagging protocol. Let's load the optotagging stimulus table and plot PSTHs triggered on the laser onset. For more examples and useful info about optotagging, you can check out the Visual Coding Neuropixels Optagging notebook here (though note that not all the functionality in the visual coding SDK will work for this dataset).\n\n    We use 2 different laser **waveforms**: a short square pulse that's **10 ms** long and a half-period cosine that's 1 second long.\n    We drive each at three light **levels**, giving us 6 total conditions\n\n    most units don't respond to the short laser pulse.\n    Note that the activity occurring at the onset and offset of the laser is artifactual and should be excluded from analysis!\n\n    ```\n    opto_table = session.optotagging_table\n    ```\n    \"\"\"\n    print(opto_table.head())\n\n    # Get the short pulses\n    duration = opto_table.duration.min()\n\n    # Get the high power trials\n    level = opto_table.level.max()\n\n    if 'VIS' not in area_of_interest:\n        raise ValueError('To perform optotagging, the area of interest must be a visual area')\n\n    cortical_units = get_area_units(area_of_interest=area_of_interest)\n\n    opto_times = opto_table.loc[(opto_table['duration'] == duration) &amp;\n                                (opto_table['level'] == level)]['start_time'].values\n\n    time_before = 0.01\n    duration = 0.03\n    binSize = 0.001\n    opto_response = []\n    unit_id = []\n    for iu, unit in cortical_units.iterrows():\n        unit_spike_times = spike_times[iu]\n        unit_response, bins = makePSTH(unit_spike_times,\n                                        opto_times - time_before, duration,\n                                        binSize=binSize)\n\n        opto_response.append(unit_response)\n        unit_id.append(iu)\n\n    opto_response = np.array(opto_response)\n\n    return opto_response\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.rasterplot","title":"<code>rasterplot(session, times)</code>","text":"<p>Generate a raster plot for a given session and times.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The session object.</p> required <code>times</code> <code>DataFrame</code> <p>The times DataFrame.</p> required Source code in <code>utils/neuropixel.py</code> <pre><code>def rasterplot(session, times):\n    \"\"\"\n    Generate a raster plot for a given session and times.\n\n    Args:\n        session (Session): The session object.\n        times (DataFrame): The times DataFrame.\n\n    \"\"\"\n    first_drifting_grating_presentation_id = times['stimulus_presentation_id'].values[0]\n    plot_times = times[times['stimulus_presentation_id'] == first_drifting_grating_presentation_id]\n\n    fig = raster_plot(plot_times, title=f'spike raster for stimulus presentation {first_drifting_grating_presentation_id}')\n    plt.show()\n\n    # Print out this presentation also\n    session.stimulus_presentations.loc[first_drifting_grating_presentation_id]\n</code></pre>"},{"location":"references/utilities/neuropixel/#utils.neuropixel.stimulus_duration","title":"<code>stimulus_duration(session, stimulus_block)</code>","text":"<p>Plot the histogram of stimulus durations for a given session and stimulus block.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>The session object.</p> required <code>stimulus_block</code> <code>int</code> <p>The stimulus block.</p> required Source code in <code>utils/neuropixel.py</code> <pre><code>def stimulus_duration(session, stimulus_block):\n    \"\"\"\n    Plot the histogram of stimulus durations for a given session and stimulus block.\n\n    Args:\n        session (Session): The session object.\n        stimulus_block (int): The stimulus block.\n\n    \"\"\"\n    stimulus_presentations = session.stimulus_presentations[session.stimulus_presentations['active'] == True &amp;\n                                                            session.stimulus_presentations['stimulus_block'] == stimulus_block &amp;\n                                                            session.stimulus_presentations['omitted'] == False]\n    stimulus_presentations['duration'].hist(bins=100)\n    plt.xlabel('Flash Duration (s)')\n    plt.ylabel('Count')\n    plt.show()\n</code></pre>"},{"location":"references/utilities/other-utils/","title":"Other Utility Tools","text":"<p>Submodule containing tools to extract features from the data.</p> <p>Tools for simple layer analysis.</p> <p>Functions: - get_layers(units): Get the list of cortical layers from unit tables.</p>"},{"location":"references/utilities/other-utils/#utils.feature_functions.perform_cross_validation","title":"<code>perform_cross_validation(X, y, model, cv=5)</code>","text":"<p>Perform cross-validation using the given model. Parameters:     X (array-like): The input features.     y (array-like): The target variable.     model: The machine learning model to use for cross-validation.     cv (int, optional): The number of folds for cross-validation. Default is 5. Returns:     scores (array-like): The cross-validation scores.</p> Source code in <code>utils/feature_functions.py</code> <pre><code>def perform_cross_validation(X, y, model, cv=5):\n    \"\"\"\n    Perform cross-validation using the given model.\n    Parameters:\n        X (array-like): The input features.\n        y (array-like): The target variable.\n        model: The machine learning model to use for cross-validation.\n        cv (int, optional): The number of folds for cross-validation. Default is 5.\n    Returns:\n        scores (array-like): The cross-validation scores.\n    \"\"\"\n\n    # X = dataframe.drop('target', axis=1)  # Assuming 'target' is the target variable column\n    # y = dataframe['target']\n\n    scores = cross_val_score(model, X, y, cv=cv)  # Perform 5-fold cross-validation\n\n    return scores\n</code></pre>"},{"location":"references/utilities/other-utils/#utils.layers.get_layers","title":"<code>get_layers(units)</code>","text":"<p>Get the unique cortical layers from the given DataFrame.</p> <p>Parameters: units (pd.DataFrame): The DataFrame containing the units.</p> <p>Returns: list: A list of unique cortical layers.</p> Source code in <code>utils/layers.py</code> <pre><code>def get_layers(units: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Get the unique cortical layers from the given DataFrame.\n\n    Parameters:\n    units (pd.DataFrame): The DataFrame containing the units.\n\n    Returns:\n    list: A list of unique cortical layers.\n\n    \"\"\"\n    # Get the unique layers\n    layers = units['ecephys_structure_acronym'].unique()\n\n    # Sort the layers\n    layers.sort()\n\n    # Return the layers\n    return layers\n</code></pre>"},{"location":"references/utilities/plots/","title":"Plots","text":"<p>This submodule contains tools for plotting the results.</p> <p>Functions:</p> <ul> <li>simple_mean_SEM_time_plot(ax, mean, ylabel, title=None, SEM=None, SEM_multiplier=2, time_series=None, color=None, xlabel=None, alpha=0.2, linewidth=None, xticks=None, xticklabels=None, yticks=None, yticklabels=None, label=None, xlim=None, ylim=None) -&gt; plt.Figure: Plots the mean and standard error of the mean of the results as a function of time.</li> <li>simple_rrr_plot(result, axs=None) -&gt; plt.Figure: Plots the results of the reduced rank regression analysis.</li> <li>simple_rrr_plot_mean(result, ax=None) -&gt; plt.Figure: Plots the mean of the results of the reduced rank regression analysis.</li> <li>raster_plot(spike_times, figsize=(8, 8), cmap=plt.cm.tab20, title='spike raster', cycle_colors=False, ax=None) -&gt; plt.Figure: Plots the spike raster.</li> <li>cross_correlation_plot(cross_correlation, time_series=None, title='Cross-correlation', ax=None) -&gt; plt.Figure: Plots the cross-correlation between two signals.</li> <li>cross_time_correlation_coefficients_plot(coeffs, time_series=None, first_dim_label=None, second_dim_label=None, title='Cross-time-correlation', ax=None) -&gt; plt.Figure: Plots the cross-correlation between two signals.</li> <li>rrr_rank_plot(scores, title='RRR test scores (r2)', time_series=None, ax=None) -&gt; plt.Figure: Plots the RRR test scores as a function of rank and time.</li> <li>rrr_rank_plot_mean(result, ax=None) -&gt; plt.Figure: Plots the mean of the results of the reduced rank regression analysis.</li> <li>rrr_rank_plot_over_time(scores, title='RRR test scores', time_series=None, fig=None, axs=None, label=None, log=False) -&gt; plt.Figure: Plots the RRR test scores as a function of rank and time.</li> <li>score_plot_by_time(scores, title=None, time_series=None, ax=None, label='', color=None) -&gt; plt.Figure: Plots the RRR test scores as a function of rank and time.</li> <li>cv_rank_time_plot(results, title=None, ax=None, max=None, xlabel=None, ylabel=None, xticks=None, yticks=None) -&gt; plt.Figure: Plots the results of the cross-validation and rank.</li> <li>score_time(mean, sem, title=None, xlabel='Time', ylabel='R^2', time_series=None) -&gt; plt.Figure: Plots the mean and standard error of the mean of the results as a function of time.</li> <li>crosstime_RRR(ax, matrix, predictor, target, timeseries, vlim, tick_frequency = 5) -&gt; image.AxesImage: Plot a cross-timepoint correlation matrix.</li> <li>rrr_time_slice(ax, results, predictor_time, timepoints=None, colors=None, ylim=(None, None), isWithinSameArea=True) -&gt; image.AxesImage: Plot the results of the reduced rank regression analysis.</li> <li>plot_stimuli(ecephys_session) -&gt; plt.Figure: Plot the stimuli templates for the unwarped images.</li> </ul>"},{"location":"references/utilities/plots/#utils.plots.cross_correlation_plot","title":"<code>cross_correlation_plot(cross_correlation, time_series=None, title='Cross-correlation', ax=None)</code>","text":"<p>Plots the cross-correlation between two signals.</p> <p>Parameters:</p> Name Type Description Default <code>cross_correlation</code> <code>array - like</code> <p>A one-dimensional array-like object representing the cross-correlation values.</p> required <code>time_series</code> <code>array - like</code> <p>A one-dimensional array-like object representing the time series. If not provided, it will be generated using the length of cross_correlation.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Default is 'Cross-correlation'.</p> <code>'Cross-correlation'</code> <code>ax</code> <code>Axes</code> <p>The axes on which to plot. If not provided, a new figure and axes will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>The figure object containing the plot.</p> Source code in <code>utils/plots.py</code> <pre><code>def cross_correlation_plot(cross_correlation, time_series=None, title='Cross-correlation', ax=None) -&gt; plt.Figure:\n    \"\"\"\n    Plots the cross-correlation between two signals.\n\n    Parameters:\n        cross_correlation (array-like): A one-dimensional array-like object representing the cross-correlation values.\n        time_series (array-like, optional): A one-dimensional array-like object representing the time series. If not provided, it will be generated using the length of cross_correlation.\n        title (str, optional): The title of the plot. Default is 'Cross-correlation'.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If not provided, a new figure and axes will be created.\n\n    Returns:\n        figure (matplotlib.figure.Figure): The figure object containing the plot.\n    \"\"\"\n\n    # If time_series is not provided, generate it\n    if time_series is None:\n        time_series = np.arange(len(cross_correlation))\n\n    # Create a new figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the cross-correlation\n    ax.plot(time_series, cross_correlation)\n    ax.set_xlabel('Time lag')\n    ax.set_ylabel('Cross-correlation')\n    ax.set_title(title)\n\n    # # Plot the cross-correlation\n    # plt.plot(time_series, cross_correlation)\n    # plt.xlabel('Time lag')\n    # plt.ylabel('Cross-correlation')\n    # plt.title(title)\n\n    return fig\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.cross_time_correlation_coefficients_plot","title":"<code>cross_time_correlation_coefficients_plot(coeffs, time_series=None, first_dim_label=None, second_dim_label=None, title='Cross-time-correlation', ax=None)</code>","text":"<p>Plots the cross-correlation between two signals. Colors range from blue (negative) to red (positive), with white representing zero.</p> <p>Parameters:</p> Name Type Description Default <code>coeffs</code> <code>array - like</code> <p>A three-dimensional array-like object representing the cross-correlation coefficients.</p> required <code>time_series</code> <code>array - like</code> <p>A one-dimensional array-like object representing the time series. If not provided, it will be generated using the length of coeffs.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Default is 'Cross-time-correlation'.</p> <code>'Cross-time-correlation'</code> <code>ax</code> <code>Axes</code> <p>The axes on which to plot. If not provided, a new figure and axes will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>The figure containing the plot.</p> Source code in <code>utils/plots.py</code> <pre><code>def cross_time_correlation_coefficients_plot(coeffs, time_series=None, first_dim_label=None, second_dim_label=None, title='Cross-time-correlation', ax=None) -&gt; plt.Figure:\n    \"\"\"\n    Plots the cross-correlation between two signals. Colors range from blue (negative) to red (positive), with white representing zero.\n\n    Parameters:\n        coeffs (array-like): A three-dimensional array-like object representing the cross-correlation coefficients.\n        time_series (array-like, optional): A one-dimensional array-like object representing the time series. If not provided, it will be generated using the length of coeffs.\n        title (str, optional): The title of the plot. Default is 'Cross-time-correlation'.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If not provided, a new figure and axes will be created.\n\n    Returns:\n        figure (matplotlib.pyplot.Figure): The figure containing the plot.\n    \"\"\"\n\n    # Reverse the second dimension of the coefficients\n    coeffs = np.flip(coeffs, axis=1)\n\n    # If time_series is not provided, generate it\n    if time_series is None:\n        time_step = preprocess['step-size']\n        duration = preprocess['stimulus-duration']\n        time_series = np.arange(0, duration, time_step).round(3)\n\n    # Create a custom color palette ranging from blue (negative) to red (positive), with white representing zero\n    cmap = 'bwr'\n\n    # Create a new figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the results using the custom color palette\n    im = ax.imshow(coeffs, cmap=cmap, aspect='auto')\n    ax.set_title(title)\n    ax.set_xlabel(first_dim_label)\n    ax.set_ylabel(second_dim_label)\n\n    # Set xticklabels and yticklabels corresponding to some values of the time_series\n    ax.set_xticks(np.arange(0, len(time_series), 4))\n    ax.set_xticklabels(time_series[::4])\n\n    # Plot a reverse time series on the y-axis\n    ax.set_yticks(np.arange(0, len(time_series), 4))\n    ax.set_yticklabels(time_series[::4][::-1])\n\n    # Attach colorbar to the last plot\n    fig.colorbar(im, ax=ax)\n\n    return fig\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.crosstime_RRR","title":"<code>crosstime_RRR(ax, matrix, predictor, target, timeseries, vlim, tick_frequency=5)</code>","text":"<p>Plot a cross-timepoint correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot the matrix.</p> required <code>matrix</code> <code>ndarray</code> <p>The correlation matrix to be plotted.</p> required <code>predictor</code> <code>str</code> <p>The label for the predictor variable.</p> required <code>target</code> <code>str</code> <p>The label for the target variable.</p> required <code>timeseries</code> <code>ndarray</code> <p>The array of timepoints.</p> required <code>vlim</code> <code>tuple</code> <p>The range of values for the colormap.</p> required <code>tick_frequency</code> <code>int</code> <p>The frequency of the ticks on the axes.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>cax</code> <code>AxesImage</code> <p>The plotted image of the matrix.</p> Source code in <code>utils/plots.py</code> <pre><code>def crosstime_RRR(ax, matrix, predictor, target, timeseries, vlim, tick_frequency = 5) -&gt; image.AxesImage:\n    \"\"\"\n    Plot a cross-timepoint correlation matrix.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The axes on which to plot the matrix.\n        matrix (numpy.ndarray): The correlation matrix to be plotted.\n        predictor (str): The label for the predictor variable.\n        target (str): The label for the target variable.\n        timeseries (numpy.ndarray): The array of timepoints.\n        vlim (tuple): The range of values for the colormap.\n        tick_frequency (int): The frequency of the ticks on the axes.\n\n    Returns:\n        cax (matplotlib.image.AxesImage): The plotted image of the matrix.\n    \"\"\"\n\n    # The diagonal of the matrix should be nan\n    # np.fill_diagonal(matrix, np.nan)\n\n    # Reverse the rows of the matrix\n    matrix = matrix[::-1]\n\n    # Plot the matrix. colormap do not use white color. Make the resolution higher.\n    cax = ax.imshow(matrix, cmap='terrain', interpolation='bilinear', \n            extent=[0, timeseries[-1], 0, timeseries[-1]], vmin=vlim[0], vmax=vlim[1])\n\n    # black line from 0;0 to the max;max\n    ax.plot([0, timeseries[-1]], [0, timeseries[-1]],\n        color='black', linewidth=1)\n\n    # Set the ticks and labels\n    ax.set_xticks(timeseries[::tick_frequency])\n    ax.set_yticks(timeseries[::tick_frequency])\n    ax.set_xlabel(f\"{target} time (s)\")\n    ax.set_ylabel(f\"{predictor} time (s)\")\n\n    return cax\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.cv_rank_time_plot","title":"<code>cv_rank_time_plot(results, title=None, ax=None, max=None, xlabel=None, ylabel=None, xticks=None, yticks=None)</code>","text":"<p>Plot the results of the cross-validation and rank.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ndarray</code> <p>The results of the cross-validation and rank.</p> required <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The axes to plot on (optional). Default is None.</p> <code>None</code> <code>max</code> <code>int</code> <p>The maximum value for the colorbar (optional). Default is None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>The label for the x-axis (optional). Default is None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>The label for the y-axis (optional). Default is None.</p> <code>None</code> <code>xticks</code> <code>List[str]</code> <p>The tick labels for the x-axis (optional). Default is None.</p> <code>None</code> <code>yticks</code> <code>List[str]</code> <p>The tick labels for the y-axis (optional). Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The figure containing the plot (optional).</p> <code>im</code> <code>AxesImage</code> <p>The image representing the plot.</p> <p>If ax is None, a new figure is created. The plot is displayed using a colormap with the 'viridis' color map. The colorbar is added to the plot. If fig is not None, the figure is returned. If fig is None, the image is returned.</p> <p>Note: Since the first dimension corresponds to the Y-axis, and the second dimension corresponds to the X-axis, the results is transposed before plotting.</p> Source code in <code>utils/plots.py</code> <pre><code>def cv_rank_time_plot(results, title=None, ax=None, max=None, xlabel=None, ylabel=None, xticks=None, yticks=None):\n    '''\n    Plot the results of the cross-validation and rank.\n\n    Parameters:\n        results (numpy.ndarray): The results of the cross-validation and rank.\n        title (str): The title of the plot.\n        ax (matplotlib.axes.Axes): The axes to plot on (optional). Default is None.\n        max (int): The maximum value for the colorbar (optional). Default is None.\n        xlabel (str): The label for the x-axis (optional). Default is None.\n        ylabel (str): The label for the y-axis (optional). Default is None.\n        xticks (List[str]): The tick labels for the x-axis (optional). Default is None.\n        yticks (List[str]): The tick labels for the y-axis (optional). Default is None.\n\n    Returns:\n        fig (matplotlib.figure.Figure): The figure containing the plot (optional).\n        im (matplotlib.image.AxesImage): The image representing the plot.\n\n    If ax is None, a new figure is created. The plot is displayed using a colormap\n    with the 'viridis' color map. The colorbar is added to the plot. If fig is not None,\n    the figure is returned. If fig is None, the image is returned.\n\n    Note:\n    Since the first dimension corresponds to the Y-axis, and the second dimension corresponds to the X-axis, the results is transposed before plotting.\n    '''\n\n    # If ax is None, create a new figure\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    else:\n        fig = None\n\n    # Plot the results\n    im = ax.imshow(results.T, cmap='viridis', vmin=0, vmax=max)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n    ax.set_xticks(range(len(xticks)))\n    ax.set_xticklabels(xticks)\n    ax.set_yticks(range(len(yticks)))\n    ax.set_yticklabels(yticks)\n\n    if fig is not None:\n        fig.colorbar(im, ax=ax)\n        return fig\n\n    if fig is None:\n        return im\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.plot_3d_scatter_with_color","title":"<code>plot_3d_scatter_with_color(ax, data, title=None, xlabel=None, ylabel=None, zlabel=None, xticks=None, yticks=None, zticks=None)</code>","text":"<p>Plots 4D data using a 3D scatter plot with color coding.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot.</p> required <code>data</code> <code>array</code> <p>A 3D array representing the data to plot.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Default is None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis. Default is None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Default is None.</p> <code>None</code> <code>zlabel</code> <code>str</code> <p>Label for the z-axis. Default is None.</p> <code>None</code> <code>xticks</code> <code>list</code> <p>Ticks for the x-axis. Default is None.</p> <code>None</code> <code>yticks</code> <code>list</code> <p>Ticks for the y-axis. Default is None.</p> <code>None</code> <code>zticks</code> <code>list</code> <p>Ticks for the z-axis. Default is None.</p> <code>None</code> Source code in <code>utils/plots.py</code> <pre><code>def plot_3d_scatter_with_color(ax: matplotlib.axes.Axes, data: np.array, title=None, xlabel=None, ylabel=None, zlabel=None, xticks=None, yticks=None, zticks=None) -&gt; None:\n    \"\"\"\n    Plots 4D data using a 3D scatter plot with color coding.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The axes on which to plot.\n        data (np.array): A 3D array representing the data to plot.\n        title (str, optional): Title of the plot. Default is None.\n        xlabel (str, optional): Label for the x-axis. Default is None.\n        ylabel (str, optional): Label for the y-axis. Default is None.\n        zlabel (str, optional): Label for the z-axis. Default is None.\n        xticks (list, optional): Ticks for the x-axis. Default is None.\n        yticks (list, optional): Ticks for the y-axis. Default is None.\n        zticks (list, optional): Ticks for the z-axis. Default is None.\n    \"\"\"\n    # Assuming data is 3D: (nLayers, nLayers, nTimepoints)\n    nLayers_source, nLayers_target, nTime_indeces = data.shape\n\n    # Turn time indeces into timepoints\n    vmi = np.arange(nTime_indeces)\n    timepoints = vmi * preprocess['bin-size']\n\n    # Create meshgrid for 3D coordinates\n    x, y, t = np.meshgrid(np.arange(nLayers_source), np.arange(nLayers_target), timepoints, indexing='ij')\n\n    # Flatten the data for 3D scatter plot\n    x = x.flatten()\n    y = y.flatten()\n    z = data.flatten()\n    c = t.flatten()\n\n    # Plot the 3D scatter plot\n    scatter = ax.scatter(x, y, z, c=c, cmap='viridis', vmin=min(c), vmax=max(c))\n\n    # Add color bar\n    colorbar = plt.colorbar(scatter, ax=ax, pad=0.1)\n    colorbar.set_label('Timepoints (s)')\n\n    # Set labels and title if provided\n    if title:\n        ax.set_title(title)\n    if xlabel:\n        ax.set_xlabel(xlabel)\n    if ylabel:\n        ax.set_ylabel(ylabel)\n    if zlabel:\n        ax.set_zlabel(zlabel, rotation=90)\n\n    # Set ticks if provided\n    if xticks is not None:\n        ax.set_xticks(xticks)\n    if yticks is not None:\n        ax.set_yticks(yticks)\n    if zticks is not None:\n        ax.set_zticks(zticks)\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.plot_stimuli","title":"<code>plot_stimuli(ecephys_session)</code>","text":"<p>['im104_r', 'im114_r', 'im083_r', 'im005_r', 'im087_r', 'im024_r', 'im111_r', 'im034_r']</p> Source code in <code>utils/plots.py</code> <pre><code>def plot_stimuli(ecephys_session):\n    \"\"\"['im104_r', 'im114_r', 'im083_r', 'im005_r', 'im087_r', 'im024_r', 'im111_r', 'im034_r']\"\"\"\n\n    # Visualizing all stimuli templates for the unwarped images on a subplot\n    fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n    for i, key in enumerate(ecephys_session.stimulus_templates['unwarped'].keys()):\n        ax[i//4, i %\n            4].imshow(ecephys_session.stimulus_templates['unwarped'][key], cmap='gray')\n        ax[i//4, i % 4].set_title(key)\n    plt.show()\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.raster_plot","title":"<code>raster_plot(spike_times, figsize=(8, 8), cmap=plt.cm.tab20, title='spike raster', cycle_colors=False, ax=None)</code>","text":"<p>imported from allensdk.brain_observatory.ecephys.visualization</p> Source code in <code>utils/plots.py</code> <pre><code>def raster_plot(spike_times, figsize=(8, 8), cmap=plt.cm.tab20, title='spike raster', cycle_colors=False, ax=None) -&gt; plt.Figure:\n    \"\"\"\n    imported from allensdk.brain_observatory.ecephys.visualization\n    \"\"\"\n\n    # Create a new figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    plotter = _VlPlotter(ax, num_objects=len(\n        spike_times.keys().unique()), cmap=cmap, cycle_colors=cycle_colors)\n    # aggregate is called on each column, so pass only one (eg the stimulus_presentation_id)\n    # to plot each unit once\n    spike_times[['stimulus_presentation_id', 'unit_id']\n                ].groupby('unit_id').agg(plotter)\n\n    ax.set_xlabel('time (s)', fontsize=16)\n    ax.set_ylabel('unit', fontsize=16)\n    ax.set_title(title, fontsize=20)\n\n    plt.yticks([])\n    plt.axis('tight')\n\n    return fig\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.rrr_rank_plot","title":"<code>rrr_rank_plot(scores, title='RRR test scores (r2)', time_series=None, ax=None)</code>","text":"<p>Plots the RRR test scores as a function of rank and time.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>array - like</code> <p>A two-dimensional array-like object representing the scores.</p> required <code>rank</code> <code>array - like</code> <p>A one-dimensional array-like object representing the rank.</p> required <code>title</code> <code>str</code> <p>The title of the plot. Default is 'Activity Estimation Error'.</p> <code>'RRR test scores (r2)'</code> <code>time_series</code> <code>array - like</code> <p>A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The axes on which to plot. If not provided, a new figure and axes will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>The figure object containing the plot.</p> Source code in <code>utils/plots.py</code> <pre><code>def rrr_rank_plot(scores, title='RRR test scores (r2)', time_series=None, ax=None) -&gt; plt.Figure:\n    \"\"\"\n    Plots the RRR test scores as a function of rank and time.\n\n    Parameters:\n        scores (array-like): A two-dimensional array-like object representing the scores.\n        rank (array-like): A one-dimensional array-like object representing the rank.\n        title (str, optional): The title of the plot. Default is 'Activity Estimation Error'.\n        time_series (array-like, optional): A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If not provided, a new figure and axes will be created.\n\n    Returns:\n        figure (matplotlib.figure.Figure): The figure object containing the plot.\n    \"\"\"\n\n    # Set default values\n    if time_series is None:\n        duration = preprocess['stimulus-duration']\n        time_step = preprocess['step-size']\n        time_series = np.arange(0, duration, time_step).round(3)\n    time_step = 2\n\n    # Create a new figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the errors as a function of rank\n    im = ax.imshow(scores, aspect='auto')\n    ax.set_xlabel('Time (ms)')\n    ax.set_xticks(np.arange(0, len(time_series), time_step))\n    ax.set_xticklabels(time_series[::time_step])\n    ax.set_ylabel('Rank')\n    ax.set_title(title)\n\n    # Add colorbar\n    fig.colorbar(im, ax=ax)\n\n    return fig\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.rrr_rank_plot_over_time","title":"<code>rrr_rank_plot_over_time(scores, title='RRR test scores', time_series=None, fig=None, axs=None, label=None, log=False)</code>","text":"<p>Plots the RRR test scores as a function of rank and time.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>array - like</code> <p>A two-dimensional array-like object representing the scores.</p> required <code>rank</code> <code>array - like</code> <p>A one-dimensional array-like object representing the rank.</p> required <code>title</code> <code>str</code> <p>The title of the plot. Default is 'Activity Estimation Error'.</p> <code>'RRR test scores'</code> <code>time_series</code> <code>array - like</code> <p>A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The axes on which to plot. If not provided, a new figure and axes will be created.</p> required <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>The figure object containing the plot.</p> Source code in <code>utils/plots.py</code> <pre><code>def rrr_rank_plot_over_time(scores, title='RRR test scores', time_series=None, fig=None, axs=None, label=None, log=False) -&gt; plt.Figure:\n    \"\"\"\n    Plots the RRR test scores as a function of rank and time.\n\n    Parameters:\n        scores (array-like): A two-dimensional array-like object representing the scores.\n        rank (array-like): A one-dimensional array-like object representing the rank.\n        title (str, optional): The title of the plot. Default is 'Activity Estimation Error'.\n        time_series (array-like, optional): A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If not provided, a new figure and axes will be created.\n\n    Returns:\n        figure (matplotlib.figure.Figure): The figure object containing the plot.\n    \"\"\"\n\n    # Set default values\n    if time_series is None:\n        duration = preprocess['stimulus-duration']\n        time_step = preprocess['step-size']\n        time_bin = preprocess['bin-size']\n        time_series = np.arange(0, duration+time_step, time_step).round(3)\n\n    # Create a new figure and axes if not provided\n    if axs is None:\n        fig, axs = plt.subplots(1, scores.shape[1], figsize=(15, 3))\n\n    # Create suptitle\n    if fig is not None:\n        fig.suptitle(title)\n\n    # Iterate through the time dimension of the scores\n    for t, scores_t in iterate_dimension(scores, 1):\n\n        # Calculate optimal rank for the current time\n        optimal_rank = np.argmax(scores_t)+1\n\n        # Set the time range for the current time\n        from_time, to_time = time_series[t].round(3), (time_series[t]+time_bin).round(3)\n\n        # Print optimal rank for the current time\n        if log:\n            print(f'Optimal rank for {from_time}-{to_time} ms: {optimal_rank}')\n\n        # Save the optimal rank for the current time\n        save_pickle(optimal_rank, f'optimal-rank-{from_time}-{to_time}ms', path='results')\n\n        # Plot the scores for the current time\n        axs[t].plot(scores_t, label=label)\n        axs[t].set_title(f'{from_time}-{to_time} ms')\n        axs[t].set_xlabel('Rank')\n\n    # Set the y-label for the first plot\n    axs[0].set_ylabel('Test score (r2)')\n\n    return fig\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.score_plot_by_time","title":"<code>score_plot_by_time(scores, title=None, time_series=None, ax=None, label='', color=None)</code>","text":"<p>Plots the RRR test scores as a function of rank and time.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>A two-dimensional array-like object representing the scores. Shape (cv, time)</p> required <code>rank</code> <code>ndarray</code> <p>A one-dimensional array-like object representing the rank.</p> required <code>title</code> <code>str</code> <p>The title of the plot. Default is 'Activity Estimation Error'. Default is None.</p> <code>None</code> <code>time_series</code> <code>array - like</code> <p>A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file. Default is None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The axes on which to plot. If not provided, a new figure and axes will be created. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>The figure object containing the plot.</p> Source code in <code>utils/plots.py</code> <pre><code>def score_plot_by_time(scores, title=None, time_series=None, ax=None, label='', color=None) -&gt; plt.Figure:\n    \"\"\"\n    Plots the RRR test scores as a function of rank and time.\n\n    Parameters:\n        scores (np.ndarray): A two-dimensional array-like object representing the scores. Shape (cv, time)\n        rank (np.ndarray): A one-dimensional array-like object representing the rank.\n        title (str, optional): The title of the plot. Default is 'Activity Estimation Error'. Default is None.\n        time_series (array-like, optional): A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file. Default is None.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If not provided, a new figure and axes will be created. Default is None.\n\n    Returns:\n        figure (matplotlib.figure.Figure): The figure object containing the plot.\n    \"\"\"\n\n    # If color is not provided, generate it from the default color cycle. If color is None, the color will be determined by the axes.\n    if color is None:\n        color = next(ax._get_lines.prop_cycler)['color']\n\n\n    # Calculate tshe mean and standard error of the mean of the scores\n    mean_scores = np.mean(scores, axis=0)\n    sem_scores = sem(scores, axis=0)\n\n    # Calculate the maximum value of the scores\n    max_score = np.max(mean_scores+sem_scores)\n\n    # Set default values\n    if time_series is None:\n        duration = preprocess['stimulus-duration'] # 0.250\n        time_step = preprocess['step-size'] # 0.050\n        half_time_step = time_step/2\n        time_series = np.arange(0+half_time_step, duration+half_time_step, time_step).round(3)\n\n    # Create a new figure and axes if not provided\n    if ax is None:\n        fig, ax = plt.subplots()\n        return_fig = True\n\n    # Create title\n    if title is not None:\n        ax.set_title(title)\n\n    # Plot the mean and standard error of the mean of the scores as a function of time\n    ax.plot(range(len(mean_scores)), mean_scores, label=label, color=color)\n    ax.fill_between(range(len(mean_scores)), mean_scores-sem_scores, mean_scores+sem_scores, alpha=0.1, color=color)\n\n    ax.set_xlabel('Time (s)')\n    ax.set_xticks(np.arange(0, len(time_series), 1))\n    ax.set_xticklabels(time_series[::1])\n    ax.set_ylabel('Test score (r2)')\n    # ax.set_xlim(1, len(mean_scores))\n    ax.set_ylim(0, max_score)\n\n    return\n</code></pre>"},{"location":"references/utilities/plots/#utils.plots.simple_mean_SEM_time_plot","title":"<code>simple_mean_SEM_time_plot(ax, mean, ylabel, title=None, SEM=None, SEM_multiplier=2, time_series=None, color=None, xlabel=None, alpha=0.2, linewidth=None, xticks=None, xticklabels=None, yticks=None, yticklabels=None, label=None, xlim=None, ylim=None)</code>","text":"<p>Plots the mean and standard error of the mean of the results as a function of time.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot. If not provided, a new figure and axes will be created.</p> required <code>mean</code> <code>array - like</code> <p>A one-dimensional array-like object representing the mean of the results.</p> required <code>ylabel</code> <code>str</code> <p>The label for the y-axis.</p> required <code>title</code> <code>str</code> <p>The title of the plot. Default is 'Mean and SEM of the results'.</p> <code>None</code> <code>SEM</code> <code>array - like</code> <p>A one-dimensional array-like object representing the standard error of the mean of the results.</p> <code>None</code> <code>SEM_multiplier</code> <code>int</code> <p>The multiplier for the standard error of the mean. Default is 2.</p> <code>2</code> <code>time_series</code> <code>array - like</code> <p>A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file.</p> <code>None</code> <code>color</code> <code>str</code> <p>The color of the plot. Default is None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>The label for the x-axis. Default is 'Time (s)'.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The transparency of the shaded area representing the standard error of the mean. Default is 0.2.</p> <code>0.2</code> <code>linewidth</code> <code>float</code> <p>The width of the line representing the mean. Default is None.</p> <code>None</code> <code>xticks</code> <code>array - like</code> <p>The locations of the x-axis ticks.</p> <code>None</code> <code>xticklabels</code> <code>array - like</code> <p>The labels for the x-axis ticks.</p> <code>None</code> <code>yticks</code> <code>array - like</code> <p>The locations of the y-axis ticks.</p> <code>None</code> <code>yticklabels</code> <code>array - like</code> <p>The labels for the y-axis ticks.</p> <code>None</code> <code>label</code> <code>str</code> <p>The label for the plot. Default is None.</p> <code>None</code> <code>xlim</code> <code>tuple</code> <p>The limits for the x-axis.</p> <code>None</code> <code>ylim</code> <code>tuple</code> <p>The limits for the y-axis.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: The figure object containing the plot.</p> Source code in <code>utils/plots.py</code> <pre><code>def simple_mean_SEM_time_plot(ax, mean, ylabel, title=None, SEM=None, SEM_multiplier=2, time_series=None, color=None, xlabel=None, alpha=0.2, linewidth=None, xticks=None, xticklabels=None, yticks=None, yticklabels=None, label=None, xlim=None, ylim=None) -&gt; plt.Figure:\n    \"\"\"\n    Plots the mean and standard error of the mean of the results as a function of time.\n\n    Parameters:\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot. If not provided, a new figure and axes will be created.\n        mean (array-like): A one-dimensional array-like object representing the mean of the results.\n        ylabel (str): The label for the y-axis.\n        title (str, optional): The title of the plot. Default is 'Mean and SEM of the results'.\n        SEM (array-like, optional): A one-dimensional array-like object representing the standard error of the mean of the results.\n        SEM_multiplier (int, optional): The multiplier for the standard error of the mean. Default is 2.\n        time_series (array-like, optional): A one-dimensional array-like object representing the time series. If not provided, it will be generated using the params.yaml file.\n        color (str, optional): The color of the plot. Default is None.\n        xlabel (str, optional): The label for the x-axis. Default is 'Time (s)'.\n        alpha (float, optional): The transparency of the shaded area representing the standard error of the mean. Default is 0.2.\n        linewidth (float, optional): The width of the line representing the mean. Default is None.\n        xticks (array-like, optional): The locations of the x-axis ticks.\n        xticklabels (array-like, optional): The labels for the x-axis ticks.\n        yticks (array-like, optional): The locations of the y-axis ticks.\n        yticklabels (array-like, optional): The labels for the y-axis ticks.\n        label (str, optional): The label for the plot. Default is None.\n        xlim (tuple, optional): The limits for the x-axis.\n        ylim (tuple, optional): The limits for the y-axis.\n\n    Returns:\n        matplotlib.figure.Figure: The figure object containing the plot.\n    \"\"\"\n\n    # Set default values\n    if time_series is None:\n        duration = preprocess['stimulus-duration']\n        time_step = preprocess['step-size']\n        time_series = np.arange(0, duration+time_step, time_step).round(3)\n\n    if xlabel is None:\n        xlabel = 'Time (s)'\n\n    # Plot the mean and standard error of the mean of the results as a function of time\n    if SEM is not None:\n        ax.fill_between(time_series,\n                        mean-SEM*SEM_multiplier,\n                        mean+SEM*SEM_multiplier,\n                        alpha=alpha, color=color)\n    cax=ax.plot(time_series, mean, color=color, linewidth=linewidth, label=label)\n\n    # Set the lims\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\n    # Set the x-axis and y-axis labels\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    # Set the title of the plot\n    ax.set_title(title)\n\n    # Set xticks and yticks\n    if xticks is not None:\n        ax.set_xticks(xticks)\n    if xticklabels is not None:\n        ax.set_xticklabels(xticklabels)\n    if yticks is not None:\n        ax.set_yticks(yticks)\n\n    return cax\n</code></pre>"},{"location":"references/utilities/utils/","title":"Important utility tools","text":"<p>This section contains a list of important utility tools that are used in the experiments.</p> <p>This submodule contains utility tools for various tasks.</p> <p>Functions:</p> <ul> <li>calculate_accuracy(Y_data, prediction) -&gt; float: Calculate the accuracy based on the similarity between Y_data and the prediction.</li> <li>MSE(target, prediction) -&gt; float: Calculate the Mean Squared Error based on the mean squared error between Y_data and the prediction.</li> <li>iterate_dimension(arr, dim) -&gt; None: Iterate through a specific dimension of a numpy array.</li> <li>normality_test(full_activity, dim=2) -&gt; None: Determine if the data has normal distribution.</li> <li>get_time(time_bin, bin_size=preprocess['step-size'], digits=3) -&gt; float: Get the time in seconds based on the time bin and step size.</li> <li>shift_with_nans(arr, shift, axis=2, constant=np.nan) -&gt; np.ndarray: Shift the elements of a numpy array along a specified axis by padding with NaNs.</li> <li>printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='\u2588', printEnd=\" \", onComplete='delete') -&gt; None: Print iterations progress.</li> <li>ProgressBarManager -&gt; class: A class to manage multiple progress bars.</li> <li>options_and_arguments() -&gt; None: A simple example of using options and arguments in a Python script.</li> <li>dfs(node, graph, visited, component) -&gt; None: Function to find the connected components in a graph using DFS.</li> <li>elements_to_dfs(input: iter) -&gt; defaultdict: Convert elements to a dictionary of dataframes.</li> <li>dfs_to_graph(column_to_dfs: defaultdict) -&gt; defaultdict: Convert a dictionary of dataframes to a graph.</li> <li>createGraph(input: Iterable) -&gt; defaultdict: Create a graph based on the input data.</li> <li>iterate_common_elements(lists) -&gt; None: Generator function to yield merged lists with common elements.</li> <li>mergeDataframes(dataframes: pd.DataFrame) -&gt; list: Merge a list of DataFrames into a list of merged DataFrames.</li> <li>get_args(argv) -&gt; tuple: Get the options and arguments from the command line.</li> </ul>"},{"location":"references/utilities/utils/#utils.utils.ProgressBarManager","title":"<code>ProgressBarManager</code>","text":"<p>A class to manage multiple progress bars.</p> Source code in <code>utils/utils.py</code> <pre><code>class ProgressBarManager:\n    \"\"\"\n    A class to manage multiple progress bars.\n    \"\"\"\n\n    def __init__(self):\n        '''\n        Initializes a ProgressBarManager object.\n\n        Usage:\n            manager = ProgressBarManager()\n\n        Example:\n        ```python\n        manager = ProgressBarManager()\n\n        # Simulate progress\n        for i in range(10):\n            manager.progress_bar('Download', i, 10)\n            # time.sleep(5)\n            for j in range(50):\n                manager.progress_bar('Upload', j, 50)\n                time.sleep(0.01)\n        ```\n        '''\n        self.progress_bars = {}\n        self.n_progress_bars = 0\n\n    def print_progress_bars(self):\n        # Clear the screen or move the cursor back to the top\n        for id, bar in self.progress_bars.items():\n            printProgressBar(bar['current'], bar['total'],\n                             prefix=f'{id}:', length=50, printEnd='\\n')\n\n        for _ in range(self.n_progress_bars):\n            print(\"\\033[F\", end='')\n\n    def progress_bar(self, id, current, total):\n        '''\n        Updates the progress bar with the given id, current value, and total value.\n\n        Args:\n            id (str): The id of the progress bar.\n            current (int): The current value of the progress bar.\n            total (int): The total value of the progress bar.\n\n        Example:\n        ```python\n        manager = ProgressBarManager()\n\n        # Simulate progress\n        for i in range(10):\n            manager.progress_bar('Download', i, 10)\n            # time.sleep(5)\n            for j in range(50):\n                manager.progress_bar('Upload', j, 50)\n                time.sleep(0.01)\n        ```\n        '''\n        # If not already initialized, initialize a new progress bar\n        if id not in self.progress_bars:\n            self.new_progress_bar(id, current, total)\n        else:\n            self.update_progress_bar(id, current)\n\n        # If the progress bar is complete, delete it\n        if current == total:\n            self.delete_progress_bar(id)\n\n        # Print all the progress bars\n        self.print_progress_bars()\n\n    def new_progress_bar(self, id, current, total):\n        # Initialize a new progress bar with the given id and step\n        self.progress_bars[id] = {'current': current, 'total': total}\n        self.n_progress_bars += 1\n\n    def update_progress_bar(self, id, current):\n        # Update the progress bar with the given id\n        self.progress_bars[id]['current'] = current\n\n    def delete_progress_bar(self, id):\n        # Delete the progress bar with the given id\n        del self.progress_bars[id]\n        self.n_progress_bars -= 1\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.ProgressBarManager.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a ProgressBarManager object.</p> Usage <p>manager = ProgressBarManager()</p> <p>Example:</p> <pre><code>manager = ProgressBarManager()\n\n# Simulate progress\nfor i in range(10):\n    manager.progress_bar('Download', i, 10)\n    # time.sleep(5)\n    for j in range(50):\n        manager.progress_bar('Upload', j, 50)\n        time.sleep(0.01)\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def __init__(self):\n    '''\n    Initializes a ProgressBarManager object.\n\n    Usage:\n        manager = ProgressBarManager()\n\n    Example:\n    ```python\n    manager = ProgressBarManager()\n\n    # Simulate progress\n    for i in range(10):\n        manager.progress_bar('Download', i, 10)\n        # time.sleep(5)\n        for j in range(50):\n            manager.progress_bar('Upload', j, 50)\n            time.sleep(0.01)\n    ```\n    '''\n    self.progress_bars = {}\n    self.n_progress_bars = 0\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.ProgressBarManager.progress_bar","title":"<code>progress_bar(id, current, total)</code>","text":"<p>Updates the progress bar with the given id, current value, and total value.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The id of the progress bar.</p> required <code>current</code> <code>int</code> <p>The current value of the progress bar.</p> required <code>total</code> <code>int</code> <p>The total value of the progress bar.</p> required <p>Example:</p> <pre><code>manager = ProgressBarManager()\n\n# Simulate progress\nfor i in range(10):\n    manager.progress_bar('Download', i, 10)\n    # time.sleep(5)\n    for j in range(50):\n        manager.progress_bar('Upload', j, 50)\n        time.sleep(0.01)\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def progress_bar(self, id, current, total):\n    '''\n    Updates the progress bar with the given id, current value, and total value.\n\n    Args:\n        id (str): The id of the progress bar.\n        current (int): The current value of the progress bar.\n        total (int): The total value of the progress bar.\n\n    Example:\n    ```python\n    manager = ProgressBarManager()\n\n    # Simulate progress\n    for i in range(10):\n        manager.progress_bar('Download', i, 10)\n        # time.sleep(5)\n        for j in range(50):\n            manager.progress_bar('Upload', j, 50)\n            time.sleep(0.01)\n    ```\n    '''\n    # If not already initialized, initialize a new progress bar\n    if id not in self.progress_bars:\n        self.new_progress_bar(id, current, total)\n    else:\n        self.update_progress_bar(id, current)\n\n    # If the progress bar is complete, delete it\n    if current == total:\n        self.delete_progress_bar(id)\n\n    # Print all the progress bars\n    self.print_progress_bars()\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.MSE","title":"<code>MSE(target, prediction)</code>","text":"<p>Calculate the Mean Squared Error based on the mean squared error between target and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>The actual data.</p> required <code>prediction</code> <code>ndarray</code> <p>The predicted data.</p> required <p>Returns:</p> Name Type Description <code>similarity</code> <code>float</code> <p>The similarity score.</p> Source code in <code>utils/utils.py</code> <pre><code>def MSE(target, prediction):\n    \"\"\"\n    Calculate the Mean Squared Error based on the mean squared error between target and prediction.\n\n    Args:\n        target (np.ndarray): The actual data.\n        prediction (np.ndarray): The predicted data.\n\n    Returns:\n        similarity (float): The similarity score.\n    \"\"\"\n    mse = np.mean((target - prediction) ** 2)\n    similarity = 1 / (1 + mse)\n    return similarity\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.calculate_accuracy","title":"<code>calculate_accuracy(Y_data, prediction)</code>","text":"<p>Calculate the accuracy based on the similarity between Y_data and the prediction.</p> <p>Parameters:</p> Name Type Description Default <code>Y_data</code> <code>ndarray</code> <p>The actual data.</p> required <code>prediction</code> <code>ndarray</code> <p>The predicted data.</p> required <p>Returns:</p> Name Type Description <code>accuracy</code> <code>float</code> <p>The accuracy score.</p> Source code in <code>utils/utils.py</code> <pre><code>def calculate_accuracy(Y_data, prediction):\n    \"\"\"\n    Calculate the accuracy based on the similarity between Y_data and the prediction.\n\n    Args:\n        Y_data (np.ndarray): The actual data.\n        prediction (np.ndarray): The predicted data.\n\n    Returns:\n        accuracy (float): The accuracy score.\n    \"\"\"\n    similarity = np.mean(np.equal(Y_data, prediction))\n    accuracy = similarity \n    return accuracy\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.createGraph","title":"<code>createGraph(input)</code>","text":"<p>Create a graph based on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Iterable</code> <p>An iterable containing the input data.</p> required <p>Returns:</p> Name Type Description <code>defaultdict</code> <code>defaultdict</code> <p>A defaultdict representing the graph.</p> Source code in <code>utils/utils.py</code> <pre><code>def createGraph(input: Iterable) -&gt; defaultdict:\n    \"\"\"\n    Create a graph based on the input data.\n\n    Args:\n        input (Iterable): An iterable containing the input data.\n\n    Returns:\n        defaultdict: A defaultdict representing the graph.\n    \"\"\"\n    column_to_dfs = elements_to_dfs(input)\n    graph = dfs_to_graph(column_to_dfs)\n    return graph\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.dfs","title":"<code>dfs(node, graph, visited, component)</code>","text":"<p>Function to find the connected components in a graph using Depth-First Search (DFS).</p> <pre><code>node (int): The starting node for the DFS traversal.\ngraph (dict): The graph represented as an adjacency list.\nvisited (list): A boolean array to keep track of visited nodes.\ncomponent (list): A list to store the nodes in the connected component.\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def dfs(node, graph, visited, component):\n    \"\"\"\n    Function to find the connected components in a graph using Depth-First Search (DFS).\n\n        node (int): The starting node for the DFS traversal.\n        graph (dict): The graph represented as an adjacency list.\n        visited (list): A boolean array to keep track of visited nodes.\n        component (list): A list to store the nodes in the connected component.\n\n    \"\"\"\n    stack = [node]\n    while stack:\n        current = stack.pop()\n        if not visited[current]:\n            visited[current] = True\n            component.append(current)\n            for neighbor in graph[current]:\n                if not visited[neighbor]:\n                    stack.append(neighbor)\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.dfs_to_graph","title":"<code>dfs_to_graph(column_to_dfs)</code>","text":"<p>Convert a dictionary of dataframes to a graph.</p> <p>Parameters:</p> Name Type Description Default <code>column_to_dfs</code> <code>defaultdict</code> <p>A defaultdict representing the dictionary of dataframes.</p> required <p>Returns:</p> Name Type Description <code>defaultdict</code> <code>defaultdict</code> <p>A defaultdict representing the graph.</p> Source code in <code>utils/utils.py</code> <pre><code>def dfs_to_graph(column_to_dfs: defaultdict) -&gt; defaultdict:\n    \"\"\"\n    Convert a dictionary of dataframes to a graph.\n\n    Args:\n        column_to_dfs (defaultdict): A defaultdict representing the dictionary of dataframes.\n\n    Returns:\n        defaultdict: A defaultdict representing the graph.\n    \"\"\"\n    graph = defaultdict(list)\n    for indices in column_to_dfs.values():\n        indices = list(indices)\n        for i in range(len(indices)):\n            for j in range(i + 1, len(indices)):\n                graph[indices[i]].append(indices[j])\n                graph[indices[j]].append(indices[i])\n    return graph\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.elements_to_dfs","title":"<code>elements_to_dfs(input)</code>","text":"<p>Convert elements to a dictionary of dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>iter</code> <p>An iterable containing the input data.</p> required <p>Returns:</p> Name Type Description <code>defaultdict</code> <code>defaultdict</code> <p>A defaultdict representing the dictionary of dataframes.</p> Source code in <code>utils/utils.py</code> <pre><code>def elements_to_dfs(input: iter) -&gt; defaultdict:\n    \"\"\"\n    Convert elements to a dictionary of dataframes.\n\n    Args:\n        input (iter): An iterable containing the input data.\n\n    Returns:\n        defaultdict: A defaultdict representing the dictionary of dataframes.\n    \"\"\"\n    column_to_dfs = defaultdict(set)\n    for idx, df in enumerate(input):\n        for column in df.columns:\n            column_to_dfs[column].add(idx)\n    return column_to_dfs\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.get_args","title":"<code>get_args(argv)</code>","text":"<p>Get the options and arguments from the command line.</p> <p>Parameters:</p> Name Type Description Default <code>argv</code> <code>list</code> <p>The list of command line arguments.</p> required <p>Returns:</p> Name Type Description <code>options</code> <code>list</code> <p>The list of options.</p> <code>args</code> <code>list</code> <p>The list of arguments.</p> Example <p>opts, args = get_args(sys.argv)</p> Source code in <code>utils/utils.py</code> <pre><code>def get_args(argv):\n    \"\"\"\n    Get the options and arguments from the command line.\n\n    Args:\n        argv (list): The list of command line arguments.\n\n    Returns:\n        options (list): The list of options.\n        args (list): The list of arguments.\n\n    Example:\n        opts, args = get_args(sys.argv)\n    \"\"\"\n\n    import getopt\n    import sys\n\n    try:\n        opts, args = getopt.getopt(argv, \"ho:v\", [\"help\", \"output=\"])\n    except getopt.GetoptError:\n        print('Usage: script.py -o &lt;output_file&gt; -v')\n        sys.exit(2)\n\n    return opts, args\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.get_time","title":"<code>get_time(time_bin, bin_size=preprocess['step-size'], digits=3)</code>","text":"<p>Get the time in seconds based on the time bin and step size.</p> <p>Parameters:</p> Name Type Description Default <code>time_bin</code> <code>float</code> <p>The time bin.</p> required <code>bin_size</code> <code>float</code> <p>The step size. Default is the value specified in the 'preprocess' parameter.</p> <code>preprocess['step-size']</code> <code>digits</code> <code>int</code> <p>The number of decimal places to round the result to. Default is 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>rounded_value</code> <code>float</code> <p>The time in seconds.</p> Source code in <code>utils/utils.py</code> <pre><code>def get_time(time_bin, bin_size=preprocess['step-size'], digits=3):\n    \"\"\"\n    Get the time in seconds based on the time bin and step size.\n\n    Args:\n        time_bin (float): The time bin.\n        bin_size (float, optional): The step size. Default is the value specified in the 'preprocess' parameter.\n        digits (int, optional): The number of decimal places to round the result to. Default is 3.\n\n    Returns:\n        rounded_value (float): The time in seconds.\n    \"\"\"\n    return round(time_bin*bin_size, digits)\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.iterate_common_elements","title":"<code>iterate_common_elements(lists)</code>","text":"<p>Generator function to yield merged lists with common elements. Merge multiple lists into a single list by finding connected components in a graph.</p> <p>Parameters:</p> Name Type Description Default <code>lists</code> <code>list</code> <p>A list of lists to be merged.</p> required <p>Yields:</p> Name Type Description <code>merged_set</code> <code>list</code> <p>A merged list containing unique elements from the input lists.</p> Source code in <code>utils/utils.py</code> <pre><code>def iterate_common_elements(lists):\n    \"\"\"\n    Generator function to yield merged lists with common elements.\n    Merge multiple lists into a single list by finding connected components in a graph.\n\n    Args:\n        lists (list): A list of lists to be merged.\n\n    Yields:\n        merged_set (list): A merged list containing unique elements from the input lists.\n\n    \"\"\"\n    # Step 1: Create a graph\n    graph = createGraph(lists)\n\n    # Step 2: Find connected components\n    visited = [False] * len(lists)\n\n    for i in range(len(lists)):\n        if not visited[i]:\n            component = []\n            dfs(i, graph, visited, component)\n            # Step 3: Merge lists in each component\n            merged_set = set()\n            for idx in component:\n                if not merged_set:\n                    merged_set = set(lists[idx])\n                else:\n                    # yield (idx, lists[idx])\n                    merged_set.update(lists[idx])\n            yield list(merged_set)\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.iterate_dimension","title":"<code>iterate_dimension(arr, dim)</code>","text":"<p>Iterate through a specific dimension of a numpy array.</p> <p>(Similar to the built-in np.ndenumerate() function, but with the ability to specify higher dimensions.)</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array.</p> required <code>dim</code> <code>int</code> <p>The dimension to iterate through.</p> required <p>Yields:</p> Type Description <p>Tuple[int, np.ndarray]: The counter and the sliced array along the specified dimension.</p> <p>Example:</p> <pre><code># Make a simple matrix with 3 dimensions\narr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\n# Iterate through the second dimension\nfor counter, sliced in iterate_dimension(arr, 1):\n    print(counter, sliced)\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def iterate_dimension(arr, dim):\n    \"\"\"\n    Iterate through a specific dimension of a numpy array.\n\n    (Similar to the built-in np.ndenumerate() function, but with the ability to specify higher dimensions.)\n\n    Args:\n        arr (np.ndarray): The input array.\n        dim (int): The dimension to iterate through.\n\n    Yields:\n        Tuple[int, np.ndarray]: The counter and the sliced array along the specified dimension.\n\n    Example:\n    ```\n    # Make a simple matrix with 3 dimensions\n    arr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\n    # Iterate through the second dimension\n    for counter, sliced in iterate_dimension(arr, 1):\n        print(counter, sliced)\n    ```\n    \"\"\"\n    for i in range(arr.shape[dim]):\n        yield i, arr.take(i, axis=dim)\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.mergeDataframes","title":"<code>mergeDataframes(dataframes)</code>","text":"<p>Merge a list of DataFrames into a list of merged DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>dataframes</code> <code>list</code> <p>A list of pandas DataFrames to be merged.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of merged pandas DataFrames.</p> <p>Example:</p> <pre><code>df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ndf2 = pd.DataFrame({'B': [6, 7, 8], 'C': [9, 10, 11]})\ndf3 = pd.DataFrame({'D': [12, 13, 14], 'E': [15, 16, 17]})\ndf4 = pd.DataFrame({'C': [10, 11, 12], 'F': [18, 19, 20]})\ndataframes = [df1, df2, df3, df4]\n\nfor df in mergeDataframes(dataframes):\n    print(df)\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def mergeDataframes(dataframes: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Merge a list of DataFrames into a list of merged DataFrames.\n\n    Args:\n        dataframes (list): A list of pandas DataFrames to be merged.\n\n    Returns:\n        list: A list of merged pandas DataFrames.\n\n    Example:\n    ```\n    df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    df2 = pd.DataFrame({'B': [6, 7, 8], 'C': [9, 10, 11]})\n    df3 = pd.DataFrame({'D': [12, 13, 14], 'E': [15, 16, 17]})\n    df4 = pd.DataFrame({'C': [10, 11, 12], 'F': [18, 19, 20]})\n    dataframes = [df1, df2, df3, df4]\n\n    for df in mergeDataframes(dataframes):\n        print(df)\n    ```\n    \"\"\"\n    # Create a graph\n    graph = createGraph(dataframes)\n\n    # Find connected components\n    visited = [False] * len(dataframes)\n\n    # Merge DataFrames in each component\n    merged_graphs = []\n    for i in range(len(dataframes)):\n        if not visited[i]:\n            component = []\n\n            # Find connected components\n            dfs(i, graph, visited, component)\n\n            # Merge DataFrames in each component\n            merged_df = pd.DataFrame()\n            for idx in component:\n                if merged_df.empty:\n                    merged_df = dataframes[idx]\n                else:\n                    merged_df = pd.merge(merged_df, dataframes[idx], how='outer', on=list(\n                        merged_df.columns.intersection(dataframes[idx].columns)) or None)\n            merged_graphs.append(merged_df)\n    return merged_graphs\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.normality_test","title":"<code>normality_test(full_activity, dim=2)</code>","text":"<p>Perform the Shapiro-Wilk test to determine if the data has a normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>full_activity</code> <code>ndarray</code> <p>The input data.</p> required <code>dim</code> <code>int</code> <p>The dimension along which to perform the test. Default is 2.</p> <code>2</code> Source code in <code>utils/utils.py</code> <pre><code>def normality_test(full_activity, dim=2):\n    \"\"\"\n    Perform the Shapiro-Wilk test to determine if the data has a normal distribution.\n\n    Args:\n        full_activity (np.ndarray): The input data.\n        dim (int, optional): The dimension along which to perform the test. Default is 2.\n\n    \"\"\"\n    for counter, activity_slice in iterate_dimension(full_activity, dim=dim):\n        # Normality test\n        stat, p = shapiro(activity_slice)\n        print('Statistics=%.3f, p=%.3f' % (stat, p))\n        # Interpret\n        alpha = 0.05\n        if p &gt; alpha:\n            print(f'{counter}: Activity looks Gaussian (fail to reject H0)')\n        else:\n            print(f'{counter}: Activity does not look Gaussian (reject H0)')\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.options_and_arguments","title":"<code>options_and_arguments()</code>","text":"<p>A simple example of using options and arguments in a Python script.</p> Options <p>-h, --help: Show a help message and exit. -v, --verbose: Print verbose output. -o, --output : Specify an output file. <p>Parameters:</p> Name Type Description Default <code>input_file</code> <p>The input file to process.</p> required Example <p><code>python script.py -v -o output.txt input_file.txt</code></p> Source code in <code>utils/utils.py</code> <pre><code>def options_and_arguments():\n    \"\"\"\n    A simple example of using options and arguments in a Python script.\n\n    Options:\n        -h, --help: Show a help message and exit.\n        -v, --verbose: Print verbose output.\n        -o, --output &lt;file&gt;: Specify an output file.\n\n    Arguments:\n        input_file: The input file to process.\n\n    Example:\n        ```python script.py -v -o output.txt input_file.txt```\n    \"\"\"\n    import argparse\n\n    # Create the parser\n    parser = argparse.ArgumentParser(description='A simple example of using options and arguments.')\n\n    # Add options\n    parser.add_argument('-v', '--verbose', action='store_true', help='Print verbose output.')\n    parser.add_argument('-o', '--output', type=str, help='Specify an output file.')\n\n    # Add arguments\n    parser.add_argument('input_file', type=str, help='The input file to process.')\n\n    # Parse the arguments\n    args = parser.parse_args()\n\n    # Print the arguments\n    print('Input file:', args.input_file)\n    print('Verbose:', args.verbose)\n    print('Output file:', args.output)\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.printProgressBar","title":"<code>printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='\u2588', printEnd='\\r', onComplete='delete')</code>","text":"<p>Print iterations progress.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Current iteration.</p> required <code>total</code> <code>int</code> <p>Total iterations.</p> required <code>prefix</code> <code>str</code> <p>Prefix string. Defaults to ''.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>Suffix string. Defaults to ''.</p> <code>''</code> <code>decimals</code> <code>int</code> <p>Number of decimals in percent complete. Defaults to 1.</p> <code>1</code> <code>length</code> <code>int</code> <p>Character length of the progress bar. Defaults to 100.</p> <code>100</code> <code>fill</code> <code>str</code> <p>Bar fill character. Defaults to '\u2588'.</p> <code>'\u2588'</code> <code>printEnd</code> <code>str</code> <p>End character. Defaults to \" \".</p> <code>'\\r'</code> <code>onComplete</code> <code>str</code> <p>Action to perform when the progress is complete.  Options: 'delete' (clear the progress bar), 'newline' (print a newline character). Defaults to 'delete'.</p> <code>'delete'</code> <p>Example:</p> <pre><code>printProgressBar(0, 10)\nfor i in range(10):\n    printProgressBar(i + 1, 10)\n    time.sleep(0.1)\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='\u2588', printEnd=\"\\r\", onComplete='delete'):\n    \"\"\"\n    Print iterations progress.\n\n    Args:\n        iteration (int): Current iteration.\n        total (int): Total iterations.\n        prefix (str, optional): Prefix string. Defaults to ''.\n        suffix (str, optional): Suffix string. Defaults to ''.\n        decimals (int, optional): Number of decimals in percent complete. Defaults to 1.\n        length (int, optional): Character length of the progress bar. Defaults to 100.\n        fill (str, optional): Bar fill character. Defaults to '\u2588'.\n        printEnd (str, optional): End character. Defaults to \"\\r\".\n        onComplete (str, optional): Action to perform when the progress is complete. \n            Options: 'delete' (clear the progress bar), 'newline' (print a newline character). Defaults to 'delete'.\n\n    Example:\n    ```\n    printProgressBar(0, 10)\n    for i in range(10):\n        printProgressBar(i + 1, 10)\n        time.sleep(0.1)\n    ```\n    \"\"\"\n    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength + '-' * (length - filledLength)\n    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=printEnd)\n\n    # Complete\n    if iteration == total: \n\n        if onComplete == 'delete':\n            print(' ' * len(prefix) + ' ' * len(suffix) + ' ' * len(percent) + ' ' * length + '      ', end=printEnd)\n\n        if onComplete == 'newline':\n            print()\n</code></pre>"},{"location":"references/utilities/utils/#utils.utils.shift_with_nans","title":"<code>shift_with_nans(arr, shift, axis=2, constant=np.nan)</code>","text":"<p>Shift the elements of a numpy array along a specified axis by padding with NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array.</p> required <code>shift</code> <code>int</code> <p>The number of positions to shift the elements. Positive values shift to the right, negative values shift to the left.</p> required <code>axis</code> <code>int</code> <p>The axis along which to shift the elements. Default is 2.</p> <code>2</code> <code>constant</code> <code>int</code> <p>The value to use for padding. Default is np.nan.</p> <code>nan</code> <p>Returns:</p> Name Type Description <code>return</code> <code>ndarray</code> <p>The shifted array.</p> <p>Example:</p> <pre><code># Create a 2D array\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Shift the elements by 1 position to the right along axis 1\nshifted_arr = shift_with_nans(arr, 1, axis=1)\nprint(shifted_arr)\n# Output: [[nan 1 2]\n#          [nan 4 5]]\n\n# Shift the elements by 2 positions to the left along axis 0\nshifted_arr = shift_with_nans(arr, -2, axis=0)\nprint(shifted_arr)\n# Output: [[nan nan nan]\n#          [nan nan nan]\n#          [1 2 3]\n#          [4 5 6]]\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def shift_with_nans(arr, shift, axis=2, constant=np.nan):\n    \"\"\"\n    Shift the elements of a numpy array along a specified axis by padding with NaNs.\n\n    Args:\n        arr (np.ndarray): The input array.\n        shift (int): The number of positions to shift the elements. Positive values shift to the right, negative values shift to the left.\n        axis (int, optional): The axis along which to shift the elements. Default is 2.\n        constant (int, optional): The value to use for padding. Default is np.nan.\n\n    Returns:\n        return (np.ndarray): The shifted array.\n\n    Example:\n    ```\n    # Create a 2D array\n    arr = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Shift the elements by 1 position to the right along axis 1\n    shifted_arr = shift_with_nans(arr, 1, axis=1)\n    print(shifted_arr)\n    # Output: [[nan 1 2]\n    #          [nan 4 5]]\n\n    # Shift the elements by 2 positions to the left along axis 0\n    shifted_arr = shift_with_nans(arr, -2, axis=0)\n    print(shifted_arr)\n    # Output: [[nan nan nan]\n    #          [nan nan nan]\n    #          [1 2 3]\n    #          [4 5 6]]\n    ```\n    \"\"\"\n    padding = [(0, 0) for _ in range(arr.ndim)]\n    if shift &gt; 0:\n        padding[axis] = (shift, 0)\n    else:\n        padding[axis] = (0, -shift)\n    arr = np.pad(arr, padding, mode='constant', constant_values=constant)\n    slices = [slice(None) if i != axis else slice(None, -shift) if shift &gt; 0 else slice(-shift, None) for i in range(arr.ndim)]\n    return arr[tuple(slices)]\n</code></pre>"}]}